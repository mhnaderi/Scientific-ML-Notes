{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "\n",
    "# Artificial Neural Networks - Part 1\n",
    "\n",
    "### Part of Scientific-ML-Notes \n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-black?logo=github&scale=5)](https://github.com/mhnaderi/Scientific-ML-Notes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always run this cell first to import all necessary libraries and set up the notebook environment\n",
    "\n",
    "! pip install -q optax, equinox\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "import jax.numpy.linalg as la\n",
    "from jax import jit, grad, vmap, value_and_grad, random\n",
    "\n",
    "import optax\n",
    "import equinox as eqx\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",              \n",
    "    \"figure.facecolor\": 'white',        \n",
    "    \"axes.facecolor\": 'white' \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Fixed Basis Functions\n",
    "\n",
    "Linear basis function models involve creating linear combinations of basis functions $\\mathbf{\\Phi}(\\mathbf{x})$ and are expressed as:\n",
    "\n",
    "$$ f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\mathbf{\\Phi}(\\mathbf{x}) \\boldsymbol{\\theta} $$\n",
    "\n",
    "These models allow us to choose arbitrary nonlinear basis functions, and theoretically, with a sufficiently rich set of basis functions, we can approximate any desired function to an arbitrary degree of accuracy. This suggests that linear basis function models could serve as a universal framework for solving regression and classification tasks in machine learning. **However, significant limitations arise from the fact that the basis functions $\\mathbf{\\Phi}(\\mathbf{x})$ are fixed and do not adapt based on the training data.**\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "\n",
    "Let's start with a simple regression model involving a single input variable, represented as a polynomial of degree $M$. When we extend this model to incorporate $D$ input variables $\\{ x_1, x_2, \\ldots, x_D \\}$, we find that the number of coefficients (parameters) grows rapidly with the number of inputs. Specifically, for a polynomial of degree $M$ in $D$ variables, the number of coefficients increases on the order of $\\mathcal{O}(D^M)$. This exponential growth means that in high-dimensional spaces, polynomial models can quickly become computationally infeasible and practically useless. This phenomenon is known as the **curse of dimensionality**.\n",
    "\n",
    "## Properties of High-Dimensional Spaces\n",
    "\n",
    "Our geometrical intuitions, shaped by living in a three-dimensional world, often mislead us when we consider high-dimensional spaces. For instance, let's examine a hypersphere with a radius $r = 1$ in $D$ dimensions. If we calculate the fraction of the hypersphere's volume that lies between the radii $r = 1 - \\epsilon$ and $r = 1$, we find that as $D$ becomes large, this fraction approaches 1 even for very small $\\epsilon$. In other words, in high-dimensional spaces, nearly all the volume of the hypersphere is concentrated in a thin shell near its outer surface!\n",
    "\n",
    "Despite the challenges posed by high-dimensional spaces, there are also benefits. For instance, consider a dataset where each point is described by a pair of features $\\{ x_1, x_2 \\}$, and suppose the classes are linearly separable when both features are considered together. However, if we only have access to $x_1$, the classes may significantly overlap, making classification much more difficult. Thus, working in the higher-dimensional space makes the problem easier, as it allows us to capture more information that helps to distinguish between classes.\n",
    "\n",
    "## Data Manifolds\n",
    "\n",
    "Imagine attempting to learn a completely random mapping from every possible $28 \\times 28$ binary image to one of ten categories. Since this mapping lacks any inherent structure, the only viable strategy would be to memorize all $2^{784}$ possible assignments—a clearly impractical approach. However, in real-world scenarios, data typically resides within a region of the space that has a much lower effective dimensionality.\n",
    "\n",
    "For example, consider images of handwritten digits. Each image corresponds to a point in a high-dimensional space where the dimensionality equals the number of pixels (e.g., 784 for a $28 \\times 28$ image). Despite this high dimensionality, the variations in the images—for instance, differences in the vertical and horizontal positions of the digit within the image and variations in orientation—are governed by only a few parameters. Essentially, these images can be characterized by three degrees of freedom: horizontal position, vertical position, and rotation angle. Consequently, the set of all such images lies on a three-dimensional manifold embedded within the high-dimensional pixel space. This manifold is highly nonlinear due to the complex relationship between these parameters and the pixel intensities.\n",
    "\n",
    "Actually, the number of pixels in an image is a byproduct of the image capture process; they represent discrete measurements of an inherently continuous world. If we capture the same scene at a higher resolution, the dimensionality $D$ of our data space increases, but the images still lie on the same three-dimensional manifold corresponding to the degrees of freedom in object position and orientation.\n",
    "\n",
    "If we can design basis functions that are localized on the data manifold rather than spanning the entire high-dimensional data space, the number of basis functions needed may grow exponentially with the dimensionality of the manifold rather than with that of the data space, which is a significant reduction given that the manifold's dimensionality is typically much lower. Neural networks effectively learn such basis functions that are adapted to the data manifolds.\n",
    "\n",
    "Moreover, not all directions within the manifold may be relevant for a particular task. For instance, if our goal is to determine only the orientation of an object, and not its position, then out of the three degrees of freedom, only one is relevant. Neural networks can also learn which directions on the manifold are significant for predicting the desired outputs, focusing on the most informative variations in the data.\n",
    "\n",
    "To illustrate this, consider a set of images of a handwritten digit where each image varies in the digit's position (both horizontally and vertically) and its orientation (rotation angle). Although these images appear different, they are variations along the three degrees of freedom we mentioned earlier—position along the $x$-axis, position along the $y$-axis, and rotation. Despite the high dimensionality of the image space (equal to the number of pixels), these images collectively form a three-dimensional nonlinear manifold within that space.\n",
    "\n",
    "Another way to appreciate that real data occupies low-dimensional manifolds is by considering the task of generating random images. Suppose we create synthetic images by randomly assigning color values to each pixel independently, sampling the red, green, and blue intensities from a uniform distribution. The resulting images appear as noise and bear no resemblance to natural images. This is because natural images exhibit strong correlations between neighboring pixels; adjacent pixels are likely to have similar or identical colors due to the continuity of real-world objects. In contrast, the randomly generated images lack these correlations, resulting in visual randomness.\n",
    "\n",
    "Even though both the natural and synthetic images occupy points in the same high-dimensional pixel space, natural images represent only an infinitesimal fraction of that space. They are confined to a low-dimensional manifold characterized by the inherent structures and correlations present in the real world.\n",
    "\n",
    "## Data-Dependent Basis Functions\n",
    "\n",
    "Using simple basis functions that are chosen independently of the specific problem can be ineffective, particularly in high-dimensional settings. One traditional solution is to craft basis functions manually, utilizing expert knowledge tailored to the application at hand. While this approach can be beneficial, its success has been somewhat limited, especially as problems become more complex.\n",
    "\n",
    "Modern machine learning often favors data-driven methods, where the basis functions are learned from the training data itself. Although domain knowledge still plays an important role—often influencing the design of network architectures—it is typically applied in a more qualitative manner.\n",
    "\n",
    "In high-dimensional spaces, since data tends to reside on low-dimensional manifolds, it's advantageous to employ basis functions that are centered on the data points. Radial Basis Function (RBF) networks are an example of this approach. In RBF networks, each basis function is associated with a data point and centered at that point, automatically adapting to the structure of the data manifold.\n",
    "\n",
    "An RBF is typically defined as:\n",
    "\n",
    "$$ \\theta_n(\\mathbf{x}) = \\exp\\left( -\\frac{ \\| \\mathbf{x} - \\mathbf{x}_n \\|^2 }{ s^2 } \\right) $$\n",
    "\n",
    "where $\\mathbf{x}_n$ is a data point, and $s$ is a parameter that controls the width of the basis function. While this method aligns the basis functions with the data distribution, it can become computationally infeasible for large datasets because the number of basis functions equals the number of data points. Additionally, such models may overfit the training data unless appropriate regularization techniques are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks (DNN)\n",
    "\n",
    "**The fundamental concept of neural networks is to utilize basis functions $\\Phi(\\mathbf{x})$ that have learnable parameters, allowing both these parameters and the coefficients $\\theta$ to be optimized during the training process.**\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "Consider a neural network with an input vector $\\mathbf{x}$, an output vector $\\mathbf{y}$, and $K$ hidden layers denoted. The weights between layers are represented by matrices $\\boldsymbol{\\Omega}_k$, which pre-multiply the activations from the preceding layer to compute the pre-activations for the subsequent layer.\n",
    "\n",
    "The entire network can be expressed as a single function:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\boldsymbol{\\beta}_K + \\boldsymbol{\\Omega}_K \\mathbf{a}\\left[ \\boldsymbol{\\beta}_{K-1} + \\boldsymbol{\\Omega}_{K-1} \\mathbf{a}\\left[ \\ldots \\boldsymbol{\\beta}_2 + \\boldsymbol{\\Omega}_2 \\mathbf{a}\\left[ \\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\mathbf{a}\\left[ \\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x} \\right] \\right] \\ldots \\right] \\right]\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}[\\cdot]$ denotes the activation function applied element-wise. Also, bias vectors $\\boldsymbol{\\beta}_k$ are added at each layer, matching the dimension of the layer they feed into.\n",
    "\n",
    "Both deep and shallow networks are capable of modeling arbitrary functions, but certain functions can be approximated much more efficiently with deep networks. Specifically, some functions require a shallow network with an exponential number of hidden units to achieve the same level of approximation as a deep network. This phenomenon is known as the **depth efficiency** of neural networks.\n",
    "\n",
    "**Universal Approximation Theorem:** A neural network with a single hidden layer can approximate any continuous function from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ to within any desired degree of accuracy $\\epsilon$.\n",
    "\n",
    "## Why Deep Networks?\n",
    "\n",
    "Deeper networks—that is, networks with more than two layers—can sometimes represent complex functions more efficiently than shallow networks with only one or two layers. Research by Montúfar et al. has shown that a deep network's function can divide the input space into a number of regions that grows exponentially with the network's depth, while only increasing polynomially with the width of the hidden layers. To capture the same complexity using a two-layer network would require an exponential increase in the number of hidden units.\n",
    "\n",
    "While increasing the depth of a network enhances its capacity to model intricate functions, it also introduces challenges. In very deep networks, the infinite-width Gaussian approximation becomes less applicable due to the accumulation of finite-width fluctuations. Interestingly, this is beneficial because infinite-width networks lack neuron correlations within layers and cannot learn complex representations from input data. Practical deep learning systems rely on these correlations and representations.\n",
    "\n",
    "However, making networks extremely deep can lead to issues. As the overall depth $L$ becomes comparable to the width of the hidden layers, fluctuations can dominate the network's behavior. In such cases, observable quantities can vary widely between different instances of the network. Even with an optimal initialization hyperparameter (e.g., $C_W = 1$), some instances may experience exploding signals, others may suffer from vanishing signals, and only rarely will signals remain stable. Consequently, from a practical standpoint, excessively deep networks without proper architecture or initialization may not be effective.\n",
    "\n",
    "### Hierarchical Representations\n",
    "\n",
    "A compelling reason to explore deep neural networks is that their architecture encodes a form of inductive bias, suggesting that outputs are related to inputs through hierarchical representations. In deep convolutional neural networks trained on image data, for example, lower layers learn to detect simple features like edges and textures, while deeper layers capture more complex structures such as object parts or entire objects. This hierarchical composition allows the network to build up intricate features from simpler ones, mirroring the way humans perceive visual information.\n",
    "\n",
    "### Distributed Representations\n",
    "\n",
    "Deep networks also leverage **distributed representations**, where the activation of each neuron in a hidden layer represents a feature of the input data. High activations indicate the presence of certain features, while low activations indicate their absence. With $M$ neurons in a layer, a network can represent up to $2^M$ different combinations of features, allowing for an exponential increase in representational capacity as the number of neurons grows. This enables the network to capture complex patterns and dependencies in the data.\n",
    "\n",
    "### Representation Learning\n",
    "\n",
    "**Representation learning** refers to the ability of neural networks to automatically discover and extract meaningful features from raw data, transforming it into a form that makes subsequent tasks, such as classification or regression, easier to perform. The learned representation, often called an embedding space, is provided by the outputs of one of the hidden layers. Any input vector can be transformed into this space through forward propagation.\n",
    "\n",
    "Representation learning is especially powerful because it can utilize unlabeled data. Collecting large amounts of unlabeled data is often feasible, but obtaining labeled data can be time-consuming and expensive. By learning from unlabeled data—a process known as unsupervised learning—networks can uncover underlying structures and patterns within the data without explicit supervision.\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "The representations learned for one task can be beneficial for other related tasks—a concept known as **transfer learning**. For example, a network trained on a large labeled dataset of everyday objects learns features that are useful for object recognition in general. This pre-trained network can then be adapted to a different task, such as classifying medical images, by retraining only the final layers using a smaller labeled dataset. Transfer learning leverages shared features between tasks, resulting in higher accuracy compared to training a network from scratch on the smaller dataset.\n",
    "\n",
    "When data for the new task is scarce, it may be sufficient to retrain only the final layer of the network. If more data is available, retraining several layers—or even fine-tuning the entire network—can yield better performance. **Fine-tuning** involves adjusting the weights of the pre-trained network slightly, typically using a lower learning rate, to fit the new task without overfitting.\n",
    "\n",
    "### Multitask Learning and Meta-Learning\n",
    "\n",
    "**Multitask learning** involves training a network to perform multiple related tasks simultaneously, sharing representations across tasks to improve generalization. This approach can lead to better performance on each task compared to training separate networks.\n",
    "\n",
    "Extending this idea, **meta-learning**, or \"learning to learn,\" focuses on training models that can adapt quickly to new tasks with minimal data. Unlike multitask learning, which aims to make predictions on a fixed set of tasks, meta-learning prepares the model to handle tasks it hasn't seen during training. This is particularly useful in scenarios like **few-shot learning**, where the goal is to generalize to new classes with only a few labeled examples, or **one-shot learning**, where only a single example per class is available.\n",
    "\n",
    "### Contrastive Learning\n",
    "\n",
    "**Contrastive learning** is a powerful representation learning technique that aims to learn an embedding space where similar inputs are close together, and dissimilar inputs are far apart. The method involves:\n",
    "\n",
    "- **Positive pairs**: Inputs that are semantically similar and should be close in the embedding space.\n",
    "- **Negative pairs**: Inputs that are semantically dissimilar and should be distant in the embedding space.\n",
    "\n",
    "By selecting positive and negative pairs based on prior knowledge of what constitutes similarity, the network learns representations that cluster similar data points together. This approach simplifies downstream tasks like classification by making the data more separable.\n",
    "\n",
    "Contrastive learning differs from traditional machine learning tasks because the loss function for a given input is defined relative to other inputs rather than relying on explicit labels. The effectiveness of a contrastive learning algorithm largely depends on how positive and negative pairs are chosen.\n",
    "\n",
    "**Examples of Contrastive Learning Paradigms:**\n",
    "\n",
    "- **Instance Discrimination**: Uses an input (anchor) and an augmented version of the same input as a positive pair. Augmentations might include cropping, flipping, or color jittering. The network learns to associate the original and augmented inputs while distinguishing them from other instances.\n",
    "  \n",
    "- **Supervised Contrastive Learning**: Utilizes labeled data to form positive pairs from different inputs belonging to the same class. This method leverages class information to enhance the semantic consistency of the embedding space.\n",
    "\n",
    "- **Cross-Modal Contrastive Learning (e.g., CLIP Model)**: Involves positive pairs from different modalities, such as an image and its corresponding text description. The network learns to align representations across modalities, beneficial in tasks like image-caption retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions play a crucial role in neural networks by introducing non-linearity into the model, allowing the network to learn complex patterns in the data. Without activation functions, a neural network composed of linear transformations would be equivalent to a single linear transformation, regardless of the number of layers, limiting its ability to model intricate relationships.\n",
    "\n",
    "### Purpose of Activation Functions\n",
    "\n",
    "An activation function defines the output of a neuron given an input or set of inputs. It determines whether a neuron should be activated or not by calculating the weighted sum and adding bias to it. The introduction of non-linear activation functions enables neural networks to capture non-linear relationships between inputs and outputs, which is essential for tasks like image recognition, natural language processing, and other complex pattern recognition problems.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "Several activation functions have been developed over time, each with its advantages and disadvantages. Below are some of the most commonly used activation functions in neural networks.\n",
    "\n",
    "#### Sigmoid Function\n",
    "\n",
    "The **sigmoid** activation function maps input values into the range (0, 1) and is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Smooth gradient, preventing abrupt changes in weights.\n",
    "- Output values bound between 0 and 1, useful for probabilities.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Can suffer from the **vanishing gradient problem**, where gradients become very small, slowing down training.\n",
    "- Outputs are not zero-centered, which can affect gradient updates.\n",
    "\n",
    "#### Hyperbolic Tangent (Tanh) Function\n",
    "\n",
    "The **tanh** activation function maps input values to the range (-1, 1):\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Zero-centered outputs, which can help with optimization.\n",
    "- Steeper gradients than sigmoid, which can lead to faster convergence.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Also susceptible to the vanishing gradient problem.\n",
    "- Computationally expensive due to exponential calculations.\n",
    "\n",
    "#### Rectified Linear Unit (ReLU)\n",
    "\n",
    "The **Rectified Linear Unit (ReLU)** is defined as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Computationally efficient; simple mathematical operations.\n",
    "- Mitigates vanishing gradient problem, enabling the training of deep networks.\n",
    "- Encourages sparse activations (many neurons output zero), which can improve model efficiency.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Can suffer from the **\"dying ReLU\" problem**, where neurons permanently output zero.\n",
    "- Outputs are not zero-centered.\n",
    "\n",
    "#### Leaky ReLU\n",
    "\n",
    "The **Leaky ReLU** addresses the dying ReLU problem by allowing a small, non-zero gradient when the input is negative:\n",
    "\n",
    "$$\n",
    "\\text{Leaky ReLU}(x) = \\begin{cases}\n",
    "x, & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha x, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a small constant (e.g., 0.01).\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Prevents neurons from dying by maintaining a small gradient for negative inputs.\n",
    "- Retains the benefits of ReLU for positive inputs.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- The slope for negative inputs is a hyperparameter that needs tuning.\n",
    "- Still not entirely zero-centered.\n",
    "\n",
    "#### Parametric ReLU (PReLU)\n",
    "\n",
    "The **Parametric ReLU** extends Leaky ReLU by making $\\alpha$ a learnable parameter:\n",
    "\n",
    "$$\n",
    "\\text{PReLU}(x) = \\begin{cases}\n",
    "x, & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha x, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Adaptively learns the slope for negative inputs.\n",
    "- Can improve model performance by adjusting $\\alpha$ during training.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Increases the number of parameters to learn.\n",
    "- Risk of overfitting if not properly regularized.\n",
    "\n",
    "#### Exponential Linear Unit (ELU)\n",
    "\n",
    "The **ELU** activation function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases}\n",
    "x, & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha (e^{x} - 1), & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Mitigates vanishing gradients with negative values.\n",
    "- Outputs can be negative, bringing mean activation closer to zero, which can speed up learning.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- More computationally intensive due to the exponential function.\n",
    "- Introduces an additional hyperparameter $\\alpha$.\n",
    "\n",
    "#### Swish Function\n",
    "\n",
    "The **Swish** activation function is a smooth, non-monotonic function defined as:\n",
    "\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\sigma(\\beta x)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\beta$ is a parameter (often set to 1).\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Has shown to outperform ReLU in some deep networks.\n",
    "- Smooth function that may improve training robustness.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Computationally more complex than ReLU.\n",
    "- Benefits may vary depending on the specific application.\n",
    "\n",
    "### Comparison and Selection of Activation Functions\n",
    "\n",
    "When choosing an activation function, consider the following factors:\n",
    "\n",
    "- **Non-linearity**: The function should introduce non-linearity to capture complex patterns.\n",
    "- **Computational Efficiency**: Functions that are easy to compute can reduce training time.\n",
    "- **Gradient Behavior**: Functions that avoid vanishing or exploding gradients facilitate efficient training, especially in deep networks.\n",
    "- **Output Range**: Functions with outputs centered around zero can simplify optimization.\n",
    "- **Sparsity**: Functions that produce sparse activations can improve computational efficiency and may enhance generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "In the context of **Maximum Likelihood Estimation (MLE)**, the goal of training a neural network is to find the parameters $\\boldsymbol{\\theta}$ that maximize the likelihood of the observed data $\\mathcal{D} = \\{(\\mathbf{x}_n, \\mathbf{y}_n)\\}_{n=1}^N$. This is equivalent to minimizing the negative log-likelihood (NLL) loss function over the training dataset:\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(\\boldsymbol{\\theta}) = -\\log p(\\mathcal{D} \\mid \\boldsymbol{\\theta}) = -\\sum_{n=1}^N \\log p\\left(\\mathbf{y}_n \\mid \\mathbf{x}_n, \\boldsymbol{\\theta}\\right)\n",
    "$$\n",
    "\n",
    "Here, $p\\left(\\mathbf{y}_n \\mid \\mathbf{x}_n, \\boldsymbol{\\theta}\\right)$ represents the probability of observing the target $\\mathbf{y}_n$ given the input $\\mathbf{x}_n$ and the model parameters $\\boldsymbol{\\theta}$.\n",
    "\n",
    "### Selecting an Appropriate Probability Distribution\n",
    "\n",
    "Choosing the correct probability distribution for $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ is crucial, as it should align with the nature of the prediction target $\\mathbf{y}$. Below summarizes various data types, their domains, suitable probability distributions, and common use cases:\n",
    "\n",
    "- **Univariate Continuous Unbounded (Regression):**\n",
    "  - **Domain:** $y \\in \\mathbb{R}$\n",
    "  - **Distribution:** **Univariate Normal (Gaussian)**\n",
    "  - **Use Case:** Standard regression tasks where the output is any real number.\n",
    "\n",
    "- **Multivariate Regression:**\n",
    "  - **Domain:** $\\mathbf{y} \\in \\mathbb{R}^K$\n",
    "  - **Distribution:** **Multivariate Normal**\n",
    "  - **Use Case:** Predicting multiple continuous outputs simultaneously, considering their correlations.\n",
    "\n",
    "- **Binary Classification:**\n",
    "  - **Domain:** $y \\in \\{0, 1\\}$\n",
    "  - **Distribution:** **Bernoulli**\n",
    "  - **Use Case:** Classifying data into two distinct classes.\n",
    "\n",
    "- **Multiclass Classification:**\n",
    "  - **Domain:** $y \\in \\{1, 2, \\ldots, K\\}$\n",
    "  - **Distribution:** **Categorical (Multinomial)**\n",
    "  - **Use Case:** Assigning inputs to one of multiple categories.\n",
    "\n",
    "### Implementing the Loss Function Based on the Distribution\n",
    "\n",
    "Once the appropriate distribution is selected, the negative log-likelihood loss function can be derived accordingly. Here are examples for some common cases:\n",
    "\n",
    "#### Regression with the Normal Distribution\n",
    "\n",
    "Assuming the target variable follows a normal distribution with mean $\\hat{y}_n$ and variance $\\sigma^2$, the likelihood for a single observation is:\n",
    "\n",
    "$$\n",
    "p\\left(y_n \\mid \\mathbf{x}_n, \\boldsymbol{\\theta}\\right) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{\\left(y_n - \\hat{y}_n\\right)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "The negative log-likelihood (excluding constants) simplifies to the Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(\\boldsymbol{\\theta}) = \\sum_{n=1}^N \\frac{\\left(y_n - \\hat{y}_n\\right)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "Minimizing the NLL in this case is equivalent to minimizing the MSE between the predictions $\\hat{y}_n$ and the true values $y_n$.\n",
    "\n",
    "#### Binary Classification with the Bernoulli Distribution\n",
    "\n",
    "For binary outcomes where $y_n \\in \\{0, 1\\}$, and the model outputs a probability $\\hat{y}_n = p\\left(y_n = 1 \\mid \\mathbf{x}_n, \\boldsymbol{\\theta}\\right)$, the likelihood is:\n",
    "\n",
    "$$\n",
    "p\\left(y_n \\mid \\mathbf{x}_n, \\boldsymbol{\\theta}\\right) = \\hat{y}_n^{y_n} (1 - \\hat{y}_n)^{1 - y_n}\n",
    "$$\n",
    "\n",
    "The negative log-likelihood becomes the **binary cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(\\boldsymbol{\\theta}) = -\\sum_{n=1}^N \\left[ y_n \\log \\hat{y}_n + (1 - y_n) \\log (1 - \\hat{y}_n) \\right]\n",
    "$$\n",
    "\n",
    "This loss encourages the model to produce probabilities close to 1 for positive instances ($y_n = 1$) and close to 0 for negative instances ($y_n = 0$).\n",
    "\n",
    "#### Multiclass Classification with the Categorical Distribution\n",
    "\n",
    "For multiclass classification where $y_n \\in \\{1, 2, \\ldots, K\\}$, and the model outputs a probability vector $\\hat{\\mathbf{y}}_n = [\\hat{y}_{n1}, \\hat{y}_{n2}, \\ldots, \\hat{y}_{nK}]$, the likelihood is:\n",
    "\n",
    "$$\n",
    "p\\left(y_n \\mid \\mathbf{x}_n, \\boldsymbol{\\theta}\\right) = \\prod_{k=1}^K \\hat{y}_{nk}^{\\mathbb{I}[y_n = k]}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{I}[y_n = k]$ is the indicator function that is 1 when $y_n = k$ and 0 otherwise.\n",
    "\n",
    "The negative log-likelihood, known as the **cross-entropy loss**, is:\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(\\boldsymbol{\\theta}) = -\\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{I}[y_n = k] \\log \\hat{y}_{nk}\n",
    "$$\n",
    "\n",
    "Minimizing this loss pushes the model to assign higher probabilities to the correct class for each instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Using Gradient Descent\n",
    "\n",
    "In the framework of **Empirical Risk Minimization (ERM)**, we aim to find the model parameters that minimize a chosen loss function over the training data. While **Maximum Likelihood Estimation (MLE)** uses the negative log-likelihood loss, ERM generalizes this by allowing any suitable loss function $\\ell$. The overall loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{n=1}^N \\ell\\left( \\mathbf{y}_n, \\boldsymbol{\\theta}; \\mathbf{x}_n \\right)\n",
    "$$\n",
    "\n",
    "Our objective is to find the parameters $\\boldsymbol{\\theta}$ that minimize this loss:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "*Refer to the \"Optimization\" section in the SC Notebook for more details on optimization techniques.*\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "One of the most fundamental optimization algorithms is **Gradient Descent**. It iteratively updates the model parameters in the direction that reduces the loss function the most. The algorithm proceeds as follows:\n",
    "\n",
    "1. **Initialize** the parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^N$ with some initial values.\n",
    "\n",
    "2. **Iterate** the following steps until convergence:\n",
    "\n",
    "   **Step 1:** Compute the gradient of the loss function with respect to the parameters:\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L} = \\left[ \\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}, \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2}, \\dots, \\frac{\\partial \\mathcal{L}}{\\partial \\theta_N} \\right]^\\top\n",
    "   $$\n",
    "\n",
    "   **Step 2:** Update the parameters in the opposite direction of the gradient:\n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha \\cdot \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\n",
    "   $$\n",
    "\n",
    "   - $\\alpha$ is the **learning rate**, a positive scalar that determines the size of the update step.\n",
    "\n",
    "In **Step 1**, the gradient vector $\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}$ indicates the direction of the steepest increase of the loss function. By moving in the opposite direction, we aim to decrease the loss.\n",
    "\n",
    "The learning rate $\\alpha$ can be fixed or adjusted during the optimization process. If chosen too large, the algorithm might overshoot the minimum; if too small, the convergence can be very slow.\n",
    "\n",
    "### Limitations of Gradient Descent\n",
    "\n",
    "- **Dependence on Initialization:** The convergence point is heavily influenced by the initial parameter values, potentially leading to convergence at a local minimum rather than the global minimum.\n",
    "\n",
    "- **Computational Cost:** Computing the gradient over the entire dataset at each iteration can be computationally expensive, especially with large datasets.\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "To mitigate the limitations of standard gradient descent, **Stochastic Gradient Descent (SGD)** introduces randomness into the optimization process by using only a subset of the data, called a **minibatch**, to compute the gradient at each iteration.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "At each iteration $t$, the steps are:\n",
    "\n",
    "1. **Sample a Minibatch:** Randomly select a subset $\\mathcal{B}_t$ of the training data.\n",
    "\n",
    "2. **Compute the Gradient on the Minibatch:**\n",
    "\n",
    "   $$\n",
    "   \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathcal{B}_t} = \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\frac{\\partial \\ell\\left( \\mathbf{y}_i, \\boldsymbol{\\theta}; \\mathbf{x}_i \\right)}{\\partial \\boldsymbol{\\theta}}\n",
    "   $$\n",
    "\n",
    "3. **Update the Parameters:**\n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\cdot \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathcal{B}_t}\n",
    "   $$\n",
    "\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $|\\mathcal{B}_t|$ is the size of the minibatch.\n",
    "\n",
    "### Batches and Epochs\n",
    "\n",
    "- **Minibatch:** A small subset of the training data used to compute the gradient at each iteration.\n",
    "- **Epoch:** One complete pass through the entire training dataset.\n",
    "\n",
    "Typically, the data is shuffled at the beginning of each epoch to ensure randomness in the minibatches.\n",
    "\n",
    "### Advantages of SGD\n",
    "\n",
    "- **Computational Efficiency:** Computing the gradient over a minibatch is faster than over the full dataset.\n",
    "- **Ability to Escape Local Minima:** The randomness can help the algorithm jump out of local minima or saddle points.\n",
    "- **Online Learning Capability:** SGD can be used in settings where data arrives sequentially.\n",
    "\n",
    "### Challenges with SGD\n",
    "\n",
    "- **Noisy Updates:** The parameter updates can be noisy due to the randomness in minibatch selection.\n",
    "- **Learning Rate Sensitivity:** Choosing an appropriate learning rate is crucial; too high can cause divergence, too low can slow convergence.\n",
    "\n",
    "## Properties of SGD\n",
    "\n",
    "- **Sensible Updates:** Even though each update is based on a subset of data, on average, the updates lead to a reduction in the loss function.\n",
    "- **Equal Contribution:** Over multiple epochs, all training examples contribute equally to the training process.\n",
    "- **Reduced Computation per Update:** Each update step is faster since it uses fewer data points.\n",
    "- **Potential for Better Generalization:** The inherent noise in SGD can help the model generalize better to new data.\n",
    "\n",
    "SGD may not converge in the traditional sense but tends to hover around the region of the global minimum when properly tuned.\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "\n",
    "To improve convergence:\n",
    "\n",
    "- **Learning Rate Decay:** Start with a higher learning rate and gradually decrease it over time.\n",
    "- **Adaptive Learning Rates:** Adjust the learning rate based on the progress of training or use algorithms that adaptively change the learning rate.\n",
    "\n",
    "## Parameter Initialization and Symmetry Breaking\n",
    "\n",
    "Proper initialization of network parameters is essential:\n",
    "\n",
    "- **Symmetry Breaking:** Initializing all weights to the same value (e.g., zeros) can cause neurons to learn the same features, rendering some redundant.\n",
    "\n",
    "- **Random Initialization:** Assigning small random values to weights ensures that neurons compute different functions and learn diverse features.\n",
    "\n",
    "## Momentum Optimization\n",
    "\n",
    "**Momentum** is a technique that helps accelerate SGD in relevant directions, dampening oscillations in the optimization path.\n",
    "\n",
    "### Algorithm with Momentum\n",
    "\n",
    "1. **Initialize Momentum Vector:** Set $\\mathbf{m}_0 = \\mathbf{0}$.\n",
    "\n",
    "2. **Iterate:**\n",
    "\n",
    "   - **Update the Momentum:**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{m}_{t+1} \\leftarrow \\beta \\cdot \\mathbf{m}_t + (1 - \\beta) \\cdot \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathcal{B}_t}\n",
    "     $$\n",
    "\n",
    "     - $\\beta \\in [0, 1)$ is the momentum coefficient.\n",
    "\n",
    "   - **Update the Parameters:**\n",
    "\n",
    "     $$\n",
    "     \\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\cdot \\mathbf{m}_{t+1}\n",
    "     $$\n",
    "\n",
    "### Benefits of Momentum\n",
    "\n",
    "- **Accelerated Convergence:** Accumulates updates in directions with persistent gradients.\n",
    "- **Reduced Oscillations:** Helps smooth out updates, especially in regions where the gradient changes direction frequently.\n",
    "- **Adaptive:** The effective step size increases in low-curvature directions and decreases in high-curvature directions.\n",
    "\n",
    "By incorporating momentum, the optimizer can navigate ravines in the loss landscape more effectively.\n",
    "\n",
    "## Adaptive Moment Estimation (ADAM)\n",
    "\n",
    "**ADAM** is an optimization algorithm that combines the advantages of both momentum and adaptive learning rates.\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **Initialize:**\n",
    "\n",
    "   - $\\mathbf{m}_0 = \\mathbf{0}$ (First moment estimate)\n",
    "   - $\\mathbf{v}_0 = \\mathbf{0}$ (Second moment estimate)\n",
    "   - $t = 0$ (Time step)\n",
    "\n",
    "2. **Iterate:**\n",
    "\n",
    "   - **Increment Time Step:**\n",
    "\n",
    "     $$\n",
    "     t \\leftarrow t + 1\n",
    "     $$\n",
    "\n",
    "   - **Compute Gradients:**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{g}_t = \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\mathcal{B}_t}\n",
    "     $$\n",
    "\n",
    "   - **Update Biased First Moment Estimate:**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{m}_t \\leftarrow \\beta_1 \\cdot \\mathbf{m}_{t-1} + (1 - \\beta_1) \\cdot \\mathbf{g}_t\n",
    "     $$\n",
    "\n",
    "   - **Update Biased Second Moment Estimate:**\n",
    "\n",
    "     $$\n",
    "     \\mathbf{v}_t \\leftarrow \\beta_2 \\cdot \\mathbf{v}_{t-1} + (1 - \\beta_2) \\cdot \\mathbf{g}_t^2\n",
    "     $$\n",
    "\n",
    "   - **Compute Bias-Corrected Moment Estimates:**\n",
    "\n",
    "     $$\n",
    "     \\hat{\\mathbf{m}}_t = \\frac{\\mathbf{m}_t}{1 - \\beta_1^t}\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_2^t}\n",
    "     $$\n",
    "\n",
    "   - **Update Parameters:**\n",
    "\n",
    "     $$\n",
    "     \\boldsymbol{\\theta}_{t+1} \\leftarrow \\boldsymbol{\\theta}_t - \\alpha \\cdot \\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon}\n",
    "     $$\n",
    "\n",
    "     - $\\epsilon$ is a small constant to prevent division by zero (e.g., $10^{-8}$).\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **First Moment ($\\mathbf{m}_t$):** Estimates the mean of the gradients (analogous to momentum).\n",
    "- **Second Moment ($\\mathbf{v}_t$):** Estimates the uncentered variance (squared gradients).\n",
    "- **Bias Correction:** Accounts for the initialization at zero, especially important during the initial iterations.\n",
    "- **Adaptive Learning Rates:** Dividing by the square root of the second moment scales the learning rate inversely with the magnitude of recent gradients.\n",
    "\n",
    "### Advantages of ADAM\n",
    "\n",
    "- **Combines Momentum and RMSProp:** Incorporates both gradient history and adaptive learning rates.\n",
    "- **Robust to Sparse Gradients:** Effective when gradients are sparse or noisy.\n",
    "- **Less Sensitive to Hyperparameters:** Generally works well with default settings (e.g., $\\beta_1=0.9$, $\\beta_2=0.999$).\n",
    "\n",
    "### Handling Different Gradient Scales\n",
    "\n",
    "In situations where the loss function varies steeply in some directions and slowly in others, standard gradient descent can struggle:\n",
    "\n",
    "- **Fixed Learning Rate Issues:**\n",
    "  - A large learning rate may cause divergence in steep directions.\n",
    "  - A small learning rate slows convergence in flat directions.\n",
    "\n",
    "ADAM adjusts the learning rate for each parameter individually, allowing for efficient progress in all directions simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance\n",
    "\n",
    "A common method for evaluating the performance of a machine learning algorithm is to use separate **training and testing datasets**. We split the data into a training set, which the algorithm learns from, and a testing set, which we use to assess the model's performance. This approach helps us evaluate how well the model generalizes to new, unseen data.\n",
    "\n",
    "For example, during training, we might observe that the error on the training set decreases steadily towards zero. However, if the error on the test set remains high or starts increasing, it indicates that the model is overfitting the training data and not generalizing well to new data. This means the model is learning patterns specific to the training data that do not apply to unseen data.\n",
    "\n",
    "**Cross-Validation**: When the available data is limited, we want to make the most of it for both training and evaluating our model. Cross-validation addresses this by partitioning the data into multiple subsets or \"folds.\" In each round, one fold is used as the validation set, and the remaining folds are used for training. This process is repeated so that each fold is used as the validation set once. Cross-validation allows us to use all the data for both training and validation, providing a more reliable estimate of the model's performance. The main drawback is the increased computational cost due to multiple training runs, which can be challenging for computationally intensive models.\n",
    "\n",
    "**Validation Sets**: To fine-tune the hyperparameters of our model, we introduce a third dataset called the validation set. We train the model on the training set with different hyperparameter settings and evaluate its performance on the validation set. We select the model that performs best on the validation set. Finally, we assess the selected model's performance on the test set to estimate its true predictive ability. This process helps prevent overfitting to the validation data and provides a more accurate measure of how the model will perform on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Neural Network Application\n",
    "\n",
    "Consider the following differential equation modeling the energy balance of a body immersed in water:\n",
    "$$\n",
    "\\frac{dT(t)}{dt} + k \\, T(t) - f(t) = 0,\n",
    "$$\n",
    "where:\n",
    "\n",
    "- $T(t)$ is the temperature at time $t$,\n",
    "- $k = 0.5$,\n",
    "- $f(t) = \\exp(5 \\sin(2t))$,\n",
    "- with the initial condition $T(0) = -5$,\n",
    "- and $t$ ranges from 0 to 10.\n",
    "\n",
    "To train our neural network, we generate training and testing datasets by sampling from the solution of this equation. We introduce zero-mean Gaussian noise $\\eta$ (i.e., $\\eta \\sim \\mathcal{N}(0, \\sigma^2)$, where $\\sigma^2$ is the noise variance) to the training data $Y_{\\text{train}}$. The neural network is then trained to predict the temperature $T(t)$ based on the time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "Nt = 5000  # Number of time points\n",
    "tf = 10    # Final time\n",
    "\n",
    "key = random.PRNGKey(0)  # Random key for reproducibility\n",
    "\n",
    "N_data_train = 50  # Number of training data points\n",
    "N_data_test = 10   # Number of test data points\n",
    "\n",
    "N_physics = 1000   # Number of physics-based data points (not used in this snippet)\n",
    "\n",
    "sigma = 2  # Standard deviation of noise\n",
    "\n",
    "# Define the right-hand side (RHS) of the differential equation\n",
    "def RHS(t, y):\n",
    "    return -0.5 * y + jnp.exp(5 * jnp.sin(2 * t))\n",
    "\n",
    "# Solve the differential equation\n",
    "sol = solve_ivp(RHS, [0, tf], [-5], t_eval=jnp.linspace(0, tf, Nt))\n",
    "\n",
    "# Extract the solution\n",
    "t = jnp.array(sol.t).reshape(-1, 1)  # Time points\n",
    "y = jnp.array(sol.y[0, :])           # Solution values\n",
    "\n",
    "# Select random training data points\n",
    "I_data_train = random.choice(key, Nt, (N_data_train,))\n",
    "X_data_train = t[I_data_train]\n",
    "Y_data_train = (y[I_data_train] + sigma * random.normal(key, (N_data_train,))).reshape(-1, 1)\n",
    "\n",
    "# Select random test data points\n",
    "I_data_test = random.choice(key, Nt, (N_data_test,))\n",
    "X_data_test = t[I_data_test]\n",
    "Y_data_test = y[I_data_test].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :     0 - Train MSE : 319.95 - Test  MSE : 167.64\n",
      "iteration :  5000 - Train MSE : 72.51 - Test  MSE : 49.63\n",
      "iteration : 10000 - Train MSE : 44.31 - Test  MSE : 48.89\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"568.8pt\" height=\"280.8pt\" viewBox=\"0 0 568.8 280.8\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-11-01T10:52:56.945515</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 280.8 \n",
       "L 568.8 280.8 \n",
       "L 568.8 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 243.24375 \n",
       "L 275.81875 243.24375 \n",
       "L 275.81875 22.318125 \n",
       "L 40.603125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mc8502393ed\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"51.294744\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(48.113494 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-30\" d=\"M 2034 219 \n",
       "Q 2513 219 2750 744 \n",
       "Q 2988 1269 2988 2328 \n",
       "Q 2988 3391 2750 3916 \n",
       "Q 2513 4441 2034 4441 \n",
       "Q 1556 4441 1318 3916 \n",
       "Q 1081 3391 1081 2328 \n",
       "Q 1081 1269 1318 744 \n",
       "Q 1556 219 2034 219 \n",
       "z\n",
       "M 2034 -91 \n",
       "Q 1275 -91 848 546 \n",
       "Q 422 1184 422 2328 \n",
       "Q 422 3475 848 4112 \n",
       "Q 1275 4750 2034 4750 \n",
       "Q 2797 4750 3222 4112 \n",
       "Q 3647 3475 3647 2328 \n",
       "Q 3647 1184 3222 546 \n",
       "Q 2797 -91 2034 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"94.061222\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(90.879972 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-32\" d=\"M 819 3553 \n",
       "L 469 3553 \n",
       "L 469 4384 \n",
       "Q 803 4563 1142 4656 \n",
       "Q 1481 4750 1806 4750 \n",
       "Q 2534 4750 2956 4397 \n",
       "Q 3378 4044 3378 3438 \n",
       "Q 3378 2753 2422 1800 \n",
       "Q 2347 1728 2309 1691 \n",
       "L 1131 513 \n",
       "L 3078 513 \n",
       "L 3078 1088 \n",
       "L 3444 1088 \n",
       "L 3444 0 \n",
       "L 434 0 \n",
       "L 434 341 \n",
       "L 1850 1753 \n",
       "Q 2319 2222 2519 2614 \n",
       "Q 2719 3006 2719 3438 \n",
       "Q 2719 3909 2473 4175 \n",
       "Q 2228 4441 1797 4441 \n",
       "Q 1350 4441 1106 4219 \n",
       "Q 863 3997 819 3553 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"136.827699\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(133.646449 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-34\" d=\"M 2234 1581 \n",
       "L 2234 4063 \n",
       "L 641 1581 \n",
       "L 2234 1581 \n",
       "z\n",
       "M 3609 0 \n",
       "L 1484 0 \n",
       "L 1484 331 \n",
       "L 2234 331 \n",
       "L 2234 1247 \n",
       "L 197 1247 \n",
       "L 197 1588 \n",
       "L 2241 4750 \n",
       "L 2859 4750 \n",
       "L 2859 1581 \n",
       "L 3750 1581 \n",
       "L 3750 1247 \n",
       "L 2859 1247 \n",
       "L 2859 331 \n",
       "L 3609 331 \n",
       "L 3609 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"179.594176\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(176.412926 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-36\" d=\"M 2094 219 \n",
       "Q 2534 219 2771 542 \n",
       "Q 3009 866 3009 1472 \n",
       "Q 3009 2078 2771 2401 \n",
       "Q 2534 2725 2094 2725 \n",
       "Q 1647 2725 1412 2412 \n",
       "Q 1178 2100 1178 1509 \n",
       "Q 1178 888 1415 553 \n",
       "Q 1653 219 2094 219 \n",
       "z\n",
       "M 1075 2569 \n",
       "Q 1288 2803 1556 2918 \n",
       "Q 1825 3034 2163 3034 \n",
       "Q 2859 3034 3264 2615 \n",
       "Q 3669 2197 3669 1472 \n",
       "Q 3669 763 3233 336 \n",
       "Q 2797 -91 2069 -91 \n",
       "Q 1278 -91 853 498 \n",
       "Q 428 1088 428 2181 \n",
       "Q 428 3406 931 4078 \n",
       "Q 1434 4750 2350 4750 \n",
       "Q 2597 4750 2869 4703 \n",
       "Q 3141 4656 3425 4563 \n",
       "L 3425 3794 \n",
       "L 3072 3794 \n",
       "Q 3034 4109 2831 4275 \n",
       "Q 2628 4441 2284 4441 \n",
       "Q 1678 4441 1381 3981 \n",
       "Q 1084 3522 1075 2569 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"222.360653\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(219.179403 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-38\" d=\"M 2981 1275 \n",
       "Q 2981 1775 2732 2051 \n",
       "Q 2484 2328 2034 2328 \n",
       "Q 1584 2328 1336 2051 \n",
       "Q 1088 1775 1088 1275 \n",
       "Q 1088 772 1336 495 \n",
       "Q 1584 219 2034 219 \n",
       "Q 2484 219 2732 495 \n",
       "Q 2981 772 2981 1275 \n",
       "z\n",
       "M 2853 3541 \n",
       "Q 2853 3966 2637 4203 \n",
       "Q 2422 4441 2034 4441 \n",
       "Q 1650 4441 1433 4203 \n",
       "Q 1216 3966 1216 3541 \n",
       "Q 1216 3113 1433 2875 \n",
       "Q 1650 2638 2034 2638 \n",
       "Q 2422 2638 2637 2875 \n",
       "Q 2853 3113 2853 3541 \n",
       "z\n",
       "M 2516 2484 \n",
       "Q 3047 2413 3344 2092 \n",
       "Q 3641 1772 3641 1275 \n",
       "Q 3641 619 3225 264 \n",
       "Q 2809 -91 2034 -91 \n",
       "Q 1263 -91 845 264 \n",
       "Q 428 619 428 1275 \n",
       "Q 428 1772 725 2092 \n",
       "Q 1022 2413 1556 2484 \n",
       "Q 1084 2569 832 2842 \n",
       "Q 581 3116 581 3541 \n",
       "Q 581 4103 968 4426 \n",
       "Q 1356 4750 2034 4750 \n",
       "Q 2713 4750 3100 4426 \n",
       "Q 3488 4103 3488 3541 \n",
       "Q 3488 3116 3236 2842 \n",
       "Q 2984 2569 2516 2484 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"265.127131\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(258.764631 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-31\" d=\"M 909 0 \n",
       "L 909 331 \n",
       "L 1722 331 \n",
       "L 1722 4213 \n",
       "L 781 3603 \n",
       "L 781 4013 \n",
       "L 1919 4750 \n",
       "L 2350 4750 \n",
       "L 2350 331 \n",
       "L 3163 331 \n",
       "L 3163 0 \n",
       "L 909 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- $t$ -->\n",
       "     <g transform=\"translate(156.210938 271.520312) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Oblique-74\" d=\"M 2706 3500 \n",
       "L 2619 3053 \n",
       "L 1472 3053 \n",
       "L 1100 1153 \n",
       "Q 1081 1047 1072 975 \n",
       "Q 1063 903 1063 863 \n",
       "Q 1063 663 1183 572 \n",
       "Q 1303 481 1569 481 \n",
       "L 2150 481 \n",
       "L 2053 0 \n",
       "L 1503 0 \n",
       "Q 991 0 739 200 \n",
       "Q 488 400 488 806 \n",
       "Q 488 878 497 964 \n",
       "Q 506 1050 525 1153 \n",
       "L 897 3053 \n",
       "L 409 3053 \n",
       "L 500 3500 \n",
       "L 978 3500 \n",
       "L 1172 4494 \n",
       "L 1747 4494 \n",
       "L 1556 3500 \n",
       "L 2706 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-74\" transform=\"translate(0 0.78125)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"mb0294f6553\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"40.603125\" y=\"220.869692\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.240625 224.668911) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"40.603125\" y=\"176.005981\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(20.878125 179.8052) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"40.603125\" y=\"131.142271\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(20.878125 134.941489) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"40.603125\" y=\"86.27856\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(20.878125 90.077779) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"40.603125\" y=\"41.414849\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(20.878125 45.214068) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- $T$ -->\n",
       "     <g transform=\"translate(14.798438 135.880937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Oblique-54\" d=\"M 378 4666 \n",
       "L 4325 4666 \n",
       "L 4225 4134 \n",
       "L 2559 4134 \n",
       "L 1759 0 \n",
       "L 1125 0 \n",
       "L 1925 4134 \n",
       "L 275 4134 \n",
       "L 378 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-54\" transform=\"translate(0 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 51.294744 232.08562 \n",
       "L 53.305171 231.228102 \n",
       "L 54.673972 230.4128 \n",
       "L 55.700573 229.554866 \n",
       "L 56.598848 228.524048 \n",
       "L 57.454349 227.201106 \n",
       "L 58.309849 225.439481 \n",
       "L 59.16535 223.12015 \n",
       "L 60.020851 220.10934 \n",
       "L 60.876352 216.258504 \n",
       "L 61.774627 211.132158 \n",
       "L 62.672902 204.688971 \n",
       "L 63.656729 195.936583 \n",
       "L 64.85443 183.141237 \n",
       "L 66.394331 164.060704 \n",
       "L 71.014035 104.976742 \n",
       "L 72.040635 95.557842 \n",
       "L 72.938912 89.220773 \n",
       "L 73.794411 84.688014 \n",
       "L 74.564361 81.744731 \n",
       "L 75.248761 79.946248 \n",
       "L 75.847612 78.944634 \n",
       "L 76.360913 78.472301 \n",
       "L 76.83144 78.324666 \n",
       "L 77.301964 78.425383 \n",
       "L 77.815265 78.790367 \n",
       "L 78.414115 79.512063 \n",
       "L 79.184065 80.831435 \n",
       "L 80.167891 83.000251 \n",
       "L 81.836119 87.324654 \n",
       "L 85.386445 97.177226 \n",
       "L 89.749497 109.09966 \n",
       "L 92.829302 116.850212 \n",
       "L 96.208526 124.729283 \n",
       "L 99.502204 131.849333 \n",
       "L 102.83866 138.53198 \n",
       "L 106.175111 144.720558 \n",
       "L 109.468788 150.369801 \n",
       "L 112.719692 155.507948 \n",
       "L 115.84227 160.031328 \n",
       "L 118.323219 163.277069 \n",
       "L 120.376422 165.582999 \n",
       "L 121.830774 166.919221 \n",
       "L 122.857375 167.586428 \n",
       "L 123.670099 167.863678 \n",
       "L 124.354502 167.864918 \n",
       "L 124.953352 167.647714 \n",
       "L 125.552202 167.185042 \n",
       "L 126.151053 166.432854 \n",
       "L 126.792677 165.250959 \n",
       "L 127.47708 163.490482 \n",
       "L 128.204251 160.961636 \n",
       "L 128.974206 157.424287 \n",
       "L 129.701377 153.033593 \n",
       "L 130.599652 146.114608 \n",
       "L 131.711806 135.686017 \n",
       "L 133.208932 119.350538 \n",
       "L 138.213614 62.860177 \n",
       "L 139.325757 53.510033 \n",
       "L 140.224033 47.685781 \n",
       "L 140.951214 44.32535 \n",
       "L 141.592838 42.373515 \n",
       "L 142.148915 41.299123 \n",
       "L 142.662218 40.758145 \n",
       "L 143.089964 40.60304 \n",
       "L 143.51771 40.689278 \n",
       "L 143.98824 41.033871 \n",
       "L 144.544317 41.737862 \n",
       "L 145.228714 42.979699 \n",
       "L 146.12699 45.11308 \n",
       "L 147.324691 48.59651 \n",
       "L 149.121242 54.594916 \n",
       "L 157.547922 83.517509 \n",
       "L 160.841599 93.700752 \n",
       "L 164.006956 102.778994 \n",
       "L 167.215086 111.303987 \n",
       "L 170.46599 119.312647 \n",
       "L 173.759667 126.831431 \n",
       "L 177.138891 133.996697 \n",
       "L 180.347022 140.291186 \n",
       "L 183.255716 145.522587 \n",
       "L 185.822223 149.692231 \n",
       "L 187.960973 152.766625 \n",
       "L 189.287004 154.323332 \n",
       "L 190.270827 155.153227 \n",
       "L 191.040772 155.526405 \n",
       "L 191.682406 155.601361 \n",
       "L 192.281257 155.438764 \n",
       "L 192.837323 155.053944 \n",
       "L 193.436174 154.350586 \n",
       "L 194.077808 153.217604 \n",
       "L 194.762205 151.51538 \n",
       "L 195.489377 149.06946 \n",
       "L 196.259332 145.66336 \n",
       "L 197.07206 141.030286 \n",
       "L 197.927552 134.834015 \n",
       "L 198.868611 126.212695 \n",
       "L 200.151859 112.195694 \n",
       "L 205.113757 55.706412 \n",
       "L 206.225911 46.410469 \n",
       "L 207.16696 40.260723 \n",
       "L 207.936915 36.528504 \n",
       "L 208.578539 34.366063 \n",
       "L 209.134616 33.182157 \n",
       "L 209.647919 32.567965 \n",
       "L 210.075665 32.368876 \n",
       "L 210.503411 32.425541 \n",
       "L 210.973941 32.752986 \n",
       "L 211.530017 33.455693 \n",
       "L 212.214415 34.721078 \n",
       "L 213.069917 36.804675 \n",
       "L 214.224844 40.263974 \n",
       "L 215.935848 46.190552 \n",
       "L 224.918605 78.334096 \n",
       "L 228.212282 88.896725 \n",
       "L 231.420412 98.44179 \n",
       "L 234.585759 107.17081 \n",
       "L 237.836652 115.485119 \n",
       "L 241.087566 123.186605 \n",
       "L 244.424007 130.527505 \n",
       "L 247.632137 137.065999 \n",
       "L 250.540842 142.510754 \n",
       "L 253.107338 146.862651 \n",
       "L 255.203315 150.0166 \n",
       "L 256.486583 151.593454 \n",
       "L 257.470406 152.469831 \n",
       "L 258.240351 152.872562 \n",
       "L 258.881985 152.968101 \n",
       "L 259.438052 152.840716 \n",
       "L 259.994139 152.487951 \n",
       "L 260.592989 151.82064 \n",
       "L 261.234603 150.72931 \n",
       "L 261.919 149.076571 \n",
       "L 262.646182 146.691227 \n",
       "L 263.416127 143.361438 \n",
       "L 264.228855 138.826352 \n",
       "L 265.084367 132.767054 \n",
       "L 265.127131 132.426533 \n",
       "L 265.127131 132.426533 \n",
       "\" clip-path=\"url(#pbab4e7d9e4)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <defs>\n",
       "     <path id=\"m65e46e6893\" d=\"M -3 3 \n",
       "L 3 -3 \n",
       "M -3 -3 \n",
       "L 3 3 \n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pbab4e7d9e4)\">\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"185.095051\" y=\"149.432922\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"255.160551\" y=\"155.463301\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"138.76969\" y=\"62.037859\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"253.021791\" y=\"146.073885\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"83.889319\" y=\"92.966384\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"127.690952\" y=\"166.1846\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"57.154924\" y=\"231.3093\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"247.076071\" y=\"135.421372\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"144.031013\" y=\"41.866252\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"241.515323\" y=\"119.411575\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"183.640699\" y=\"147.275598\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"55.529472\" y=\"233.201676\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"131.198503\" y=\"138.895561\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"187.362122\" y=\"149.555502\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"94.326428\" y=\"126.507999\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"145.613687\" y=\"53.61265\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"157.376827\" y=\"84.188272\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"105.619034\" y=\"137.595083\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"165.076336\" y=\"104.302504\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"74.436038\" y=\"80.607342\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"106.902287\" y=\"147.660906\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"98.005078\" y=\"127.447172\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"242.114173\" y=\"120.044816\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"57.967649\" y=\"227.541866\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"113.361316\" y=\"164.351983\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"242.413598\" y=\"119.183153\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"260.207996\" y=\"154.653537\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"127.733726\" y=\"162.097532\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"261.961784\" y=\"154.733323\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"192.110152\" y=\"161.683579\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"254.133945\" y=\"145.844949\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"251.567449\" y=\"142.249352\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"76.104263\" y=\"78.479044\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"116.740545\" y=\"161.524101\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"245.450614\" y=\"131.296717\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"134.706058\" y=\"100.923932\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"255.246099\" y=\"150.310454\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"191.639622\" y=\"150.364941\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"225.046936\" y=\"71.036957\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"53.134071\" y=\"229.662898\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"230.736015\" y=\"100.686764\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"169.696035\" y=\"117.974834\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"113.703515\" y=\"153.727251\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"243.311874\" y=\"124.054926\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"94.625853\" y=\"125.495552\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"215.893075\" y=\"47.471358\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"217.005218\" y=\"56.484027\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"132.909507\" y=\"123.234054\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"69.388583\" y=\"127.990213\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m65e46e6893\" x=\"140.993988\" y=\"45.934833\" style=\"stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 51.294744 232.934186 \n",
       "L 56.128323 230.114502 \n",
       "L 57.753774 227.67802 \n",
       "L 58.7376 224.425595 \n",
       "L 59.16535 220.38308 \n",
       "L 73.922737 75.395417 \n",
       "L 74.992112 69.017094 \n",
       "L 75.034888 69.096777 \n",
       "L 81.536693 81.438653 \n",
       "L 85.343672 90.382421 \n",
       "L 94.968052 117.776062 \n",
       "L 114.259591 154.788619 \n",
       "L 116.056143 157.422302 \n",
       "L 118.922069 160.471311 \n",
       "L 120.804173 161.083917 \n",
       "L 123.285127 161.291507 \n",
       "L 127.519853 162.244093 \n",
       "L 129.701377 149.736638 \n",
       "L 134.064434 107.831005 \n",
       "L 137.57199 67.949462 \n",
       "L 139.111889 50.435716 \n",
       "L 140.224033 48.351829 \n",
       "L 144.415986 45.517966 \n",
       "L 146.255321 44.222767 \n",
       "L 151.901616 44.744184 \n",
       "L 167.942257 86.718163 \n",
       "L 173.588562 99.648046 \n",
       "L 175.770086 104.728732 \n",
       "L 182.271894 126.246581 \n",
       "L 186.207195 140.30848 \n",
       "L 187.062697 142.388978 \n",
       "L 196.00268 155.064922 \n",
       "L 196.045453 155.025962 \n",
       "L 203.702188 147.915085 \n",
       "L 209.519588 134.051607 \n",
       "L 210.460637 121.683555 \n",
       "L 215.208667 54.081047 \n",
       "L 215.251441 54.102645 \n",
       "L 226.501278 59.867528 \n",
       "L 232.062026 79.738945 \n",
       "L 236.681735 96.41758 \n",
       "L 243.440184 118.717614 \n",
       "L 244.039035 120.302444 \n",
       "L 261.32015 143.64816 \n",
       "L 265.127131 143.319526 \n",
       "L 265.127131 143.319526 \n",
       "\" clip-path=\"url(#pbab4e7d9e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 243.24375 \n",
       "L 40.603125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 275.81875 243.24375 \n",
       "L 275.81875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 243.24375 \n",
       "L 275.81875 243.24375 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 22.318125 \n",
       "L 275.81875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_14\">\n",
       "    <!-- Prediction -->\n",
       "    <g transform=\"translate(126.828125 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSerif-50\" d=\"M 1581 2375 \n",
       "L 2406 2375 \n",
       "Q 2872 2375 3115 2626 \n",
       "Q 3359 2878 3359 3353 \n",
       "Q 3359 3831 3115 4081 \n",
       "Q 2872 4331 2406 4331 \n",
       "L 1581 4331 \n",
       "L 1581 2375 \n",
       "z\n",
       "M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2559 4666 \n",
       "Q 3259 4666 3668 4311 \n",
       "Q 4078 3956 4078 3353 \n",
       "Q 4078 2753 3668 2397 \n",
       "Q 3259 2041 2559 2041 \n",
       "L 1581 2041 \n",
       "L 1581 331 \n",
       "L 2303 331 \n",
       "L 2303 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-72\" d=\"M 3059 3328 \n",
       "L 3059 2497 \n",
       "L 2728 2497 \n",
       "Q 2713 2744 2591 2866 \n",
       "Q 2469 2988 2234 2988 \n",
       "Q 1809 2988 1582 2694 \n",
       "Q 1356 2400 1356 1850 \n",
       "L 1356 331 \n",
       "L 2022 331 \n",
       "L 2022 0 \n",
       "L 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2994 \n",
       "L 231 2994 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1525 3078 1790 3245 \n",
       "Q 2056 3413 2438 3413 \n",
       "Q 2578 3413 2733 3391 \n",
       "Q 2888 3369 3059 3328 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-65\" d=\"M 3469 1600 \n",
       "L 991 1600 \n",
       "L 991 1575 \n",
       "Q 991 903 1244 561 \n",
       "Q 1497 219 1991 219 \n",
       "Q 2369 219 2611 417 \n",
       "Q 2853 616 2950 1006 \n",
       "L 3413 1006 \n",
       "Q 3275 459 2904 184 \n",
       "Q 2534 -91 1931 -91 \n",
       "Q 1203 -91 761 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2450 753 2931 \n",
       "Q 1188 3413 1894 3413 \n",
       "Q 2647 3413 3050 2948 \n",
       "Q 3453 2484 3469 1600 \n",
       "z\n",
       "M 2791 1931 \n",
       "Q 2772 2513 2545 2808 \n",
       "Q 2319 3103 1894 3103 \n",
       "Q 1497 3103 1269 2806 \n",
       "Q 1041 2509 991 1931 \n",
       "L 2791 1931 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-64\" d=\"M 3359 331 \n",
       "L 3909 331 \n",
       "L 3909 0 \n",
       "L 2784 0 \n",
       "L 2784 519 \n",
       "Q 2616 206 2355 57 \n",
       "Q 2094 -91 1709 -91 \n",
       "Q 1097 -91 708 395 \n",
       "Q 319 881 319 1663 \n",
       "Q 319 2444 706 2928 \n",
       "Q 1094 3413 1709 3413 \n",
       "Q 2094 3413 2355 3264 \n",
       "Q 2616 3116 2784 2803 \n",
       "L 2784 4531 \n",
       "L 2241 4531 \n",
       "L 2241 4863 \n",
       "L 3359 4863 \n",
       "L 3359 331 \n",
       "z\n",
       "M 2784 1497 \n",
       "L 2784 1825 \n",
       "Q 2784 2422 2554 2737 \n",
       "Q 2325 3053 1888 3053 \n",
       "Q 1444 3053 1217 2703 \n",
       "Q 991 2353 991 1663 \n",
       "Q 991 975 1217 622 \n",
       "Q 1444 269 1888 269 \n",
       "Q 2325 269 2554 583 \n",
       "Q 2784 897 2784 1497 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-69\" d=\"M 622 4353 \n",
       "Q 622 4497 726 4603 \n",
       "Q 831 4709 978 4709 \n",
       "Q 1122 4709 1226 4603 \n",
       "Q 1331 4497 1331 4353 \n",
       "Q 1331 4206 1228 4103 \n",
       "Q 1125 4000 978 4000 \n",
       "Q 831 4000 726 4103 \n",
       "Q 622 4206 622 4353 \n",
       "z\n",
       "M 1356 331 \n",
       "L 1900 331 \n",
       "L 1900 0 \n",
       "L 231 0 \n",
       "L 231 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 331 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-63\" d=\"M 3291 997 \n",
       "Q 3169 466 2822 187 \n",
       "Q 2475 -91 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2459 759 2936 \n",
       "Q 1200 3413 1925 3413 \n",
       "Q 2241 3413 2553 3339 \n",
       "Q 2866 3266 3181 3116 \n",
       "L 3181 2266 \n",
       "L 2847 2266 \n",
       "Q 2781 2703 2561 2903 \n",
       "Q 2341 3103 1931 3103 \n",
       "Q 1466 3103 1228 2742 \n",
       "Q 991 2381 991 1663 \n",
       "Q 991 944 1227 581 \n",
       "Q 1463 219 1931 219 \n",
       "Q 2303 219 2525 412 \n",
       "Q 2747 606 2828 997 \n",
       "L 3291 997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-74\" d=\"M 691 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 691 3322 \n",
       "L 691 4353 \n",
       "L 1269 4353 \n",
       "L 1269 3322 \n",
       "L 2350 3322 \n",
       "L 2350 2988 \n",
       "L 1269 2988 \n",
       "L 1269 878 \n",
       "Q 1269 456 1350 337 \n",
       "Q 1431 219 1650 219 \n",
       "Q 1875 219 1978 351 \n",
       "Q 2081 484 2088 781 \n",
       "L 2522 781 \n",
       "Q 2497 328 2275 118 \n",
       "Q 2053 -91 1600 -91 \n",
       "Q 1103 -91 897 129 \n",
       "Q 691 350 691 878 \n",
       "L 691 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-6f\" d=\"M 1925 219 \n",
       "Q 2388 219 2623 584 \n",
       "Q 2859 950 2859 1663 \n",
       "Q 2859 2375 2623 2739 \n",
       "Q 2388 3103 1925 3103 \n",
       "Q 1463 3103 1227 2739 \n",
       "Q 991 2375 991 1663 \n",
       "Q 991 950 1228 584 \n",
       "Q 1466 219 1925 219 \n",
       "z\n",
       "M 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2456 758 2934 \n",
       "Q 1197 3413 1925 3413 \n",
       "Q 2653 3413 3092 2934 \n",
       "Q 3531 2456 3531 1663 \n",
       "Q 3531 869 3092 389 \n",
       "Q 2653 -91 1925 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-6e\" d=\"M 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1516 3069 1770 3241 \n",
       "Q 2025 3413 2363 3413 \n",
       "Q 2913 3413 3172 3097 \n",
       "Q 3431 2781 3431 2113 \n",
       "L 3431 331 \n",
       "L 3944 331 \n",
       "L 3944 0 \n",
       "L 2356 0 \n",
       "L 2356 331 \n",
       "L 2853 331 \n",
       "L 2853 1931 \n",
       "Q 2853 2541 2703 2767 \n",
       "Q 2553 2994 2175 2994 \n",
       "Q 1775 2994 1565 2701 \n",
       "Q 1356 2409 1356 1850 \n",
       "L 1356 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 263 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSerif-50\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-72\" x=\"67.285156\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-65\" x=\"115.087891\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-64\" x=\"174.267578\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-69\" x=\"238.28125\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-63\" x=\"270.263672\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-74\" x=\"326.269531\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-69\" x=\"366.455078\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6f\" x=\"398.4375\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6e\" x=\"458.642578\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 166.5875 238.24375 \n",
       "L 268.81875 238.24375 \n",
       "Q 270.81875 238.24375 270.81875 236.24375 \n",
       "L 270.81875 193.067187 \n",
       "Q 270.81875 191.067187 268.81875 191.067187 \n",
       "L 166.5875 191.067187 \n",
       "Q 164.5875 191.067187 164.5875 193.067187 \n",
       "L 164.5875 236.24375 \n",
       "Q 164.5875 238.24375 166.5875 238.24375 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\">\n",
       "     <path d=\"M 168.5875 199.165625 \n",
       "L 178.5875 199.165625 \n",
       "L 188.5875 199.165625 \n",
       "\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- True -->\n",
       "     <g transform=\"translate(196.5875 202.665625) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-54\" d=\"M 1222 0 \n",
       "L 1222 331 \n",
       "L 1819 331 \n",
       "L 1819 4294 \n",
       "L 447 4294 \n",
       "L 447 3566 \n",
       "L 63 3566 \n",
       "L 63 4666 \n",
       "L 4206 4666 \n",
       "L 4206 3566 \n",
       "L 3822 3566 \n",
       "L 3822 4294 \n",
       "L 2450 4294 \n",
       "L 2450 331 \n",
       "L 3047 331 \n",
       "L 3047 0 \n",
       "L 1222 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-75\" d=\"M 2266 3322 \n",
       "L 3341 3322 \n",
       "L 3341 331 \n",
       "L 3884 331 \n",
       "L 3884 0 \n",
       "L 2766 0 \n",
       "L 2766 588 \n",
       "Q 2606 256 2353 82 \n",
       "Q 2100 -91 1766 -91 \n",
       "Q 1213 -91 952 223 \n",
       "Q 691 538 691 1209 \n",
       "L 691 2988 \n",
       "L 172 2988 \n",
       "L 172 3322 \n",
       "L 1269 3322 \n",
       "L 1269 1388 \n",
       "Q 1269 781 1417 556 \n",
       "Q 1566 331 1947 331 \n",
       "Q 2347 331 2556 625 \n",
       "Q 2766 919 2766 1478 \n",
       "L 2766 2988 \n",
       "L 2266 2988 \n",
       "L 2266 3322 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-75\" x=\"114.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"178.90625\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <g>\n",
       "      <use xlink:href=\"#m65e46e6893\" x=\"178.5875\" y=\"213.84375\" style=\"stroke: #000000\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- Training Data -->\n",
       "     <g transform=\"translate(196.5875 217.34375) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-61\" d=\"M 2547 1044 \n",
       "L 2547 1747 \n",
       "L 1806 1747 \n",
       "Q 1378 1747 1168 1562 \n",
       "Q 959 1378 959 997 \n",
       "Q 959 650 1171 447 \n",
       "Q 1384 244 1747 244 \n",
       "Q 2106 244 2326 466 \n",
       "Q 2547 688 2547 1044 \n",
       "z\n",
       "M 3122 2075 \n",
       "L 3122 331 \n",
       "L 3634 331 \n",
       "L 3634 0 \n",
       "L 2547 0 \n",
       "L 2547 359 \n",
       "Q 2356 128 2106 18 \n",
       "Q 1856 -91 1522 -91 \n",
       "Q 969 -91 644 203 \n",
       "Q 319 497 319 997 \n",
       "Q 319 1513 691 1797 \n",
       "Q 1063 2081 1741 2081 \n",
       "L 2547 2081 \n",
       "L 2547 2309 \n",
       "Q 2547 2688 2317 2895 \n",
       "Q 2088 3103 1672 3103 \n",
       "Q 1328 3103 1125 2947 \n",
       "Q 922 2791 872 2484 \n",
       "L 575 2484 \n",
       "L 575 3156 \n",
       "Q 875 3284 1158 3348 \n",
       "Q 1441 3413 1709 3413 \n",
       "Q 2400 3413 2761 3070 \n",
       "Q 3122 2728 3122 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-67\" d=\"M 3359 2988 \n",
       "L 3359 72 \n",
       "Q 3359 -644 2965 -1033 \n",
       "Q 2572 -1422 1844 -1422 \n",
       "Q 1516 -1422 1216 -1362 \n",
       "Q 916 -1303 641 -1184 \n",
       "L 641 -488 \n",
       "L 941 -488 \n",
       "Q 997 -813 1206 -963 \n",
       "Q 1416 -1113 1806 -1113 \n",
       "Q 2313 -1113 2548 -827 \n",
       "Q 2784 -541 2784 72 \n",
       "L 2784 519 \n",
       "Q 2616 206 2355 57 \n",
       "Q 2094 -91 1709 -91 \n",
       "Q 1097 -91 708 395 \n",
       "Q 319 881 319 1663 \n",
       "Q 319 2444 706 2928 \n",
       "Q 1094 3413 1709 3413 \n",
       "Q 2094 3413 2355 3264 \n",
       "Q 2616 3116 2784 2803 \n",
       "L 2784 3322 \n",
       "L 3909 3322 \n",
       "L 3909 2988 \n",
       "L 3359 2988 \n",
       "z\n",
       "M 2784 1825 \n",
       "Q 2784 2422 2554 2737 \n",
       "Q 2325 3053 1888 3053 \n",
       "Q 1444 3053 1217 2703 \n",
       "Q 991 2353 991 1663 \n",
       "Q 991 975 1217 622 \n",
       "Q 1444 269 1888 269 \n",
       "Q 2325 269 2554 583 \n",
       "Q 2784 897 2784 1497 \n",
       "L 2784 1825 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-44\" d=\"M 1581 331 \n",
       "L 2163 331 \n",
       "Q 3072 331 3558 850 \n",
       "Q 4044 1369 4044 2338 \n",
       "Q 4044 3306 3559 3818 \n",
       "Q 3075 4331 2163 4331 \n",
       "L 1581 4331 \n",
       "L 1581 331 \n",
       "z\n",
       "M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2209 4666 \n",
       "Q 3416 4666 4089 4050 \n",
       "Q 4763 3434 4763 2338 \n",
       "Q 4763 1238 4088 619 \n",
       "Q 3413 0 2209 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"114.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"174.121094\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"206.103516\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"270.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"302.490234\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-67\" x=\"366.894531\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"430.908203\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-44\" x=\"462.695312\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"542.871094\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"602.490234\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"642.675781\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 168.5875 228.664062 \n",
       "L 178.5875 228.664062 \n",
       "L 188.5875 228.664062 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Prediction -->\n",
       "     <g transform=\"translate(196.5875 232.164062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"67.285156\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"115.087891\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-64\" x=\"174.267578\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"238.28125\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"270.263672\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"326.269531\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"366.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"398.4375\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"458.642578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 326.384375 243.24375 \n",
       "L 561.6 243.24375 \n",
       "L 561.6 22.318125 \n",
       "L 326.384375 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"337.075994\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(333.894744 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"375.260349\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 2500 -->\n",
       "      <g transform=\"translate(362.535349 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-35\" d=\"M 3219 4666 \n",
       "L 3219 4153 \n",
       "L 1081 4153 \n",
       "L 1081 2816 \n",
       "Q 1244 2928 1461 2984 \n",
       "Q 1678 3041 1947 3041 \n",
       "Q 2703 3041 3140 2622 \n",
       "Q 3578 2203 3578 1478 \n",
       "Q 3578 738 3136 323 \n",
       "Q 2694 -91 1894 -91 \n",
       "Q 1572 -91 1234 -12 \n",
       "Q 897 66 544 225 \n",
       "L 544 1131 \n",
       "L 897 1131 \n",
       "Q 925 688 1179 453 \n",
       "Q 1434 219 1894 219 \n",
       "Q 2388 219 2653 544 \n",
       "Q 2919 869 2919 1478 \n",
       "Q 2919 2084 2655 2407 \n",
       "Q 2391 2731 1894 2731 \n",
       "Q 1613 2731 1398 2631 \n",
       "Q 1184 2531 1019 2322 \n",
       "L 750 2322 \n",
       "L 750 4666 \n",
       "L 3219 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"413.444704\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 5000 -->\n",
       "      <g transform=\"translate(400.719704 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"451.629058\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 7500 -->\n",
       "      <g transform=\"translate(438.904058 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-37\" d=\"M 3609 4347 \n",
       "L 1784 0 \n",
       "L 1319 0 \n",
       "L 3059 4153 \n",
       "L 903 4153 \n",
       "L 903 3578 \n",
       "L 538 3578 \n",
       "L 538 4666 \n",
       "L 3609 4666 \n",
       "L 3609 4347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"489.813413\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 10000 -->\n",
       "      <g transform=\"translate(473.907163 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc8502393ed\" x=\"527.997768\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 12500 -->\n",
       "      <g transform=\"translate(512.091518 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_24\">\n",
       "     <!-- Epoch -->\n",
       "     <g transform=\"translate(428.110937 271.520312) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-45\" d=\"M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 4109 4666 \n",
       "L 4109 3628 \n",
       "L 3725 3628 \n",
       "L 3725 4281 \n",
       "L 1581 4281 \n",
       "L 1581 2719 \n",
       "L 3109 2719 \n",
       "L 3109 3303 \n",
       "L 3494 3303 \n",
       "L 3494 1753 \n",
       "L 3109 1753 \n",
       "L 3109 2338 \n",
       "L 1581 2338 \n",
       "L 1581 384 \n",
       "L 3775 384 \n",
       "L 3775 1038 \n",
       "L 4159 1038 \n",
       "L 4159 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-70\" d=\"M 1313 1825 \n",
       "L 1313 1497 \n",
       "Q 1313 897 1542 583 \n",
       "Q 1772 269 2209 269 \n",
       "Q 2650 269 2876 622 \n",
       "Q 3103 975 3103 1663 \n",
       "Q 3103 2353 2876 2703 \n",
       "Q 2650 3053 2209 3053 \n",
       "Q 1772 3053 1542 2737 \n",
       "Q 1313 2422 1313 1825 \n",
       "z\n",
       "M 738 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 1313 3322 \n",
       "L 1313 2803 \n",
       "Q 1481 3116 1742 3264 \n",
       "Q 2003 3413 2388 3413 \n",
       "Q 3000 3413 3387 2928 \n",
       "Q 3775 2444 3775 1663 \n",
       "Q 3775 881 3387 395 \n",
       "Q 3000 -91 2388 -91 \n",
       "Q 2003 -91 1742 57 \n",
       "Q 1481 206 1313 519 \n",
       "L 1313 -997 \n",
       "L 1856 -997 \n",
       "L 1856 -1331 \n",
       "L 184 -1331 \n",
       "L 184 -997 \n",
       "L 738 -997 \n",
       "L 738 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-68\" d=\"M 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 4531 \n",
       "L 231 4531 \n",
       "L 231 4863 \n",
       "L 1356 4863 \n",
       "L 1356 2731 \n",
       "Q 1516 3069 1770 3241 \n",
       "Q 2025 3413 2363 3413 \n",
       "Q 2913 3413 3172 3097 \n",
       "Q 3431 2781 3431 2113 \n",
       "L 3431 331 \n",
       "L 3944 331 \n",
       "L 3944 0 \n",
       "L 2356 0 \n",
       "L 2356 331 \n",
       "L 2853 331 \n",
       "L 2853 1931 \n",
       "Q 2853 2541 2704 2764 \n",
       "Q 2556 2988 2175 2988 \n",
       "Q 1775 2988 1565 2697 \n",
       "Q 1356 2406 1356 1850 \n",
       "L 1356 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 263 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-70\" x=\"72.998047\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"137.011719\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"197.216797\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-68\" x=\"253.222656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"326.384375\" y=\"213.822173\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(306.659375 217.621392) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"326.384375\" y=\"180.211745\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(300.296875 184.010963) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"326.384375\" y=\"146.601316\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(300.296875 150.400535) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"326.384375\" y=\"112.990887\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(300.296875 116.790106) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"326.384375\" y=\"79.380458\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(300.296875 83.179677) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_29\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb0294f6553\" x=\"326.384375\" y=\"45.77003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_30\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(300.296875 49.569248) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-33\" d=\"M 622 4469 \n",
       "Q 988 4606 1323 4678 \n",
       "Q 1659 4750 1953 4750 \n",
       "Q 2638 4750 3022 4454 \n",
       "Q 3406 4159 3406 3634 \n",
       "Q 3406 3213 3140 2930 \n",
       "Q 2875 2647 2388 2547 \n",
       "Q 2963 2466 3280 2130 \n",
       "Q 3597 1794 3597 1259 \n",
       "Q 3597 606 3158 257 \n",
       "Q 2719 -91 1894 -91 \n",
       "Q 1528 -91 1179 -12 \n",
       "Q 831 66 488 225 \n",
       "L 488 1131 \n",
       "L 838 1131 \n",
       "Q 869 681 1141 450 \n",
       "Q 1413 219 1906 219 \n",
       "Q 2384 219 2661 495 \n",
       "Q 2938 772 2938 1253 \n",
       "Q 2938 1803 2653 2086 \n",
       "Q 2369 2369 1819 2369 \n",
       "L 1522 2369 \n",
       "L 1522 2688 \n",
       "L 1678 2688 \n",
       "Q 2225 2688 2498 2914 \n",
       "Q 2772 3141 2772 3597 \n",
       "Q 2772 4006 2547 4223 \n",
       "Q 2322 4441 1900 4441 \n",
       "Q 1478 4441 1245 4241 \n",
       "Q 1013 4041 972 3647 \n",
       "L 622 3647 \n",
       "L 622 4469 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_31\">\n",
       "     <!-- Loss -->\n",
       "     <g transform=\"translate(294.217188 144.242656) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-4c\" d=\"M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2175 4666 \n",
       "L 2175 4331 \n",
       "L 1581 4331 \n",
       "L 1581 384 \n",
       "L 3713 384 \n",
       "L 3713 1166 \n",
       "L 4097 1166 \n",
       "L 4097 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-73\" d=\"M 359 184 \n",
       "L 359 959 \n",
       "L 691 959 \n",
       "Q 703 588 923 403 \n",
       "Q 1144 219 1575 219 \n",
       "Q 1963 219 2166 364 \n",
       "Q 2369 509 2369 788 \n",
       "Q 2369 1006 2220 1140 \n",
       "Q 2072 1275 1594 1428 \n",
       "L 1178 1569 \n",
       "Q 750 1706 558 1912 \n",
       "Q 366 2119 366 2438 \n",
       "Q 366 2894 700 3153 \n",
       "Q 1034 3413 1625 3413 \n",
       "Q 1888 3413 2178 3344 \n",
       "Q 2469 3275 2778 3144 \n",
       "L 2778 2419 \n",
       "L 2447 2419 \n",
       "Q 2434 2741 2221 2922 \n",
       "Q 2009 3103 1644 3103 \n",
       "Q 1281 3103 1095 2975 \n",
       "Q 909 2847 909 2591 \n",
       "Q 909 2381 1050 2254 \n",
       "Q 1191 2128 1613 1997 \n",
       "L 2069 1856 \n",
       "Q 2541 1709 2748 1489 \n",
       "Q 2956 1269 2956 922 \n",
       "Q 2956 450 2595 179 \n",
       "Q 2234 -91 1600 -91 \n",
       "Q 1278 -91 972 -22 \n",
       "Q 666 47 359 184 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"66.40625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"126.611328\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"177.929688\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_30\">\n",
       "    <path d=\"M 337.075994 32.360199 \n",
       "L 352.349736 163.361035 \n",
       "L 367.623478 182.240294 \n",
       "L 382.89722 195.168298 \n",
       "L 398.170962 197.199233 \n",
       "L 413.444704 198.690013 \n",
       "L 428.718446 198.437766 \n",
       "L 443.992188 200.951773 \n",
       "L 459.265929 201.99616 \n",
       "L 474.539671 218.407882 \n",
       "L 489.813413 217.644739 \n",
       "L 505.087155 225.349775 \n",
       "L 520.360897 221.531873 \n",
       "L 535.634639 215.470434 \n",
       "L 550.908381 233.201676 \n",
       "\" clip-path=\"url(#pdbdbc2893e)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_31\">\n",
       "    <path d=\"M 337.075994 134.745383 \n",
       "L 352.349736 189.261251 \n",
       "L 367.623478 199.415769 \n",
       "L 382.89722 213.806478 \n",
       "L 398.170962 211.385294 \n",
       "L 413.444704 214.072472 \n",
       "L 428.718446 214.273729 \n",
       "L 443.992188 217.357477 \n",
       "L 459.265929 213.437133 \n",
       "L 474.539671 218.073591 \n",
       "L 489.813413 214.569023 \n",
       "L 505.087155 232.217121 \n",
       "L 520.360897 217.844119 \n",
       "L 535.634639 220.082854 \n",
       "L 550.908381 216.903439 \n",
       "\" clip-path=\"url(#pdbdbc2893e)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 326.384375 243.24375 \n",
       "L 326.384375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 561.6 243.24375 \n",
       "L 561.6 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 326.384375 243.24375 \n",
       "L 561.6 243.24375 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 326.384375 22.318125 \n",
       "L 561.6 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_32\">\n",
       "    <!-- Loss -->\n",
       "    <g transform=\"translate(430.238125 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSerif-4c\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6f\" x=\"66.40625\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-73\" x=\"126.611328\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-73\" x=\"177.929688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_2\">\n",
       "    <g id=\"patch_13\">\n",
       "     <path d=\"M 469.446875 59.674375 \n",
       "L 554.6 59.674375 \n",
       "Q 556.6 59.674375 556.6 57.674375 \n",
       "L 556.6 29.318125 \n",
       "Q 556.6 27.318125 554.6 27.318125 \n",
       "L 469.446875 27.318125 \n",
       "Q 467.446875 27.318125 467.446875 29.318125 \n",
       "L 467.446875 57.674375 \n",
       "Q 467.446875 59.674375 469.446875 59.674375 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_32\">\n",
       "     <path d=\"M 471.446875 35.416563 \n",
       "L 481.446875 35.416563 \n",
       "L 491.446875 35.416563 \n",
       "\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_33\">\n",
       "     <!-- Train Loss -->\n",
       "     <g transform=\"translate(499.446875 38.916563) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"114.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"174.121094\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"206.103516\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"270.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\" x=\"302.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"368.701172\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"428.90625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"480.224609\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_33\">\n",
       "     <path d=\"M 471.446875 50.094688 \n",
       "L 481.446875 50.094688 \n",
       "L 491.446875 50.094688 \n",
       "\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_34\">\n",
       "     <!-- Test Loss -->\n",
       "     <g transform=\"translate(499.446875 53.594688) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"58.949219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"118.128906\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"169.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"209.632812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\" x=\"241.419922\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"307.826172\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"368.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"419.349609\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pbab4e7d9e4\">\n",
       "   <rect x=\"40.603125\" y=\"22.318125\" width=\"235.215625\" height=\"220.925625\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"pdbdbc2893e\">\n",
       "   <rect x=\"326.384375\" y=\"22.318125\" width=\"235.215625\" height=\"220.925625\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 2400x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Multi Layer Perceptron (MLP)\n",
    "alpha = 1e-3  # Learning rate\n",
    "epochs = 15000  # Number of training epochs\n",
    "\n",
    "class MLP(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        keys = jax.random.split(key, 4)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(1, 10 , key=keys[0]),  # Input layer\n",
    "            eqx.nn.Linear(10, 10, key=keys[1]),  # Hidden layer 1\n",
    "            eqx.nn.Linear(10, 10, key=keys[2]),  # Hidden layer 2\n",
    "            eqx.nn.Linear(10, 10, key=keys[2]),  # Hidden layer 3\n",
    "            eqx.nn.Linear(10, 1 , key=keys[3]),  # Output layer\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = jax.nn.relu(layer(x))  # Apply ReLU activation function\n",
    "        return self.layers[-1](x)  # No activation on the output layer\n",
    "\n",
    "@jit\n",
    "@value_and_grad\n",
    "def loss(model, x, y):\n",
    "    \"\"\"\n",
    "    Compute the loss between the model predictions and the true values.\n",
    "    \n",
    "    Parameters:\n",
    "    model : MLP model\n",
    "    x : Input data\n",
    "    y : True values\n",
    "    \n",
    "    Returns:\n",
    "    loss : Computed loss\n",
    "    \"\"\"\n",
    "    pred_y = vmap(model)(x)\n",
    "    loss = la.norm(pred_y - y)\n",
    "    return loss\n",
    "\n",
    "@jit\n",
    "def make_step(model, opt_state, x, y):\n",
    "    \"\"\"\n",
    "    Perform a single optimization step.\n",
    "    \n",
    "    Parameters:\n",
    "    model : MLP model\n",
    "    opt_state : Optimizer state\n",
    "    x : Input data\n",
    "    y : True values\n",
    "    \n",
    "    Returns:\n",
    "    model : Updated model\n",
    "    opt_state : Updated optimizer state\n",
    "    loss_value : Computed loss\n",
    "    \"\"\"\n",
    "    loss_value, grads = loss(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss_value\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = MLP(key)\n",
    "optim = optax.sgd(alpha)\n",
    "opt_state = optim.init(model)\n",
    "\n",
    "# Lists to store training and test loss\n",
    "ltrain_loss, ltest_loss, lepoch = [], [], []\n",
    "\n",
    "# Training loop\n",
    "for step in range(epochs):\n",
    "    model, opt_state, train_loss = make_step(model, opt_state, X_data_train, Y_data_train)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        lepoch.append(step)\n",
    "        ltrain_loss.append(train_loss)\n",
    "        ltest_loss.append(loss(model, X_data_test, Y_data_test)[0])\n",
    "    if step % 5000 == 0:\n",
    "        print(f\"iteration : {step:5d} - Train MSE : {train_loss:.2f} - Test  MSE : {loss(model, X_data_test, Y_data_test)[0]:.2f}\")\n",
    "\n",
    "# Plotting the results\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), dpi=300, tight_layout=True)\n",
    "\n",
    "# First subplot for the prediction\n",
    "axs[0].plot(t, y, 'b', label='True')\n",
    "axs[0].plot(X_data_train, Y_data_train, 'xk', label='Training Data')\n",
    "axs[0].plot(t, vmap(model)(t).squeeze(), '--r', label='Prediction')\n",
    "axs[0].set_xlabel(\"$t$\")\n",
    "axs[0].set_ylabel(\"$T$\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Prediction\")\n",
    "\n",
    "# Second subplot for the loss\n",
    "axs[1].plot(lepoch, ltrain_loss, 'b', label='Train Loss')\n",
    "axs[1].plot(lepoch, ltest_loss, 'r', label='Test Loss')\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].legend()\n",
    "axs[1].set_title(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise, Bias, and Variance\n",
    "\n",
    "The expected loss of a model, considering the uncertainties in both the training data $\\mathcal{D}$ and the test data $y$, can be decomposed as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{D}}\\left[\\mathbb{E}_y[L(x)]\\right] = \\underbrace{\\mathbb{E}_{\\mathcal{D}}\\left[\\left(f(x; \\theta(\\mathcal{D})) - f_\\mu(x)\\right)^2\\right]}_{\\text{Variance}} + \\underbrace{\\left(f_\\mu(x) - \\mu(x)\\right)^2}_{\\text{Bias}} + \\underbrace{\\sigma^2}_{\\text{Noise}}\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- **Variance**: Represents the model's sensitivity to fluctuations in the training data. It quantifies how much the model's predictions at point $x$ would vary if we trained it on different datasets.\n",
    "- **Bias**: Measures the error introduced by approximating a complex real-world problem with a simpler model. It is the squared difference between the expected prediction of the model and the true underlying function $\\mu(x)$.\n",
    "- **Noise**: Captures the irreducible error inherent in the data due to randomness or measurement errors. This component is unavoidable and sets a fundamental limit on the model's performance.\n",
    "\n",
    "**Reducing Noise**:\n",
    "\n",
    "The noise term $\\sigma^2$ is intrinsic to the data and cannot be reduced through modeling. **It represents inherent uncertainty that we cannot eliminate**.\n",
    "\n",
    "**Reducing Variance**:\n",
    "\n",
    "Variance arises from the model's complexity and the finite size of the training data. **Increasing the amount of training data** can reduce variance by providing the model with more information, leading to more stable and generalized predictions.\n",
    "\n",
    "**Reducing Bias**:\n",
    "\n",
    "Bias results from simplifying assumptions made by the model. **We can reduce bias by increasing the model's capacity or complexity**, allowing it to capture more intricate patterns in the data.\n",
    "\n",
    "**Bias-Variance Trade-off**:\n",
    "\n",
    "There is an inherent trade-off between bias and variance:\n",
    "\n",
    "- Increasing model complexity can **reduce bias** but may **increase variance**, especially with limited data.\n",
    "- A highly complex model might **overfit** the training data, capturing noise as if it were a signal, leading to poor generalization on new data.\n",
    "- Conversely, a very simple model might **underfit**, failing to capture essential patterns, resulting in high bias.\n",
    "\n",
    "To achieve optimal performance, we seek a balance where both bias and variance are minimized as much as possible. This often involves selecting a model with the right capacity that is neither too simple nor too complex for the given dataset.\n",
    "\n",
    "**Illustration of the Bias-Variance Trade-off**:\n",
    "\n",
    "As the model capacity increases:\n",
    "\n",
    "- **Bias decreases** because the model becomes more flexible and can better fit the training data.\n",
    "- **Variance increases** because the model is more sensitive to the specific training data and may capture noise.\n",
    "\n",
    "The total expected loss is minimized at an optimal model capacity where the sum of bias and variance is the lowest. Selecting this optimal point is crucial for building models that generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Descent\n",
    "\n",
    "Double descent is a phenomenon observed in machine learning where the test error exhibits a distinctive pattern as model capacity increases. Initially, as the capacity grows, the test error decreases because the model can better capture the underlying patterns in the data. Traditionally, we expect that beyond a certain point, increasing capacity leads to overfitting, causing the test error to rise. However, with double descent, further increasing the model capacity after this peak causes the test error to decrease again.\n",
    "\n",
    "**Understanding the Phenomenon:**\n",
    "\n",
    "1. **Initial Descent (Underfitting Region):**\n",
    "   - With low model capacity, the model cannot capture the complexity of the data.\n",
    "   - Both training and test errors are high due to high bias.\n",
    "\n",
    "2. **Peak (Overfitting Onset):**\n",
    "   - As capacity increases, training error decreases.\n",
    "   - Test error reaches a minimum but starts to increase as the model begins to overfit the training data.\n",
    "\n",
    "3. **Second Descent (Interpolation Region):**\n",
    "   - Further increasing capacity allows the model to fit the training data perfectly.\n",
    "   - Surprisingly, the test error starts decreasing again despite the model overfitting the training data.\n",
    "   - The model generalizes better due to inherent inductive biases and smoother interpolation between data points.\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- **Inductive Bias:** The model's tendencies influence how it generalizes beyond the training data. High-capacity models can interpolate training data smoothly, leading to better performance on unseen data.\n",
    "- **Curse of Dimensionality:** In high-dimensional spaces, training data is sparse. Models rely on assumptions (like smoothness) to make predictions between known data points.\n",
    "- **Model Capacity:** There's an optimal range where increasing capacity improves generalization, contradicting the traditional bias-variance trade-off.\n",
    "\n",
    "**Implications for Machine Learning:**\n",
    "\n",
    "- **Beyond Bias-Variance Trade-off:** Double descent challenges the conventional understanding that increasing model capacity beyond a certain point worsens test performance.\n",
    "- **Designing Models:** Encourages the use of high-capacity models while being mindful of their inductive biases.\n",
    "- **Data Considerations:** Highlights the importance of understanding data dimensionality and distribution when training complex models.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Double descent reveals that increasing a model's capacity can eventually lead to improved generalization performance, even after overfitting occurs. This underscores the significance of model architecture and inductive biases in achieving optimal results in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "We introduced regularization before. Using a regularizer can improve predictive accuracy by reducing the variance of the solution at the expense of a slight increase in bias, as described by the bias–variance trade-off. **In practice, the best generalization results are often achieved by combining a larger network with some form of regularization.** Regularization reduces the generalization gap between training and testing performance by adding explicit terms to the loss function that favor certain parameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inductive Bias (Prior)\n",
    "\n",
    "In machine learning, when we aim to make predictions for new input values $\\mathbf{x}$, we need to select a specific distribution from infinitely many possibilities. The preference for one distribution over others is known as an **inductive bias** or **prior**. This prior knowledge helps constrain the solution space based on background information. Often, we expect that small changes in input should lead to small changes in output, so we bias our solutions toward functions that vary smoothly. Regularization terms encourage model weights to have smaller magnitudes, introducing a bias toward functions that change more gradually with respect to the inputs. For example, in object detection within images, we can incorporate prior knowledge that an object's identity is generally independent of its position in the image—a concept known as **translation invariance**. Integrating such priors can greatly simplify building models that generalize well.\n",
    "\n",
    "### No Free Lunch Theorem\n",
    "\n",
    "Even highly flexible neural networks possess significant inductive biases. For instance, convolutional neural networks encode specific forms of inductive bias, such as translation equivariance, which are particularly beneficial in image-related applications. The **No Free Lunch Theorem**, derived from the saying \"There's no such thing as a free lunch,\" states that every learning algorithm performs equally well when averaged over all possible problems. If a particular model or algorithm excels on some problems, it must perform worse on others.\n",
    "\n",
    "Although the No Free Lunch Theorem is largely theoretical, it highlights the central importance of bias in determining a machine learning algorithm's performance. It's impossible to learn \"purely from data\" without any bias. In practice, bias may be implicit—for example, neural networks have a finite number of parameters, limiting the functions they can represent. Bias can also be explicitly encoded as prior knowledge related to the specific problem being solved. When developing general-purpose learning algorithms, we're seeking inductive biases appropriate for broad classes of applications we expect to encounter. However, for any given application, better results can be achieved by incorporating stronger, application-specific inductive biases.\n",
    "\n",
    "The model-based machine learning perspective advocates making all assumptions explicit within models so that appropriate choices can be made for inductive biases. Inductive bias can be incorporated through the form of the distribution—for example, specifying that the output is a linear function of fixed basis functions. It can also be introduced by adding a regularization term to the error function during training. Another way to control a neural network's complexity is through the training process itself. Deep neural networks can generalize well even when the number of adjustable parameters exceeds the number of training data points, provided the training process is properly configured. Skillfully designing inductive biases and incorporating prior knowledge are essential in applying deep learning to real-world problems.\n",
    "\n",
    "### Symmetry and Invariance\n",
    "\n",
    "In many applications, predictions should remain unchanged, or **invariant**, under certain transformations of the input variables. For example, in image classification, an object should be assigned the same label regardless of its position within the image—this is called **translation invariance**. Similarly, changes in the size of the object should not affect its classification, known as **scale invariance**. Exploiting such symmetries to create inductive biases can significantly enhance machine learning models' performance and is a key aspect of geometric deep learning. Transformations like translation or scaling that preserve certain properties are known as symmetries.\n",
    "\n",
    "To efficiently encourage a model to exhibit the required invariances, we can use several strategies:\n",
    "\n",
    "1. **Pre-processing**: Incorporate invariance into a pre-processing stage by computing features of the data that are invariant under the desired transformations. Any subsequent regression or classification system using these features will inherently respect these invariances.\n",
    "\n",
    "2. **Regularized Error Function**: Add a regularization term to the error function to penalize changes in the model's output when the input undergoes one of the invariant transformations.\n",
    "\n",
    "3. **Data Augmentation**: Expand the training set by including transformed replicas of the training data points according to the desired invariances, assigning them the same target outputs as the original examples.\n",
    "\n",
    "4. **Network Architecture**: Embed invariance properties directly into the neural network's structure through appropriate architectural choices.\n",
    "\n",
    "### Equivariance\n",
    "\n",
    "An important generalization of invariance is called **equivariance**, where the network's output transforms in a specific way when the input is transformed, rather than remaining constant. For example, consider a network that takes an image as input and outputs a segmentation map classifying each pixel as foreground or background. If the object in the image is translated, we expect the segmentation output to be translated in the same manner. In this case, the network's output changes predictably in response to transformations of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Regularization\n",
    "\n",
    "In the \"Maximum A Posteriori (MAP) Estimation\" section of the ML 2 Notebook, we explored how to incorporate prior knowledge into our models. To steer the minimization process toward preferred solutions, we add an extra term to the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}; \\lambda) = \\frac{1}{N} \\sum_{n=1}^N \\ell\\left(\\boldsymbol{y}_n, \\boldsymbol{\\theta}; \\boldsymbol{x}_n\\right) + \\lambda C(\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Here, $C(\\boldsymbol{\\theta})$ is a scalar function that assigns higher values to less favored parameter configurations. The positive scalar $\\lambda$ balances the influence of the original loss function and the regularization term. Incorporating this term alters the landscape of the loss function, often leading the training process to converge to different parameter values than it would without regularization.\n",
    "\n",
    "To understand the effect of explicit regularization, imagine a loss function for a model that has multiple local minima and a global minimum. The regularization term adds a penalty that increases as parameters move away from a central, preferred region. This penalty smooths the overall loss function when combined with the original loss, reducing the number of local minima and shifting the global minimum. As a result, the optimization process is guided toward solutions that not only fit the data but also adhere to the preferred parameter characteristics.\n",
    "\n",
    "### L2 Regularization (Weight Decay)\n",
    "\n",
    "We've yet to discuss which solutions the regularization term should penalize—or, conversely, which solutions the prior should favor. Given the vast array of applications for neural networks, our regularization preferences need to be quite general. The most commonly used regularization term is based on the L2 norm, penalizing the sum of the squares of the parameter values:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}; \\lambda) = \\frac{1}{N} \\sum_{n=1}^N \\ell\\left(\\boldsymbol{y}_n, \\boldsymbol{\\theta}; \\boldsymbol{x}_n\\right) + \\lambda \\sum_i \\theta_i^2\n",
    "$$\n",
    "\n",
    "Here, $i$ indexes the parameters. **In neural networks, L2 regularization is typically applied to the weights but not the biases, a practice known as weight decay. This encourages the weights to be smaller, leading to a smoother output function.**\n",
    "\n",
    "To illustrate the impact of weight decay, consider fitting a simplified neural network model using different values of the regularization coefficient $\\lambda$. When $\\lambda$ is small, the regularization has little effect, and the model may fit the training data closely—even to the point of overfitting. As $\\lambda$ increases, the regularization term penalizes larger weights more strongly, resulting in a smoother function that fits the data less precisely but may generalize better to unseen data.\n",
    "\n",
    "This improvement in test performance occurs for a couple of reasons:\n",
    "\n",
    "- **Reduction of Overfitting**: If the network is overfitting, regularization forces it to balance fitting the training data with maintaining smoothness. This reduces variance (the model stops trying to pass through every data point exactly) but may introduce some bias (the model might underfit certain aspects of the data).\n",
    "- **Handling Over-parameterization**: In networks with more parameters than necessary, extra capacity can lead to complex models that capture noise. Regularization promotes functions that smoothly interpolate between data points, making reasonable predictions in areas without training data.\n",
    "\n",
    "Because weight decay is so prevalent in neural network training, modern deep learning frameworks have integrated it directly into optimization algorithms. This integration makes it convenient to apply weight decay alongside any loss function with minimal extra code. Additionally, incorporating weight decay at the optimization level offers computational benefits. Since the weight decay update depends only on the current parameter values, it can be seamlessly integrated into the parameter update steps without significant additional computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :     0 - Train MSE : 319.95 - Test  MSE : 167.68\n",
      "iteration :  5000 - Train MSE : 140.56 - Test  MSE : 63.24\n",
      "iteration : 10000 - Train MSE : 140.56 - Test  MSE : 63.24\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"568.8pt\" height=\"280.8pt\" viewBox=\"0 0 568.8 280.8\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-11-01T11:18:57.385351</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 280.8 \n",
       "L 568.8 280.8 \n",
       "L 568.8 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 243.24375 \n",
       "L 275.81875 243.24375 \n",
       "L 275.81875 22.318125 \n",
       "L 40.603125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m5753964004\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"51.294744\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(48.113494 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-30\" d=\"M 2034 219 \n",
       "Q 2513 219 2750 744 \n",
       "Q 2988 1269 2988 2328 \n",
       "Q 2988 3391 2750 3916 \n",
       "Q 2513 4441 2034 4441 \n",
       "Q 1556 4441 1318 3916 \n",
       "Q 1081 3391 1081 2328 \n",
       "Q 1081 1269 1318 744 \n",
       "Q 1556 219 2034 219 \n",
       "z\n",
       "M 2034 -91 \n",
       "Q 1275 -91 848 546 \n",
       "Q 422 1184 422 2328 \n",
       "Q 422 3475 848 4112 \n",
       "Q 1275 4750 2034 4750 \n",
       "Q 2797 4750 3222 4112 \n",
       "Q 3647 3475 3647 2328 \n",
       "Q 3647 1184 3222 546 \n",
       "Q 2797 -91 2034 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"94.061222\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(90.879972 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-32\" d=\"M 819 3553 \n",
       "L 469 3553 \n",
       "L 469 4384 \n",
       "Q 803 4563 1142 4656 \n",
       "Q 1481 4750 1806 4750 \n",
       "Q 2534 4750 2956 4397 \n",
       "Q 3378 4044 3378 3438 \n",
       "Q 3378 2753 2422 1800 \n",
       "Q 2347 1728 2309 1691 \n",
       "L 1131 513 \n",
       "L 3078 513 \n",
       "L 3078 1088 \n",
       "L 3444 1088 \n",
       "L 3444 0 \n",
       "L 434 0 \n",
       "L 434 341 \n",
       "L 1850 1753 \n",
       "Q 2319 2222 2519 2614 \n",
       "Q 2719 3006 2719 3438 \n",
       "Q 2719 3909 2473 4175 \n",
       "Q 2228 4441 1797 4441 \n",
       "Q 1350 4441 1106 4219 \n",
       "Q 863 3997 819 3553 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"136.827699\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(133.646449 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-34\" d=\"M 2234 1581 \n",
       "L 2234 4063 \n",
       "L 641 1581 \n",
       "L 2234 1581 \n",
       "z\n",
       "M 3609 0 \n",
       "L 1484 0 \n",
       "L 1484 331 \n",
       "L 2234 331 \n",
       "L 2234 1247 \n",
       "L 197 1247 \n",
       "L 197 1588 \n",
       "L 2241 4750 \n",
       "L 2859 4750 \n",
       "L 2859 1581 \n",
       "L 3750 1581 \n",
       "L 3750 1247 \n",
       "L 2859 1247 \n",
       "L 2859 331 \n",
       "L 3609 331 \n",
       "L 3609 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"179.594176\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(176.412926 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-36\" d=\"M 2094 219 \n",
       "Q 2534 219 2771 542 \n",
       "Q 3009 866 3009 1472 \n",
       "Q 3009 2078 2771 2401 \n",
       "Q 2534 2725 2094 2725 \n",
       "Q 1647 2725 1412 2412 \n",
       "Q 1178 2100 1178 1509 \n",
       "Q 1178 888 1415 553 \n",
       "Q 1653 219 2094 219 \n",
       "z\n",
       "M 1075 2569 \n",
       "Q 1288 2803 1556 2918 \n",
       "Q 1825 3034 2163 3034 \n",
       "Q 2859 3034 3264 2615 \n",
       "Q 3669 2197 3669 1472 \n",
       "Q 3669 763 3233 336 \n",
       "Q 2797 -91 2069 -91 \n",
       "Q 1278 -91 853 498 \n",
       "Q 428 1088 428 2181 \n",
       "Q 428 3406 931 4078 \n",
       "Q 1434 4750 2350 4750 \n",
       "Q 2597 4750 2869 4703 \n",
       "Q 3141 4656 3425 4563 \n",
       "L 3425 3794 \n",
       "L 3072 3794 \n",
       "Q 3034 4109 2831 4275 \n",
       "Q 2628 4441 2284 4441 \n",
       "Q 1678 4441 1381 3981 \n",
       "Q 1084 3522 1075 2569 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"222.360653\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <g transform=\"translate(219.179403 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-38\" d=\"M 2981 1275 \n",
       "Q 2981 1775 2732 2051 \n",
       "Q 2484 2328 2034 2328 \n",
       "Q 1584 2328 1336 2051 \n",
       "Q 1088 1775 1088 1275 \n",
       "Q 1088 772 1336 495 \n",
       "Q 1584 219 2034 219 \n",
       "Q 2484 219 2732 495 \n",
       "Q 2981 772 2981 1275 \n",
       "z\n",
       "M 2853 3541 \n",
       "Q 2853 3966 2637 4203 \n",
       "Q 2422 4441 2034 4441 \n",
       "Q 1650 4441 1433 4203 \n",
       "Q 1216 3966 1216 3541 \n",
       "Q 1216 3113 1433 2875 \n",
       "Q 1650 2638 2034 2638 \n",
       "Q 2422 2638 2637 2875 \n",
       "Q 2853 3113 2853 3541 \n",
       "z\n",
       "M 2516 2484 \n",
       "Q 3047 2413 3344 2092 \n",
       "Q 3641 1772 3641 1275 \n",
       "Q 3641 619 3225 264 \n",
       "Q 2809 -91 2034 -91 \n",
       "Q 1263 -91 845 264 \n",
       "Q 428 619 428 1275 \n",
       "Q 428 1772 725 2092 \n",
       "Q 1022 2413 1556 2484 \n",
       "Q 1084 2569 832 2842 \n",
       "Q 581 3116 581 3541 \n",
       "Q 581 4103 968 4426 \n",
       "Q 1356 4750 2034 4750 \n",
       "Q 2713 4750 3100 4426 \n",
       "Q 3488 4103 3488 3541 \n",
       "Q 3488 3116 3236 2842 \n",
       "Q 2984 2569 2516 2484 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-38\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"265.127131\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(258.764631 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-31\" d=\"M 909 0 \n",
       "L 909 331 \n",
       "L 1722 331 \n",
       "L 1722 4213 \n",
       "L 781 3603 \n",
       "L 781 4013 \n",
       "L 1919 4750 \n",
       "L 2350 4750 \n",
       "L 2350 331 \n",
       "L 3163 331 \n",
       "L 3163 0 \n",
       "L 909 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- $t$ -->\n",
       "     <g transform=\"translate(156.210938 271.520312) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Oblique-74\" d=\"M 2706 3500 \n",
       "L 2619 3053 \n",
       "L 1472 3053 \n",
       "L 1100 1153 \n",
       "Q 1081 1047 1072 975 \n",
       "Q 1063 903 1063 863 \n",
       "Q 1063 663 1183 572 \n",
       "Q 1303 481 1569 481 \n",
       "L 2150 481 \n",
       "L 2053 0 \n",
       "L 1503 0 \n",
       "Q 991 0 739 200 \n",
       "Q 488 400 488 806 \n",
       "Q 488 878 497 964 \n",
       "Q 506 1050 525 1153 \n",
       "L 897 3053 \n",
       "L 409 3053 \n",
       "L 500 3500 \n",
       "L 978 3500 \n",
       "L 1172 4494 \n",
       "L 1747 4494 \n",
       "L 1556 3500 \n",
       "L 2706 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-74\" transform=\"translate(0 0.78125)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m1f421ac517\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"40.603125\" y=\"220.869692\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.240625 224.668911) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"40.603125\" y=\"176.005981\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(20.878125 179.8052) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"40.603125\" y=\"131.142271\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(20.878125 134.941489) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"40.603125\" y=\"86.27856\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(20.878125 90.077779) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"40.603125\" y=\"41.414849\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(20.878125 45.214068) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- $T$ -->\n",
       "     <g transform=\"translate(14.798438 135.880937) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Oblique-54\" d=\"M 378 4666 \n",
       "L 4325 4666 \n",
       "L 4225 4134 \n",
       "L 2559 4134 \n",
       "L 1759 0 \n",
       "L 1125 0 \n",
       "L 1925 4134 \n",
       "L 275 4134 \n",
       "L 378 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-54\" transform=\"translate(0 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 51.294744 232.08562 \n",
       "L 53.305171 231.228102 \n",
       "L 54.673972 230.4128 \n",
       "L 55.700573 229.554866 \n",
       "L 56.598848 228.524048 \n",
       "L 57.454349 227.201106 \n",
       "L 58.309849 225.439481 \n",
       "L 59.16535 223.12015 \n",
       "L 60.020851 220.10934 \n",
       "L 60.876352 216.258504 \n",
       "L 61.774627 211.132158 \n",
       "L 62.672902 204.688971 \n",
       "L 63.656729 195.936583 \n",
       "L 64.85443 183.141237 \n",
       "L 66.394331 164.060704 \n",
       "L 71.014035 104.976742 \n",
       "L 72.040635 95.557842 \n",
       "L 72.938912 89.220773 \n",
       "L 73.794411 84.688014 \n",
       "L 74.564361 81.744731 \n",
       "L 75.248761 79.946248 \n",
       "L 75.847612 78.944634 \n",
       "L 76.360913 78.472301 \n",
       "L 76.83144 78.324666 \n",
       "L 77.301964 78.425383 \n",
       "L 77.815265 78.790367 \n",
       "L 78.414115 79.512063 \n",
       "L 79.184065 80.831435 \n",
       "L 80.167891 83.000251 \n",
       "L 81.836119 87.324654 \n",
       "L 85.386445 97.177226 \n",
       "L 89.749497 109.09966 \n",
       "L 92.829302 116.850212 \n",
       "L 96.208526 124.729283 \n",
       "L 99.502204 131.849333 \n",
       "L 102.83866 138.53198 \n",
       "L 106.175111 144.720558 \n",
       "L 109.468788 150.369801 \n",
       "L 112.719692 155.507948 \n",
       "L 115.84227 160.031328 \n",
       "L 118.323219 163.277069 \n",
       "L 120.376422 165.582999 \n",
       "L 121.830774 166.919221 \n",
       "L 122.857375 167.586428 \n",
       "L 123.670099 167.863678 \n",
       "L 124.354502 167.864918 \n",
       "L 124.953352 167.647714 \n",
       "L 125.552202 167.185042 \n",
       "L 126.151053 166.432854 \n",
       "L 126.792677 165.250959 \n",
       "L 127.47708 163.490482 \n",
       "L 128.204251 160.961636 \n",
       "L 128.974206 157.424287 \n",
       "L 129.701377 153.033593 \n",
       "L 130.599652 146.114608 \n",
       "L 131.711806 135.686017 \n",
       "L 133.208932 119.350538 \n",
       "L 138.213614 62.860177 \n",
       "L 139.325757 53.510033 \n",
       "L 140.224033 47.685781 \n",
       "L 140.951214 44.32535 \n",
       "L 141.592838 42.373515 \n",
       "L 142.148915 41.299123 \n",
       "L 142.662218 40.758145 \n",
       "L 143.089964 40.60304 \n",
       "L 143.51771 40.689278 \n",
       "L 143.98824 41.033871 \n",
       "L 144.544317 41.737862 \n",
       "L 145.228714 42.979699 \n",
       "L 146.12699 45.11308 \n",
       "L 147.324691 48.59651 \n",
       "L 149.121242 54.594916 \n",
       "L 157.547922 83.517509 \n",
       "L 160.841599 93.700752 \n",
       "L 164.006956 102.778994 \n",
       "L 167.215086 111.303987 \n",
       "L 170.46599 119.312647 \n",
       "L 173.759667 126.831431 \n",
       "L 177.138891 133.996697 \n",
       "L 180.347022 140.291186 \n",
       "L 183.255716 145.522587 \n",
       "L 185.822223 149.692231 \n",
       "L 187.960973 152.766625 \n",
       "L 189.287004 154.323332 \n",
       "L 190.270827 155.153227 \n",
       "L 191.040772 155.526405 \n",
       "L 191.682406 155.601361 \n",
       "L 192.281257 155.438764 \n",
       "L 192.837323 155.053944 \n",
       "L 193.436174 154.350586 \n",
       "L 194.077808 153.217604 \n",
       "L 194.762205 151.51538 \n",
       "L 195.489377 149.06946 \n",
       "L 196.259332 145.66336 \n",
       "L 197.07206 141.030286 \n",
       "L 197.927552 134.834015 \n",
       "L 198.868611 126.212695 \n",
       "L 200.151859 112.195694 \n",
       "L 205.113757 55.706412 \n",
       "L 206.225911 46.410469 \n",
       "L 207.16696 40.260723 \n",
       "L 207.936915 36.528504 \n",
       "L 208.578539 34.366063 \n",
       "L 209.134616 33.182157 \n",
       "L 209.647919 32.567965 \n",
       "L 210.075665 32.368876 \n",
       "L 210.503411 32.425541 \n",
       "L 210.973941 32.752986 \n",
       "L 211.530017 33.455693 \n",
       "L 212.214415 34.721078 \n",
       "L 213.069917 36.804675 \n",
       "L 214.224844 40.263974 \n",
       "L 215.935848 46.190552 \n",
       "L 224.918605 78.334096 \n",
       "L 228.212282 88.896725 \n",
       "L 231.420412 98.44179 \n",
       "L 234.585759 107.17081 \n",
       "L 237.836652 115.485119 \n",
       "L 241.087566 123.186605 \n",
       "L 244.424007 130.527505 \n",
       "L 247.632137 137.065999 \n",
       "L 250.540842 142.510754 \n",
       "L 253.107338 146.862651 \n",
       "L 255.203315 150.0166 \n",
       "L 256.486583 151.593454 \n",
       "L 257.470406 152.469831 \n",
       "L 258.240351 152.872562 \n",
       "L 258.881985 152.968101 \n",
       "L 259.438052 152.840716 \n",
       "L 259.994139 152.487951 \n",
       "L 260.592989 151.82064 \n",
       "L 261.234603 150.72931 \n",
       "L 261.919 149.076571 \n",
       "L 262.646182 146.691227 \n",
       "L 263.416127 143.361438 \n",
       "L 264.228855 138.826352 \n",
       "L 265.084367 132.767054 \n",
       "L 265.127131 132.426533 \n",
       "L 265.127131 132.426533 \n",
       "\" clip-path=\"url(#pb41f46ab78)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <defs>\n",
       "     <path id=\"m3aa3c876aa\" d=\"M -3 3 \n",
       "L 3 -3 \n",
       "M -3 -3 \n",
       "L 3 3 \n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pb41f46ab78)\">\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"185.095051\" y=\"149.432922\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"255.160551\" y=\"155.463301\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"138.76969\" y=\"62.037859\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"253.021791\" y=\"146.073885\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"83.889319\" y=\"92.966384\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"127.690952\" y=\"166.1846\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"57.154924\" y=\"231.3093\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"247.076071\" y=\"135.421372\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"144.031013\" y=\"41.866252\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"241.515323\" y=\"119.411575\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"183.640699\" y=\"147.275598\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"55.529472\" y=\"233.201676\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"131.198503\" y=\"138.895561\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"187.362122\" y=\"149.555502\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"94.326428\" y=\"126.507999\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"145.613687\" y=\"53.61265\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"157.376827\" y=\"84.188272\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"105.619034\" y=\"137.595083\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"165.076336\" y=\"104.302504\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"74.436038\" y=\"80.607342\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"106.902287\" y=\"147.660906\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"98.005078\" y=\"127.447172\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"242.114173\" y=\"120.044816\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"57.967649\" y=\"227.541866\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"113.361316\" y=\"164.351983\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"242.413598\" y=\"119.183153\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"260.207996\" y=\"154.653537\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"127.733726\" y=\"162.097532\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"261.961784\" y=\"154.733323\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"192.110152\" y=\"161.683579\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"254.133945\" y=\"145.844949\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"251.567449\" y=\"142.249352\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"76.104263\" y=\"78.479044\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"116.740545\" y=\"161.524101\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"245.450614\" y=\"131.296717\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"134.706058\" y=\"100.923932\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"255.246099\" y=\"150.310454\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"191.639622\" y=\"150.364941\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"225.046936\" y=\"71.036957\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"53.134071\" y=\"229.662898\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"230.736015\" y=\"100.686764\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"169.696035\" y=\"117.974834\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"113.703515\" y=\"153.727251\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"243.311874\" y=\"124.054926\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"94.625853\" y=\"125.495552\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"215.893075\" y=\"47.471358\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"217.005218\" y=\"56.484027\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"132.909507\" y=\"123.234054\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"69.388583\" y=\"127.990213\" style=\"stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3aa3c876aa\" x=\"140.993988\" y=\"45.934833\" style=\"stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 51.294744 145.112489 \n",
       "L 54.502872 144.784343 \n",
       "L 265.127131 115.414977 \n",
       "L 265.127131 115.414977 \n",
       "\" clip-path=\"url(#pb41f46ab78)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 243.24375 \n",
       "L 40.603125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 275.81875 243.24375 \n",
       "L 275.81875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 243.24375 \n",
       "L 275.81875 243.24375 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 22.318125 \n",
       "L 275.81875 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_14\">\n",
       "    <!-- Prediction -->\n",
       "    <g transform=\"translate(126.828125 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSerif-50\" d=\"M 1581 2375 \n",
       "L 2406 2375 \n",
       "Q 2872 2375 3115 2626 \n",
       "Q 3359 2878 3359 3353 \n",
       "Q 3359 3831 3115 4081 \n",
       "Q 2872 4331 2406 4331 \n",
       "L 1581 4331 \n",
       "L 1581 2375 \n",
       "z\n",
       "M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2559 4666 \n",
       "Q 3259 4666 3668 4311 \n",
       "Q 4078 3956 4078 3353 \n",
       "Q 4078 2753 3668 2397 \n",
       "Q 3259 2041 2559 2041 \n",
       "L 1581 2041 \n",
       "L 1581 331 \n",
       "L 2303 331 \n",
       "L 2303 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-72\" d=\"M 3059 3328 \n",
       "L 3059 2497 \n",
       "L 2728 2497 \n",
       "Q 2713 2744 2591 2866 \n",
       "Q 2469 2988 2234 2988 \n",
       "Q 1809 2988 1582 2694 \n",
       "Q 1356 2400 1356 1850 \n",
       "L 1356 331 \n",
       "L 2022 331 \n",
       "L 2022 0 \n",
       "L 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2994 \n",
       "L 231 2994 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1525 3078 1790 3245 \n",
       "Q 2056 3413 2438 3413 \n",
       "Q 2578 3413 2733 3391 \n",
       "Q 2888 3369 3059 3328 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-65\" d=\"M 3469 1600 \n",
       "L 991 1600 \n",
       "L 991 1575 \n",
       "Q 991 903 1244 561 \n",
       "Q 1497 219 1991 219 \n",
       "Q 2369 219 2611 417 \n",
       "Q 2853 616 2950 1006 \n",
       "L 3413 1006 \n",
       "Q 3275 459 2904 184 \n",
       "Q 2534 -91 1931 -91 \n",
       "Q 1203 -91 761 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2450 753 2931 \n",
       "Q 1188 3413 1894 3413 \n",
       "Q 2647 3413 3050 2948 \n",
       "Q 3453 2484 3469 1600 \n",
       "z\n",
       "M 2791 1931 \n",
       "Q 2772 2513 2545 2808 \n",
       "Q 2319 3103 1894 3103 \n",
       "Q 1497 3103 1269 2806 \n",
       "Q 1041 2509 991 1931 \n",
       "L 2791 1931 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-64\" d=\"M 3359 331 \n",
       "L 3909 331 \n",
       "L 3909 0 \n",
       "L 2784 0 \n",
       "L 2784 519 \n",
       "Q 2616 206 2355 57 \n",
       "Q 2094 -91 1709 -91 \n",
       "Q 1097 -91 708 395 \n",
       "Q 319 881 319 1663 \n",
       "Q 319 2444 706 2928 \n",
       "Q 1094 3413 1709 3413 \n",
       "Q 2094 3413 2355 3264 \n",
       "Q 2616 3116 2784 2803 \n",
       "L 2784 4531 \n",
       "L 2241 4531 \n",
       "L 2241 4863 \n",
       "L 3359 4863 \n",
       "L 3359 331 \n",
       "z\n",
       "M 2784 1497 \n",
       "L 2784 1825 \n",
       "Q 2784 2422 2554 2737 \n",
       "Q 2325 3053 1888 3053 \n",
       "Q 1444 3053 1217 2703 \n",
       "Q 991 2353 991 1663 \n",
       "Q 991 975 1217 622 \n",
       "Q 1444 269 1888 269 \n",
       "Q 2325 269 2554 583 \n",
       "Q 2784 897 2784 1497 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-69\" d=\"M 622 4353 \n",
       "Q 622 4497 726 4603 \n",
       "Q 831 4709 978 4709 \n",
       "Q 1122 4709 1226 4603 \n",
       "Q 1331 4497 1331 4353 \n",
       "Q 1331 4206 1228 4103 \n",
       "Q 1125 4000 978 4000 \n",
       "Q 831 4000 726 4103 \n",
       "Q 622 4206 622 4353 \n",
       "z\n",
       "M 1356 331 \n",
       "L 1900 331 \n",
       "L 1900 0 \n",
       "L 231 0 \n",
       "L 231 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 331 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-63\" d=\"M 3291 997 \n",
       "Q 3169 466 2822 187 \n",
       "Q 2475 -91 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2459 759 2936 \n",
       "Q 1200 3413 1925 3413 \n",
       "Q 2241 3413 2553 3339 \n",
       "Q 2866 3266 3181 3116 \n",
       "L 3181 2266 \n",
       "L 2847 2266 \n",
       "Q 2781 2703 2561 2903 \n",
       "Q 2341 3103 1931 3103 \n",
       "Q 1466 3103 1228 2742 \n",
       "Q 991 2381 991 1663 \n",
       "Q 991 944 1227 581 \n",
       "Q 1463 219 1931 219 \n",
       "Q 2303 219 2525 412 \n",
       "Q 2747 606 2828 997 \n",
       "L 3291 997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-74\" d=\"M 691 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 691 3322 \n",
       "L 691 4353 \n",
       "L 1269 4353 \n",
       "L 1269 3322 \n",
       "L 2350 3322 \n",
       "L 2350 2988 \n",
       "L 1269 2988 \n",
       "L 1269 878 \n",
       "Q 1269 456 1350 337 \n",
       "Q 1431 219 1650 219 \n",
       "Q 1875 219 1978 351 \n",
       "Q 2081 484 2088 781 \n",
       "L 2522 781 \n",
       "Q 2497 328 2275 118 \n",
       "Q 2053 -91 1600 -91 \n",
       "Q 1103 -91 897 129 \n",
       "Q 691 350 691 878 \n",
       "L 691 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-6f\" d=\"M 1925 219 \n",
       "Q 2388 219 2623 584 \n",
       "Q 2859 950 2859 1663 \n",
       "Q 2859 2375 2623 2739 \n",
       "Q 2388 3103 1925 3103 \n",
       "Q 1463 3103 1227 2739 \n",
       "Q 991 2375 991 1663 \n",
       "Q 991 950 1228 584 \n",
       "Q 1466 219 1925 219 \n",
       "z\n",
       "M 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2456 758 2934 \n",
       "Q 1197 3413 1925 3413 \n",
       "Q 2653 3413 3092 2934 \n",
       "Q 3531 2456 3531 1663 \n",
       "Q 3531 869 3092 389 \n",
       "Q 2653 -91 1925 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-6e\" d=\"M 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1516 3069 1770 3241 \n",
       "Q 2025 3413 2363 3413 \n",
       "Q 2913 3413 3172 3097 \n",
       "Q 3431 2781 3431 2113 \n",
       "L 3431 331 \n",
       "L 3944 331 \n",
       "L 3944 0 \n",
       "L 2356 0 \n",
       "L 2356 331 \n",
       "L 2853 331 \n",
       "L 2853 1931 \n",
       "Q 2853 2541 2703 2767 \n",
       "Q 2553 2994 2175 2994 \n",
       "Q 1775 2994 1565 2701 \n",
       "Q 1356 2409 1356 1850 \n",
       "L 1356 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 263 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSerif-50\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-72\" x=\"67.285156\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-65\" x=\"115.087891\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-64\" x=\"174.267578\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-69\" x=\"238.28125\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-63\" x=\"270.263672\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-74\" x=\"326.269531\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-69\" x=\"366.455078\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6f\" x=\"398.4375\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6e\" x=\"458.642578\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 166.5875 238.24375 \n",
       "L 268.81875 238.24375 \n",
       "Q 270.81875 238.24375 270.81875 236.24375 \n",
       "L 270.81875 193.067187 \n",
       "Q 270.81875 191.067187 268.81875 191.067187 \n",
       "L 166.5875 191.067187 \n",
       "Q 164.5875 191.067187 164.5875 193.067187 \n",
       "L 164.5875 236.24375 \n",
       "Q 164.5875 238.24375 166.5875 238.24375 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\">\n",
       "     <path d=\"M 168.5875 199.165625 \n",
       "L 178.5875 199.165625 \n",
       "L 188.5875 199.165625 \n",
       "\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- True -->\n",
       "     <g transform=\"translate(196.5875 202.665625) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-54\" d=\"M 1222 0 \n",
       "L 1222 331 \n",
       "L 1819 331 \n",
       "L 1819 4294 \n",
       "L 447 4294 \n",
       "L 447 3566 \n",
       "L 63 3566 \n",
       "L 63 4666 \n",
       "L 4206 4666 \n",
       "L 4206 3566 \n",
       "L 3822 3566 \n",
       "L 3822 4294 \n",
       "L 2450 4294 \n",
       "L 2450 331 \n",
       "L 3047 331 \n",
       "L 3047 0 \n",
       "L 1222 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-75\" d=\"M 2266 3322 \n",
       "L 3341 3322 \n",
       "L 3341 331 \n",
       "L 3884 331 \n",
       "L 3884 0 \n",
       "L 2766 0 \n",
       "L 2766 588 \n",
       "Q 2606 256 2353 82 \n",
       "Q 2100 -91 1766 -91 \n",
       "Q 1213 -91 952 223 \n",
       "Q 691 538 691 1209 \n",
       "L 691 2988 \n",
       "L 172 2988 \n",
       "L 172 3322 \n",
       "L 1269 3322 \n",
       "L 1269 1388 \n",
       "Q 1269 781 1417 556 \n",
       "Q 1566 331 1947 331 \n",
       "Q 2347 331 2556 625 \n",
       "Q 2766 919 2766 1478 \n",
       "L 2766 2988 \n",
       "L 2266 2988 \n",
       "L 2266 3322 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-75\" x=\"114.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"178.90625\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <g>\n",
       "      <use xlink:href=\"#m3aa3c876aa\" x=\"178.5875\" y=\"213.84375\" style=\"stroke: #000000\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- Training Data -->\n",
       "     <g transform=\"translate(196.5875 217.34375) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-61\" d=\"M 2547 1044 \n",
       "L 2547 1747 \n",
       "L 1806 1747 \n",
       "Q 1378 1747 1168 1562 \n",
       "Q 959 1378 959 997 \n",
       "Q 959 650 1171 447 \n",
       "Q 1384 244 1747 244 \n",
       "Q 2106 244 2326 466 \n",
       "Q 2547 688 2547 1044 \n",
       "z\n",
       "M 3122 2075 \n",
       "L 3122 331 \n",
       "L 3634 331 \n",
       "L 3634 0 \n",
       "L 2547 0 \n",
       "L 2547 359 \n",
       "Q 2356 128 2106 18 \n",
       "Q 1856 -91 1522 -91 \n",
       "Q 969 -91 644 203 \n",
       "Q 319 497 319 997 \n",
       "Q 319 1513 691 1797 \n",
       "Q 1063 2081 1741 2081 \n",
       "L 2547 2081 \n",
       "L 2547 2309 \n",
       "Q 2547 2688 2317 2895 \n",
       "Q 2088 3103 1672 3103 \n",
       "Q 1328 3103 1125 2947 \n",
       "Q 922 2791 872 2484 \n",
       "L 575 2484 \n",
       "L 575 3156 \n",
       "Q 875 3284 1158 3348 \n",
       "Q 1441 3413 1709 3413 \n",
       "Q 2400 3413 2761 3070 \n",
       "Q 3122 2728 3122 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-67\" d=\"M 3359 2988 \n",
       "L 3359 72 \n",
       "Q 3359 -644 2965 -1033 \n",
       "Q 2572 -1422 1844 -1422 \n",
       "Q 1516 -1422 1216 -1362 \n",
       "Q 916 -1303 641 -1184 \n",
       "L 641 -488 \n",
       "L 941 -488 \n",
       "Q 997 -813 1206 -963 \n",
       "Q 1416 -1113 1806 -1113 \n",
       "Q 2313 -1113 2548 -827 \n",
       "Q 2784 -541 2784 72 \n",
       "L 2784 519 \n",
       "Q 2616 206 2355 57 \n",
       "Q 2094 -91 1709 -91 \n",
       "Q 1097 -91 708 395 \n",
       "Q 319 881 319 1663 \n",
       "Q 319 2444 706 2928 \n",
       "Q 1094 3413 1709 3413 \n",
       "Q 2094 3413 2355 3264 \n",
       "Q 2616 3116 2784 2803 \n",
       "L 2784 3322 \n",
       "L 3909 3322 \n",
       "L 3909 2988 \n",
       "L 3359 2988 \n",
       "z\n",
       "M 2784 1825 \n",
       "Q 2784 2422 2554 2737 \n",
       "Q 2325 3053 1888 3053 \n",
       "Q 1444 3053 1217 2703 \n",
       "Q 991 2353 991 1663 \n",
       "Q 991 975 1217 622 \n",
       "Q 1444 269 1888 269 \n",
       "Q 2325 269 2554 583 \n",
       "Q 2784 897 2784 1497 \n",
       "L 2784 1825 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-44\" d=\"M 1581 331 \n",
       "L 2163 331 \n",
       "Q 3072 331 3558 850 \n",
       "Q 4044 1369 4044 2338 \n",
       "Q 4044 3306 3559 3818 \n",
       "Q 3075 4331 2163 4331 \n",
       "L 1581 4331 \n",
       "L 1581 331 \n",
       "z\n",
       "M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2209 4666 \n",
       "Q 3416 4666 4089 4050 \n",
       "Q 4763 3434 4763 2338 \n",
       "Q 4763 1238 4088 619 \n",
       "Q 3413 0 2209 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"114.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"174.121094\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"206.103516\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"270.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"302.490234\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-67\" x=\"366.894531\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"430.908203\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-44\" x=\"462.695312\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"542.871094\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"602.490234\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"642.675781\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 168.5875 228.664062 \n",
       "L 178.5875 228.664062 \n",
       "L 188.5875 228.664062 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff0000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Prediction -->\n",
       "     <g transform=\"translate(196.5875 232.164062) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"67.285156\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"115.087891\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-64\" x=\"174.267578\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"238.28125\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"270.263672\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"326.269531\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"366.455078\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"398.4375\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"458.642578\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 326.384375 243.24375 \n",
       "L 561.6 243.24375 \n",
       "L 561.6 22.318125 \n",
       "L 326.384375 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"337.075994\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(333.894744 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"375.260349\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_19\">\n",
       "      <!-- 2500 -->\n",
       "      <g transform=\"translate(362.535349 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-35\" d=\"M 3219 4666 \n",
       "L 3219 4153 \n",
       "L 1081 4153 \n",
       "L 1081 2816 \n",
       "Q 1244 2928 1461 2984 \n",
       "Q 1678 3041 1947 3041 \n",
       "Q 2703 3041 3140 2622 \n",
       "Q 3578 2203 3578 1478 \n",
       "Q 3578 738 3136 323 \n",
       "Q 2694 -91 1894 -91 \n",
       "Q 1572 -91 1234 -12 \n",
       "Q 897 66 544 225 \n",
       "L 544 1131 \n",
       "L 897 1131 \n",
       "Q 925 688 1179 453 \n",
       "Q 1434 219 1894 219 \n",
       "Q 2388 219 2653 544 \n",
       "Q 2919 869 2919 1478 \n",
       "Q 2919 2084 2655 2407 \n",
       "Q 2391 2731 1894 2731 \n",
       "Q 1613 2731 1398 2631 \n",
       "Q 1184 2531 1019 2322 \n",
       "L 750 2322 \n",
       "L 750 4666 \n",
       "L 3219 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"413.444704\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_20\">\n",
       "      <!-- 5000 -->\n",
       "      <g transform=\"translate(400.719704 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_10\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"451.629058\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_21\">\n",
       "      <!-- 7500 -->\n",
       "      <g transform=\"translate(438.904058 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-37\" d=\"M 3609 4347 \n",
       "L 1784 0 \n",
       "L 1319 0 \n",
       "L 3059 4153 \n",
       "L 903 4153 \n",
       "L 903 3578 \n",
       "L 538 3578 \n",
       "L 538 4666 \n",
       "L 3609 4666 \n",
       "L 3609 4347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_11\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"489.813413\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 10000 -->\n",
       "      <g transform=\"translate(473.907163 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_12\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5753964004\" x=\"527.997768\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 12500 -->\n",
       "      <g transform=\"translate(512.091518 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_24\">\n",
       "     <!-- Epoch -->\n",
       "     <g transform=\"translate(428.110937 271.520312) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-45\" d=\"M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 4109 4666 \n",
       "L 4109 3628 \n",
       "L 3725 3628 \n",
       "L 3725 4281 \n",
       "L 1581 4281 \n",
       "L 1581 2719 \n",
       "L 3109 2719 \n",
       "L 3109 3303 \n",
       "L 3494 3303 \n",
       "L 3494 1753 \n",
       "L 3109 1753 \n",
       "L 3109 2338 \n",
       "L 1581 2338 \n",
       "L 1581 384 \n",
       "L 3775 384 \n",
       "L 3775 1038 \n",
       "L 4159 1038 \n",
       "L 4159 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-70\" d=\"M 1313 1825 \n",
       "L 1313 1497 \n",
       "Q 1313 897 1542 583 \n",
       "Q 1772 269 2209 269 \n",
       "Q 2650 269 2876 622 \n",
       "Q 3103 975 3103 1663 \n",
       "Q 3103 2353 2876 2703 \n",
       "Q 2650 3053 2209 3053 \n",
       "Q 1772 3053 1542 2737 \n",
       "Q 1313 2422 1313 1825 \n",
       "z\n",
       "M 738 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 1313 3322 \n",
       "L 1313 2803 \n",
       "Q 1481 3116 1742 3264 \n",
       "Q 2003 3413 2388 3413 \n",
       "Q 3000 3413 3387 2928 \n",
       "Q 3775 2444 3775 1663 \n",
       "Q 3775 881 3387 395 \n",
       "Q 3000 -91 2388 -91 \n",
       "Q 2003 -91 1742 57 \n",
       "Q 1481 206 1313 519 \n",
       "L 1313 -997 \n",
       "L 1856 -997 \n",
       "L 1856 -1331 \n",
       "L 184 -1331 \n",
       "L 184 -997 \n",
       "L 738 -997 \n",
       "L 738 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-68\" d=\"M 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 4531 \n",
       "L 231 4531 \n",
       "L 231 4863 \n",
       "L 1356 4863 \n",
       "L 1356 2731 \n",
       "Q 1516 3069 1770 3241 \n",
       "Q 2025 3413 2363 3413 \n",
       "Q 2913 3413 3172 3097 \n",
       "Q 3431 2781 3431 2113 \n",
       "L 3431 331 \n",
       "L 3944 331 \n",
       "L 3944 0 \n",
       "L 2356 0 \n",
       "L 2356 331 \n",
       "L 2853 331 \n",
       "L 2853 1931 \n",
       "Q 2853 2541 2704 2764 \n",
       "Q 2556 2988 2175 2988 \n",
       "Q 1775 2988 1565 2697 \n",
       "Q 1356 2406 1356 1850 \n",
       "L 1356 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 263 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-70\" x=\"72.998047\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"137.011719\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"197.216797\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-68\" x=\"253.222656\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_4\">\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"326.384375\" y=\"204.444786\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(300.296875 208.244005) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"326.384375\" y=\"165.325569\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(300.296875 169.124788) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"326.384375\" y=\"126.206352\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(300.296875 130.00557) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"326.384375\" y=\"87.087134\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(300.296875 90.886353) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1f421ac517\" x=\"326.384375\" y=\"47.967917\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(300.296875 51.767135) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-33\" d=\"M 622 4469 \n",
       "Q 988 4606 1323 4678 \n",
       "Q 1659 4750 1953 4750 \n",
       "Q 2638 4750 3022 4454 \n",
       "Q 3406 4159 3406 3634 \n",
       "Q 3406 3213 3140 2930 \n",
       "Q 2875 2647 2388 2547 \n",
       "Q 2963 2466 3280 2130 \n",
       "Q 3597 1794 3597 1259 \n",
       "Q 3597 606 3158 257 \n",
       "Q 2719 -91 1894 -91 \n",
       "Q 1528 -91 1179 -12 \n",
       "Q 831 66 488 225 \n",
       "L 488 1131 \n",
       "L 838 1131 \n",
       "Q 869 681 1141 450 \n",
       "Q 1413 219 1906 219 \n",
       "Q 2384 219 2661 495 \n",
       "Q 2938 772 2938 1253 \n",
       "Q 2938 1803 2653 2086 \n",
       "Q 2369 2369 1819 2369 \n",
       "L 1522 2369 \n",
       "L 1522 2688 \n",
       "L 1678 2688 \n",
       "Q 2225 2688 2498 2914 \n",
       "Q 2772 3141 2772 3597 \n",
       "Q 2772 4006 2547 4223 \n",
       "Q 2322 4441 1900 4441 \n",
       "Q 1478 4441 1245 4241 \n",
       "Q 1013 4041 972 3647 \n",
       "L 622 3647 \n",
       "L 622 4469 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_30\">\n",
       "     <!-- Loss -->\n",
       "     <g transform=\"translate(294.217188 144.242656) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-4c\" d=\"M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2175 4666 \n",
       "L 2175 4331 \n",
       "L 1581 4331 \n",
       "L 1581 384 \n",
       "L 3713 384 \n",
       "L 3713 1166 \n",
       "L 4097 1166 \n",
       "L 4097 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-73\" d=\"M 359 184 \n",
       "L 359 959 \n",
       "L 691 959 \n",
       "Q 703 588 923 403 \n",
       "Q 1144 219 1575 219 \n",
       "Q 1963 219 2166 364 \n",
       "Q 2369 509 2369 788 \n",
       "Q 2369 1006 2220 1140 \n",
       "Q 2072 1275 1594 1428 \n",
       "L 1178 1569 \n",
       "Q 750 1706 558 1912 \n",
       "Q 366 2119 366 2438 \n",
       "Q 366 2894 700 3153 \n",
       "Q 1034 3413 1625 3413 \n",
       "Q 1888 3413 2178 3344 \n",
       "Q 2469 3275 2778 3144 \n",
       "L 2778 2419 \n",
       "L 2447 2419 \n",
       "Q 2434 2741 2221 2922 \n",
       "Q 2009 3103 1644 3103 \n",
       "Q 1281 3103 1095 2975 \n",
       "Q 909 2847 909 2591 \n",
       "Q 909 2381 1050 2254 \n",
       "Q 1191 2128 1613 1997 \n",
       "L 2069 1856 \n",
       "Q 2541 1709 2748 1489 \n",
       "Q 2956 1269 2956 922 \n",
       "Q 2956 450 2595 179 \n",
       "Q 2234 -91 1600 -91 \n",
       "Q 1278 -91 972 -22 \n",
       "Q 666 47 359 184 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"66.40625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"126.611328\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"177.929688\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_29\">\n",
       "    <path d=\"M 337.075994 32.360199 \n",
       "L 352.349736 172.711673 \n",
       "L 367.623478 172.711673 \n",
       "L 382.89722 172.711673 \n",
       "L 398.170962 172.711673 \n",
       "L 413.444704 172.711673 \n",
       "L 428.718446 172.711673 \n",
       "L 443.992188 172.711673 \n",
       "L 459.265929 172.711673 \n",
       "L 474.539671 172.711673 \n",
       "L 489.813413 172.711673 \n",
       "L 505.087155 172.711673 \n",
       "L 520.360897 172.711673 \n",
       "L 535.634639 172.711673 \n",
       "L 550.908381 172.711673 \n",
       "\" clip-path=\"url(#p0edd1f6b03)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_30\">\n",
       "    <path d=\"M 337.075994 151.492327 \n",
       "L 352.349736 233.201676 \n",
       "L 367.623478 233.201676 \n",
       "L 382.89722 233.201676 \n",
       "L 398.170962 233.201676 \n",
       "L 413.444704 233.201676 \n",
       "L 428.718446 233.201676 \n",
       "L 443.992188 233.201676 \n",
       "L 459.265929 233.201676 \n",
       "L 474.539671 233.201676 \n",
       "L 489.813413 233.201676 \n",
       "L 505.087155 233.201676 \n",
       "L 520.360897 233.201676 \n",
       "L 535.634639 233.201676 \n",
       "L 550.908381 233.201676 \n",
       "\" clip-path=\"url(#p0edd1f6b03)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 326.384375 243.24375 \n",
       "L 326.384375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 561.6 243.24375 \n",
       "L 561.6 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 326.384375 243.24375 \n",
       "L 561.6 243.24375 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 326.384375 22.318125 \n",
       "L 561.6 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_31\">\n",
       "    <!-- Loss -->\n",
       "    <g transform=\"translate(430.238125 16.318125) scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSerif-4c\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6f\" x=\"66.40625\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-73\" x=\"126.611328\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-73\" x=\"177.929688\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_2\">\n",
       "    <g id=\"patch_13\">\n",
       "     <path d=\"M 469.446875 59.674375 \n",
       "L 554.6 59.674375 \n",
       "Q 556.6 59.674375 556.6 57.674375 \n",
       "L 556.6 29.318125 \n",
       "Q 556.6 27.318125 554.6 27.318125 \n",
       "L 469.446875 27.318125 \n",
       "Q 467.446875 27.318125 467.446875 29.318125 \n",
       "L 467.446875 57.674375 \n",
       "Q 467.446875 59.674375 469.446875 59.674375 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_31\">\n",
       "     <path d=\"M 471.446875 35.416563 \n",
       "L 481.446875 35.416563 \n",
       "L 491.446875 35.416563 \n",
       "\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_32\">\n",
       "     <!-- Train Loss -->\n",
       "     <g transform=\"translate(499.446875 38.916563) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"114.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"174.121094\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"206.103516\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"270.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\" x=\"302.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"368.701172\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"428.90625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"480.224609\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_32\">\n",
       "     <path d=\"M 471.446875 50.094688 \n",
       "L 481.446875 50.094688 \n",
       "L 491.446875 50.094688 \n",
       "\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_33\">\n",
       "     <!-- Test Loss -->\n",
       "     <g transform=\"translate(499.446875 53.594688) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"58.949219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"118.128906\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"169.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"209.632812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\" x=\"241.419922\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"307.826172\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"368.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"419.349609\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pb41f46ab78\">\n",
       "   <rect x=\"40.603125\" y=\"22.318125\" width=\"235.215625\" height=\"220.925625\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p0edd1f6b03\">\n",
       "   <rect x=\"326.384375\" y=\"22.318125\" width=\"235.215625\" height=\"220.925625\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 2400x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Many research models make use of techniques such as learning rate scheduling, and gradient clipping. \n",
    "These may be achieved by chaining together gradient transformations such as optax.adam and optax.clip.\n",
    "We will use Adam with weight decay (optax.adamw), a cosine learning rate schedule (with warmup) and also gradient clipping.\n",
    "\"\"\"\n",
    "\n",
    "# Data & Model from the previous example\n",
    "\n",
    "# Initialize the model\n",
    "model = MLP(key)\n",
    "\n",
    "# Define the learning rate schedule with warmup and cosine decay\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0, peak_value=1.0, warmup_steps=50, decay_steps=1_000, end_value=0.0\n",
    ")\n",
    "\n",
    "# Define the optimizer with gradient clipping and AdamW (Adam with weight decay)\n",
    "optim = optax.chain(\n",
    "    optax.clip(1.0),  # Gradient clipping\n",
    "    optax.adamw(learning_rate=schedule)  # Adam with weight decay\n",
    ")\n",
    "\n",
    "# Initialize the optimizer state\n",
    "opt_state = optim.init(model)\n",
    "\n",
    "# Lists to store training and test loss\n",
    "ltrain_loss, ltest_loss, lepoch = [], [], []\n",
    "\n",
    "# Training loop\n",
    "for step in range(epochs):\n",
    "    model, opt_state, train_loss = make_step(model, opt_state, X_data_train, Y_data_train)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        lepoch.append(step)\n",
    "        ltrain_loss.append(train_loss)\n",
    "        ltest_loss.append(loss(model, X_data_test, Y_data_test)[0])\n",
    "    if step % 5000 == 0:\n",
    "        print(f\"iteration : {step:5d} - Train MSE : {train_loss:.2f} - Test  MSE : {loss(model, X_data_test, Y_data_test)[0]:.2f}\")\n",
    "\n",
    "# Plotting the results\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), dpi=300, tight_layout=True)\n",
    "\n",
    "# First subplot for the prediction\n",
    "axs[0].plot(t, y, 'b', label='True')\n",
    "axs[0].plot(X_data_train, Y_data_train, 'xk', label='Training Data')\n",
    "axs[0].plot(t, vmap(model)(t).squeeze(), '--r', label='Prediction')\n",
    "axs[0].set_xlabel(\"$t$\")\n",
    "axs[0].set_ylabel(\"$T$\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Prediction\")\n",
    "\n",
    "# Second subplot for the loss\n",
    "axs[1].plot(lepoch, ltrain_loss, 'b', label='Train Loss')\n",
    "axs[1].plot(lepoch, ltest_loss, 'r', label='Test Loss')\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].legend()\n",
    "axs[1].set_title(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Regularization\n",
    "\n",
    "### Implicit Regularization in Gradient Descent\n",
    "\n",
    "To explore the concept of implicit regularization, let's consider the continuous form of gradient descent where the step size is infinitesimally small. In this scenario, the change in the parameters $\\theta$ is governed by the differential equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\theta}{\\partial t} = -\\frac{\\partial \\mathcal{L}}{\\partial \\theta}.\n",
    "$$\n",
    "\n",
    "Here, $\\mathcal{L}$ represents the loss function. In practical applications, gradient descent implements discrete updates with a finite step size $\\alpha$:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\partial \\mathcal{L}(\\theta_t)}{\\partial \\theta}.\n",
    "$$\n",
    "\n",
    "The discretization introduces deviations from the continuous path due to the finite step size. This deviation can be interpreted by deriving a modified loss function $\\tilde{\\mathcal{L}}$ for the continuous case, such that it converges to the same point as the discrete updates on the original loss $\\mathcal{L}$. It turns out that this modified loss function is:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}_{\\text{GD}}(\\theta) = \\mathcal{L}(\\theta) + \\frac{\\alpha}{4} \\left\\| \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\right\\|^2.\n",
    "$$\n",
    "\n",
    "This additional term effectively penalizes the squared magnitude of the gradient, acting as an implicit regularizer.\n",
    "\n",
    "To illustrate this, imagine a loss function where there is a family of global minima along a horizontal line at $\\theta_1 = 0.61$. In the continuous gradient descent (with infinitesimal step size), starting from an initial point in the bottom-left, the path of $\\theta$ (shown as a dashed blue line) smoothly reaches this line of minima. However, in discrete gradient descent with a finite step size (e.g., $\\alpha = 0.1$), the trajectory (represented by arrows for the first few steps) deviates from the continuous path and converges to a different point. The finite step size causes the optimizer to settle at a location influenced by the implicit regularization term, which can be interpreted as the continuous gradient descent on the modified loss function $\\tilde{\\mathcal{L}}_{\\text{GD}}(\\theta)$. This demonstrates how discretization introduces an implicit bias toward certain solutions.\n",
    "\n",
    "### Implicit Regularization in Stochastic Gradient Descent\n",
    "\n",
    "A similar analysis applies to stochastic gradient descent (SGD). In SGD, we aim to find a modified loss function such that the continuous-time dynamics reach the same expected outcome as the average over possible stochastic updates. The modified loss function for SGD is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{\\mathcal{L}}_{\\text{SGD}}(\\theta) &= \\tilde{\\mathcal{L}}_{\\text{GD}}(\\theta) + \\frac{\\alpha}{4B} \\sum_{b=1}^{B} \\left\\| \\frac{\\partial \\mathcal{L}_b}{\\partial \\theta} - \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\right\\|^2 \\\\\n",
    "&= \\mathcal{L}(\\theta) + \\frac{\\alpha}{4} \\left\\| \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\right\\|^2 + \\frac{\\alpha}{4B} \\sum_{b=1}^{B} \\left\\| \\frac{\\partial \\mathcal{L}_b}{\\partial \\theta} - \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\right\\|^2,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $B$ is the batch size, and $\\mathcal{L}_b$ represents the loss for batch $b$. This expression reveals that SGD introduces an additional regularization term related to the variance of the gradients across different batches.\n",
    "\n",
    "Empirically, **SGD often generalizes better than standard gradient descent**, and smaller batch sizes tend to yield better performance than larger ones. One explanation is that the randomness inherent in SGD allows the algorithm to explore different parts of the loss surface, potentially escaping local minima. Alternatively, the implicit regularization term encourages solutions where the loss is consistently low across all data points, minimizing the gradient variance. Solutions with uniformly low loss are more likely to generalize well because they avoid overfitting to specific subsets of the data.\n",
    "\n",
    "For example, when training a neural network on a dataset, using a larger learning rate can lead to better performance compared to smaller learning rates. Although each iteration takes a bigger step, the total number of iterations can be adjusted so that the optimizer has the opportunity to traverse the parameter space adequately. Similarly, training with smaller batch sizes introduces more noise into the gradient estimation, which can act as a regularizer and improve generalization. However, there is a trade-off, as too much noise can hinder convergence.\n",
    "\n",
    "In practice, careful tuning of the learning rate and batch size is crucial. Larger learning rates can accelerate training and help the optimizer find flatter minima that generalize better, but if the learning rate is too high, the training may become unstable. Smaller batch sizes can improve generalization through implicit regularization, but very small batches may increase computational overhead and slow down training due to less efficient use of parallel hardware.\n",
    "\n",
    "Understanding implicit regularization provides valuable insights into why certain optimization choices, like learning rate and batch size, can significantly impact the performance of neural networks. By recognizing how these factors influence the implicit bias of the training process, we can make informed decisions to enhance model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics to Improve Performance\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Early stopping is a technique where the training process is halted before the model fully converges. By stopping training at an earlier point, we aim to prevent the model from overfitting to the training data. The idea is that the model has already learned the essential patterns in the data but hasn't yet started to memorize the noise.\n",
    "\n",
    "### Ensembling\n",
    "\n",
    "Ensembling involves training multiple models and combining their predictions to improve generalization. By averaging the outputs of several models, an ensemble reduces variance and often achieves better performance than any single model. This approach helps to close the generalization gap between training and test data.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a regularization method used during training, not during inference. At each iteration of stochastic gradient descent (SGD), a random subset of hidden units—typically 50%—is set to zero. This random \"dropping out\" of units prevents the network from becoming overly reliant on any single hidden unit and encourages the weights to distribute more evenly. As a result, the network becomes less sensitive to the omission of individual units, and the weights tend to have smaller magnitudes.\n",
    "\n",
    "### Applying Noise\n",
    "\n",
    "Building on the idea that dropout applies multiplicative Bernoulli noise to network activations, we can consider introducing noise to other parts of the network during training to enhance robustness.\n",
    "\n",
    "**Adding Noise to Input Data:** Introducing random noise to the input data at each training step can smooth out the learned function. In regression problems, this is equivalent to adding a regularization term that penalizes large derivatives of the network's output with respect to its input, encouraging the model to be less sensitive to small input changes.\n",
    "\n",
    "Consider what happens when we add noise with variance $\\sigma_x^2$ to the input data at each SGD iteration. As the level of noise increases, the fitted model becomes smoother because it learns to generalize over the noisy inputs. This smoothing effect helps prevent the model from overfitting to the training data's noise.\n",
    "\n",
    "**Adding Noise to the Weights:** Injecting noise directly into the weights during training encourages the network to make stable predictions even when the weights are slightly perturbed. This leads the training process to converge to local minima in wide, flat regions of the loss landscape, where small changes in weights have minimal impact on performance. Models that settle in these flat minima generally exhibit better generalization.\n",
    "\n",
    "**Adding Noise to Labels:** In classification tasks, the maximum likelihood criterion pushes the network to predict the correct class with absolute certainty, often resulting in extremely high activations for the correct class and very low activations for incorrect classes before the softmax layer. By adding noise to the labels, we prevent the network from becoming overly confident, encouraging it to produce more calibrated probabilities and improving generalization.\n",
    "\n",
    "### Augmentation\n",
    "\n",
    "Data augmentation involves transforming input data examples in ways that do not alter their corresponding labels, effectively increasing the size and diversity of the training set. For instance, in an image classification task aiming to detect birds, we can rotate, flip, blur, or adjust the color balance of the images. These transformations produce new training examples where the label \"bird\" remains valid. Augmentation helps the model become invariant to such transformations, enhancing its ability to generalize to new, unseen data.\n",
    "\n",
    "### Summary of Regularization Methods\n",
    "\n",
    "The regularization techniques discussed aim to improve generalization by employing one or more of the following mechanisms:\n",
    "\n",
    "- **Smoothing the Function:** Methods like adding noise to inputs or applying weight decay encourage the model to produce smoother output functions, reducing overfitting to the training data's noise.\n",
    "  \n",
    "- **Increasing Effective Data:** Data augmentation expands the training set with transformed versions of the data, providing more varied examples without the need to collect new data.\n",
    "\n",
    "- **Combining Multiple Models:** Ensembling combines predictions from multiple models to mitigate uncertainties and reduce variance, often yielding better performance than individual models.\n",
    "\n",
    "- **Finding Wide Minima:** Techniques such as adding noise to the weights or using dropout encourage the training process to converge to wider, flatter minima in the loss landscape. Models that settle in these regions are more robust, as small deviations in parameters have minimal effect on the loss.\n",
    "\n",
    "By understanding and applying these heuristics, we can enhance the performance of neural networks, leading to models that generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Recipe for Training Neural Networks\n",
    "\n",
    "*The following content is inspired by Andrej Karpathy's blog post \"[A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)\".*\n",
    "\n",
    "One of the significant challenges in training neural networks is preventing the inadvertent introduction of excessive complexity, which can lead to elusive bugs and misconfigurations that are hard to detect and resolve.\n",
    "\n",
    "## Become Familiar with Your Data\n",
    "\n",
    "Before engaging with neural network architectures or code, it's essential to deeply understand your dataset. Invest substantial time—several hours—examining thousands of examples to comprehend their distribution and uncover patterns or anomalies. Human intuition is powerful for spotting issues like duplicate entries, corrupted data, imbalances, or biases. Reflect on how you process and classify the data yourself to gain insights into the features and architectures that might be effective.\n",
    "\n",
    "Consider the following aspects:\n",
    "\n",
    "- **Feature Importance**: Do local features suffice, or is global context necessary?\n",
    "- **Variation Analysis**: What types of variation exist? Can spurious variations be removed through preprocessing?\n",
    "- **Spatial Relevance**: Does the spatial arrangement matter, or can it be abstracted away (e.g., via pooling)?\n",
    "- **Detail Significance**: How much detail is crucial? To what extent can images be downsampled without losing important information?\n",
    "- **Label Quality**: Assess the noise level in the labels and consider its impact on training.\n",
    "\n",
    "Understanding your data thoroughly enables you to interpret the model's predictions and misclassifications more effectively. If the neural network produces unexpected results, a solid grasp of the data can help identify whether the issue lies in the data itself or the model.\n",
    "\n",
    "Additionally, write scripts to filter, sort, and visualize your data based on various criteria—such as label types, annotation sizes, or other relevant features. Examining distributions and identifying outliers can reveal hidden problems in data quality or preprocessing that need to be addressed before training.\n",
    "\n",
    "## Set Up the Training and Evaluation Framework with Simple Baselines\n",
    "\n",
    "Rather than jumping into training complex models, start by establishing a reliable training and evaluation pipeline using a simple, well-understood model. This foundational step helps ensure that your entire workflow is correct and that you can trust the results of your experiments.\n",
    "\n",
    "Follow these best practices:\n",
    "\n",
    "- **Fixed Random Seed**: Use a consistent random seed to make your experiments reproducible. This eliminates variability due to random initialization or processing order.\n",
    "  \n",
    "- **Simplify the Setup**: Disable any non-essential components like data augmentation or complex regularization techniques. Introduce these features later once the basic pipeline is verified.\n",
    "  \n",
    "- **Comprehensive Evaluation**: When tracking metrics like test loss or accuracy, evaluate them over the entire test set rather than on individual batches. This provides a more reliable assessment of performance.\n",
    "  \n",
    "- **Verify Initial Loss**: Ensure that the initial loss value matches expected theoretical values. For example, with a properly initialized softmax classifier, the initial loss should be approximately \\(-\\log(1 / \\text{number of classes})\\).\n",
    "  \n",
    "- **Proper Weight Initialization**: Initialize model weights thoughtfully. If you're working with regression targets that have a mean of 50, set the final layer's bias to 50. For imbalanced classification tasks, adjust the initial biases to reflect the class distribution.\n",
    "  \n",
    "- **Establish Human-Level Baselines**: If possible, determine human performance on your task as a benchmark. Alternatively, use multiple annotations per example to assess the variability and establish an upper bound on performance.\n",
    "  \n",
    "- **Input-Independent Baseline**: Train a model that makes predictions without considering the input data (e.g., by using constant inputs). This helps confirm that your model is learning meaningful patterns from the actual data.\n",
    "  \n",
    "- **Overfit a Small Batch**: Attempt to overfit your model on a small batch of data (even just a few examples). Increase the model capacity if necessary and ensure it can achieve near-zero loss on this batch. Visualize the predictions to confirm they match the targets exactly.\n",
    "  \n",
    "- **Check Training Loss Decrease**: Starting with a simple model, verify that increasing the model's capacity leads to a decrease in training loss, as expected.\n",
    "  \n",
    "- **Visualize Model Inputs**: Inspect the processed inputs fed into the model to ensure they are correctly preprocessed and augmented. Visualizing these inputs can help catch data handling errors early.\n",
    "  \n",
    "- **Monitor Prediction Evolution**: Visualize model predictions on a fixed set of examples throughout training. Observing how predictions change over time can provide insights into the learning dynamics and help detect issues like instability or inappropriate learning rates.\n",
    "  \n",
    "- **Trace Dependencies with Gradients**: Utilize backpropagation to confirm that gradients flow correctly through the network. For example, verify that the gradient of a loss associated with a specific input only affects that input.\n",
    "  \n",
    "- **Implement Specific Cases Before Generalizing**: Start by coding specific functionalities tailored to your immediate needs. Once these are verified to work correctly, generalize the code as needed. This approach minimizes the risk of introducing bugs.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "With a solid understanding of your data and a reliable training pipeline, the next step is to develop a model that can capture the underlying patterns in the training data.\n",
    "\n",
    "**Strategies:**\n",
    "\n",
    "- **Select an Appropriate Model**: Begin with proven architectures suitable for your task. For image classification, this might be a standard convolutional network like ResNet-50. Avoid experimenting with novel architectures until you have established a strong baseline.\n",
    "  \n",
    "- **Use a Robust Optimizer**: Start with the Adam optimizer using a learning rate of \\(3 \\times 10^{-4}\\). Adam is generally more forgiving with hyperparameters and can simplify the initial training phase. For certain models like convolutional networks, finely tuned stochastic gradient descent (SGD) may eventually offer better performance but requires careful tuning.\n",
    "  \n",
    "- **Incrementally Add Complexity**: If your model accepts multiple types of inputs or signals, incorporate them one at a time. Ensure that each addition leads to the expected improvement before adding more complexity.\n",
    "  \n",
    "- **Carefully Manage Learning Rate Schedules**: Be cautious with default learning rate decay schedules, especially if you're adapting code from other projects. Schedules based on epoch counts may not translate well to your dataset's size or complexity. Consider starting with a constant learning rate and tuning the schedule later.\n",
    "  \n",
    "By initially focusing on achieving low training loss, you can confirm that your model has the capacity to learn the data. If it cannot overfit the training data, there might be fundamental issues with the model architecture, data processing, or implementation that need to be resolved.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Once you have a model that fits the training data well, the goal is to improve its generalization performance on unseen data, even if it means sacrificing some training accuracy.\n",
    "\n",
    "**Regularization Techniques:**\n",
    "\n",
    "- **Increase Training Data**: The most effective way to enhance generalization is to gather more real-world training data. This method provides new information for the model to learn and reduces overfitting.\n",
    "  \n",
    "- **Data Augmentation**: Apply transformations to your existing data (e.g., rotations, flips, color jittering) to create new training examples. This helps the model become invariant to certain changes and improves robustness.\n",
    "  \n",
    "- **Creative Augmentation Strategies**: Employ advanced techniques like synthetic data generation, domain randomization, or using simulations to expand your dataset in meaningful ways.\n",
    "  \n",
    "- **Leverage Pretrained Models**: Utilize models pretrained on large datasets. Fine-tuning pretrained networks can lead to better performance, especially when training data is limited.\n",
    "  \n",
    "- **Reduce Input Complexity**: Simplify the inputs by removing features that may contribute noise or be irrelevant. This reduction can help the model focus on the most informative aspects of the data.\n",
    "  \n",
    "- **Decrease Model Complexity**: Simplify the model by reducing its size or depth. Smaller models have less capacity to overfit and may generalize better on limited data.\n",
    "  \n",
    "- **Adjust Batch Size**: Smaller batch sizes can introduce noise into the training process, which can act as a form of regularization. However, balance this with computational efficiency considerations.\n",
    "  \n",
    "- **Apply Dropout Sparingly**: Introduce dropout layers to prevent co-adaptation of neurons. For convolutional networks, spatial dropout (`Dropout2D`) can be used. Be cautious, as dropout can interact with batch normalization in complex ways.\n",
    "  \n",
    "- **Increase Weight Decay**: Strengthen L2 regularization by increasing the weight decay hyperparameter, which penalizes large weights and encourages simpler models.\n",
    "  \n",
    "- **Implement Early Stopping**: Monitor validation performance and halt training when the model starts to overfit. This prevents the model from learning noise in the training data.\n",
    "  \n",
    "- **Consider Larger Models with Early Stopping**: Sometimes, larger models trained with early stopping can outperform smaller ones, as they may capture more complex patterns before overfitting.\n",
    "\n",
    "Additionally, inspect the learned features and internal representations:\n",
    "\n",
    "- **Visualize First-Layer Weights**: In image models, the first-layer filters should display coherent patterns like edge detectors or color blobs. Random or noisy patterns might indicate problems.\n",
    "  \n",
    "- **Examine Activations**: Check activations throughout the network to identify any unusual behavior that could signal issues with training or model architecture.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Fine-tuning hyperparameters is critical for optimizing model performance.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "- **Random Search Over Grid Search**: When tuning multiple hyperparameters, random search is generally more effective than grid search. It allows for a broader exploration of the hyperparameter space and can more efficiently identify optimal configurations.\n",
    "  \n",
    "- **Automated Hyperparameter Optimization**: Consider using Bayesian optimization tools or hyperparameter tuning libraries to systematically explore the hyperparameter space.\n",
    "\n",
    "## Maximizing Performance\n",
    "\n",
    "To achieve the best possible performance from your neural network:\n",
    "\n",
    "- **Ensemble Models**: Combining predictions from multiple models can lead to significant performance improvements. If computational constraints prevent deploying ensembles, explore techniques like knowledge distillation to compress the ensemble into a single model.\n",
    "  \n",
    "- **Allow Extended Training**: Don't be too eager to stop training when progress seems to slow down. Neural networks can continue to improve with additional training time, sometimes leading to breakthroughs in performance.\n",
    "\n",
    "By methodically applying these strategies—deep data understanding, careful setup, disciplined training, regularization, hyperparameter tuning, and perseverance—you can develop effective neural network models that perform well on real-world tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facts about Deep Learning\n",
    "\n",
    "- **The Lottery Ticket Hypothesis**: In 2019, Frankle and Carbin introduced the concept of the lottery ticket hypothesis. They demonstrated that for overparameterized neural networks, such as VGG, it's possible to identify smaller subnetworks that can achieve the same or even better performance than the full network. The procedure involves:\n",
    "\n",
    "    1. **Training the full network** to completion.\n",
    "    2. **Pruning** the weights with the smallest magnitudes, effectively removing less important connections.\n",
    "    3. **Reinitializing** the remaining weights to their original values from before the initial training.\n",
    "    4. **Retraining** this smaller, pruned network from scratch.\n",
    "\n",
    "  Crucially, this approach only works when the pruned network is reinitialized to its original weights; reinitializing with new random weights does not yield the same results. The findings suggest that the original, larger network contains \"winning tickets\"—small, trainable subnetworks that are inherently capable of learning effectively. This implies that the sheer number of parameters isn't the sole factor in a network's success; instead, the presence of these effective subnetworks plays a key role. While it is suspected that the depth of the network might influence the number of such subnetworks for a given parameter count, a precise understanding of this relationship remains an open area of research.\n",
    "\n",
    "- **The Alignment Problem and Reward Hacking**: Machine learning models often make predictions used to inform decisions that aim to minimize expected loss. However, crafting a loss function that perfectly encapsulates all human preferences and constraints is inherently challenging. This difficulty can lead to **reward hacking**, where an AI system aggressively optimizes the provided reward function but in doing so exploits gaps or oversights, leading to unintended and undesired behaviors. Reward hacking is a manifestation of the broader **alignment problem**, which highlights the mismatch between the objectives we set for our algorithms and the outcomes we truly desire. This issue raises significant concerns in AI ethics and safety.\n",
    "\n",
    "  To address the alignment problem, Stuart Russell proposes avoiding the explicit specification of a reward function. Instead, he suggests that machines should infer the intended reward by observing human behavior—a process known as inverse reinforcement learning. Despite its potential, this approach has challenges. Relying solely on past human behavior may be undesirable, as it can perpetuate existing biases present in the training data.\n",
    "\n",
    "  The conventional view of AI envisions autonomous systems making independent decisions, steering toward the development of **Artificial General Intelligence (AGI)**. As an alternative, the concept of **Augmented Intelligence** emphasizes the creation of intelligent tools that assist humans while keeping them involved in the decision-making process. Examples include adaptive cruise control in vehicles or autocomplete features in search engines. From this perspective, AI and machine learning components become integral parts of complex, semi-autonomous systems—much like autopilot in airplanes, online trading platforms, or advanced medical diagnostic equipment—where human oversight remains essential."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
