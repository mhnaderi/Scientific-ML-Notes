{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "\n",
    "# Artificial Neural Networks - Part 2\n",
    "\n",
    "### Part of Scientific-ML-Notes \n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repository-black?logo=github&scale=5)](https://github.com/mhnaderi/Scientific-ML-Notes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Always run this cell first to import all necessary libraries and to set up the notebook environment\n",
    "\n",
    "! pip install -q flax\n",
    "! pip install -q optax\n",
    "! pip install -q equinox\n",
    "! pip install -q jaxtyping\n",
    "! pip install -q torch torchvision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from flax import linen as nn\n",
    "from jaxtyping import Array, Float, Int, PyTree \n",
    "\n",
    "import optax \n",
    "import equinox as eqx\n",
    "import torch, torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.rcParams.update({\"font.family\": \"serif\"})\n",
    "plt.rcParams.update({\"figure.facecolor\": 'white', \"axes.facecolor\": 'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are specialized deep learning models designed to process data with a grid-like topology, such as images. They have become the cornerstone of computer vision tasks due to their ability to capture spatial hierarchies and patterns in data. This comprehensive explanation delves into the motivations behind CNNs, their fundamental operations, and the architectural components that make them effective.\n",
    "\n",
    "## Motivations for CNNs in Image Processing\n",
    "\n",
    "Images possess unique characteristics that necessitate specialized architectures beyond traditional fully connected neural networks:\n",
    "\n",
    "1. **High Dimensionality**: Images are high-dimensional data. For example, a standard RGB image of size $224 \\times 224$ pixels contains $224 \\times 224 \\times 3 = 150,528$ input values. Fully connected networks connecting each input pixel to neurons in the next layer would require an impractical number of parameters (weights). For even a shallow network, this could amount to over 22 billion parameters ($150,528^2$), leading to excessive computational demands and the risk of overfitting due to insufficient training data.\n",
    "\n",
    "2. **Local Correlations**: Nearby pixels in an image are often statistically related and form meaningful patterns (e.g., edges, textures). Fully connected networks do not inherently consider spatial relationships and treat each pixel independently. This lack of spatial awareness means they cannot efficiently capture local patterns crucial for image understanding.\n",
    "\n",
    "3. **Spatial Invariance**: The meaning of an image is generally stable under certain geometric transformations like translation, rotation, or scaling. For instance, shifting an image of a cat slightly to the left does not change its identity as a cat. Fully connected networks cannot inherently handle such invariance, as any shift alters all input values, requiring the network to relearn features for every possible position—a highly inefficient process.\n",
    "\n",
    "## Convolutional Layers: Addressing Image Properties\n",
    "\n",
    "Convolutional layers are the fundamental building blocks of CNNs that effectively address the aforementioned challenges:\n",
    "\n",
    "- **Local Connectivity**: Each neuron in a convolutional layer is connected to a small, localized region of the input (receptive field). This allows the network to capture local features and patterns while significantly reducing the number of parameters compared to full connections.\n",
    "\n",
    "- **Weight Sharing**: The same set of weights (kernel or filter) is applied across all spatial locations in the input. This property reduces the number of unique parameters and ensures that learned features are translationally invariant—enabling the network to recognize patterns regardless of their position in the input.\n",
    "\n",
    "By leveraging local connectivity and weight sharing, convolutional layers exploit the spatial hierarchies in images, capturing low-level features in early layers and more abstract, high-level features in deeper layers.\n",
    "\n",
    "## Invariance and Equivariance in CNNs\n",
    "\n",
    "Understanding how CNNs handle transformations involves two key concepts:\n",
    "\n",
    "- **Invariance**: A function $ f(\\mathbf{x}) $ is invariant to a transformation $ t(\\mathbf{x}) $ if the output remains unchanged under the transformation: $ f(t(\\mathbf{x})) = f(\\mathbf{x}) $. In image classification, we desire the network's output (class label) to be invariant to transformations like translation or rotation.\n",
    "\n",
    "- **Equivariance (Covariance)**: A function $ f(\\mathbf{x}) $ is equivariant to a transformation $ t(\\mathbf{x}) $ if applying the transformation to the input results in a corresponding transformation of the output: $ f(t(\\mathbf{x})) = t(f(\\mathbf{x})) $. For tasks like image segmentation, we want the output segmentation map to shift correspondingly if the input image is shifted.\n",
    "\n",
    "Convolutional layers are inherently equivariant to translation, meaning they preserve the spatial arrangement of features relative to input transformations.\n",
    "\n",
    "## The Convolution Operation\n",
    "\n",
    "Convolution is the process of applying a filter (kernel) to an input to produce an output (feature map). In one dimension, for input $ \\mathbf{x} $ and kernel $ \\boldsymbol{\\omega} $ of size $ K $, the convolution operation at position $ i $ is:\n",
    "\n",
    "$$\n",
    "z_i = \\sum_{k=1}^{K} \\omega_k x_{i + k - \\lfloor K/2 \\rfloor}\n",
    "$$\n",
    "\n",
    "- **Kernel Size**: The size $ K $ of the kernel determines the width of the receptive field.\n",
    "\n",
    "- **Stride**: The number of positions the kernel moves after each operation. A stride of 1 means the kernel moves one position at a time; a stride of 2 skips every other position, effectively downsampling the output.\n",
    "\n",
    "- **Padding**: Adding zeros (zero-padding) to the input boundaries allows the kernel to be applied to edge elements, preserving the input size in the output.\n",
    "\n",
    "- **Dilation**: Spacing out the elements in the kernel to increase the receptive field without increasing the kernel size. This allows the network to capture broader contextual information.\n",
    "\n",
    "In two dimensions (images), the convolution operation involves sliding the kernel over the height and width dimensions, computing the dot product at each spatial location.\n",
    "\n",
    "## Comparison with Fully Connected Layers\n",
    "\n",
    "In fully connected (dense) layers:\n",
    "\n",
    "- **Connectivity**: Each neuron is connected to every neuron in the previous layer.\n",
    "\n",
    "- **Parameters**: The number of weights is $ n_{\\text{inputs}} \\times n_{\\text{outputs}} $, which can be massive for high-dimensional inputs like images.\n",
    "\n",
    "- **Spatial Awareness**: There is no inherent consideration of spatial structure or local correlations.\n",
    "\n",
    "In convolutional layers:\n",
    "\n",
    "- **Connectivity**: Neurons are connected only to a local region of the input.\n",
    "\n",
    "- **Parameters**: The number of unique weights depends on the kernel size and the number of filters, not on the input size.\n",
    "\n",
    "- **Spatial Awareness**: Local connectivity and weight sharing capture spatial hierarchies and local patterns.\n",
    "\n",
    "The sparsity and parameter sharing in convolutional layers make them computationally efficient and better suited for processing images.\n",
    "\n",
    "## Channels and Feature Maps\n",
    "\n",
    "Images and feature maps in CNNs have multiple channels:\n",
    "\n",
    "- **Input Channels**: For color images, there are typically three channels corresponding to red, green, and blue (RGB).\n",
    "\n",
    "- **Output Channels (Feature Maps)**: Each convolutional layer applies multiple kernels (filters), producing a set of feature maps. Each feature map corresponds to a specific learned feature across the spatial dimensions.\n",
    "\n",
    "- **Convolution Across Channels**: The convolution operation sums over both spatial dimensions and input channels. For a kernel $ \\boldsymbol{\\omega} $ and input with $ C_{\\text{in}} $ channels, the output at position $ (i, j) $ is:\n",
    "\n",
    "$$\n",
    "z_{i,j} = \\sum_{c=1}^{C_{\\text{in}}} \\sum_{k=1}^{K} \\sum_{l=1}^{K} \\omega_{k,l,c} \\cdot x_{i+k-1,j+l-1,c}\n",
    "$$\n",
    "\n",
    "Each kernel is a 3D tensor (height, width, input channels), and multiple such kernels produce multiple output channels.\n",
    "\n",
    "## Receptive Fields\n",
    "\n",
    "The receptive field of a neuron refers to the specific region of the input that influences its activation:\n",
    "\n",
    "- **Local Receptive Fields**: In early layers, neurons have small receptive fields, focusing on local patterns like edges and textures.\n",
    "\n",
    "- **Increasing Receptive Fields**: As we stack more layers, the receptive fields of neurons in deeper layers cover larger portions of the input, allowing the network to learn more global and abstract features.\n",
    "\n",
    "- **Controlling Receptive Fields**: Adjusting kernel sizes, strides, and the number of layers affects the receptive field size. Larger kernels or more layers increase the receptive field.\n",
    "\n",
    "Understanding and controlling receptive fields is crucial for designing networks that capture the necessary context for a given task.\n",
    "\n",
    "## Example: CNN for MNIST-1D Classification\n",
    "\n",
    "Consider a simplified CNN applied to a one-dimensional version of the MNIST dataset:\n",
    "\n",
    "- **Input Dimension**: $ D_i = 40 $\n",
    "\n",
    "- **First Convolutional Layer**:\n",
    "  - **Number of Filters**: 15\n",
    "  - **Kernel Size**: 3\n",
    "  - **Stride**: 2\n",
    "  - **Padding**: None (valid convolution)\n",
    "  - **Output Size**: The output has $ \\left\\lfloor \\frac{D_i - K}{\\text{stride}} + 1 \\right\\rfloor = 19 $ positions and 15 channels.\n",
    "\n",
    "- **Subsequent Convolutional Layers**:\n",
    "  - Similar configurations with adjustments to reduce the spatial dimension and increase the depth.\n",
    "\n",
    "- **Fully Connected Layer**:\n",
    "  - Takes the flattened output from the last convolutional layer.\n",
    "  - Outputs ten activations corresponding to the ten digit classes.\n",
    "  - Applies a softmax function to produce class probabilities.\n",
    "\n",
    "**Results**:\n",
    "\n",
    "- The CNN effectively learns to classify the digits with better generalization compared to a fully connected network with the same depth and number of neurons.\n",
    "\n",
    "- The fully connected network overfits the training data but fails to generalize due to the lack of inductive bias and inability to capture spatial hierarchies.\n",
    "\n",
    "## Inductive Bias and Generalization\n",
    "\n",
    "- **Inductive Bias**: CNNs incorporate the prior knowledge that local patterns and spatial hierarchies are important in images. This bias guides the learning process toward more plausible solutions.\n",
    "\n",
    "- **Parameter Sharing and Locality**: By sharing parameters across spatial locations and focusing on local regions, CNNs require fewer parameters and can generalize better from limited data.\n",
    "\n",
    "- **Overparameterization**: While overparameterization can sometimes aid learning, in the case of fully connected networks on image data, it leads to memorization without understanding spatial structures, resulting in poor generalization.\n",
    "\n",
    "## Downsampling Techniques\n",
    "\n",
    "Reducing the spatial dimensions of feature maps is common in CNNs to:\n",
    "\n",
    "- **Decrease Computational Load**: Smaller feature maps require fewer computations in subsequent layers.\n",
    "\n",
    "- **Increase Receptive Field**: Downsampling effectively increases the receptive field of neurons in deeper layers.\n",
    "\n",
    "Common downsampling methods:\n",
    "\n",
    "1. **Strided Convolution**:\n",
    "   - Using a stride greater than 1 in convolutional layers skips positions, reducing output size.\n",
    "   - Example: A stride of 2 halves the spatial dimensions.\n",
    "\n",
    "2. **Pooling Layers**:\n",
    "   - **Max Pooling**: Divides the input into non-overlapping regions and outputs the maximum value from each region.\n",
    "   - **Average Pooling**: Outputs the average value from each region.\n",
    "   - Pooling introduces a form of translational invariance by summarizing features within regions.\n",
    "\n",
    "3. **Sub-sampling**:\n",
    "   - Retains every $ n $-th element along spatial dimensions.\n",
    "   - Simple but may discard potentially important information.\n",
    "\n",
    "## Upsampling Techniques\n",
    "\n",
    "For tasks requiring output at the same spatial resolution as the input (e.g., image segmentation), upsampling methods are used to increase the spatial dimensions:\n",
    "\n",
    "1. **Nearest Neighbor Upsampling**:\n",
    "   - Duplicates each element along spatial dimensions.\n",
    "   - Simple but may produce blocky outputs.\n",
    "\n",
    "2. **Bilinear or Bicubic Interpolation**:\n",
    "   - Estimates new pixel values by interpolating between neighboring pixels.\n",
    "   - Produces smoother outputs but does not involve learnable parameters.\n",
    "\n",
    "3. **Max Unpooling**:\n",
    "   - Used in conjunction with max pooling.\n",
    "   - During pooling, the indices of the maximum values are recorded.\n",
    "   - During unpooling, the pooled values are placed back into their original positions, and zeros fill the rest.\n",
    "\n",
    "4. **Transposed Convolution (Fractionally Strided Convolution)**:\n",
    "   - Also known as deconvolution (though it does not perform a true mathematical deconvolution).\n",
    "   - Applies learnable kernels to upsample the input.\n",
    "   - The operation effectively reverses the dimensions of convolution, using the transpose of the convolutional weight matrix.\n",
    "   - Can learn how to combine features to produce high-resolution outputs.\n",
    "\n",
    "## Transposed Convolution in Detail\n",
    "\n",
    "Transposed convolution increases the spatial dimensions while applying convolution-like operations:\n",
    "\n",
    "- **Operation**:\n",
    "  - For a 1D input, each element contributes to multiple output positions based on the kernel size.\n",
    "  - The process involves interleaving zeros between input elements (if necessary) and performing a standard convolution.\n",
    "\n",
    "- **Weight Matrix**:\n",
    "  - The weight matrix used in transposed convolution is the transpose of the weight matrix used in the corresponding convolution operation.\n",
    "\n",
    "- **Learnable Parameters**:\n",
    "  - The kernels are learned during training, enabling the network to adaptively upsample the feature maps.\n",
    "\n",
    "- **Applications**:\n",
    "  - Widely used in generative models and image segmentation tasks where reconstructing high-resolution outputs is required.\n",
    "\n",
    "## Architectural Components of CNNs\n",
    "\n",
    "A typical CNN architecture comprises several key components:\n",
    "\n",
    "1. **Convolutional Layers**:\n",
    "   - Extract features through learned kernels.\n",
    "   - Capture spatial hierarchies and patterns.\n",
    "   - Often followed by activation functions like ReLU (Rectified Linear Unit) to introduce non-linearity.\n",
    "\n",
    "2. **Pooling Layers**:\n",
    "   - Reduce spatial dimensions.\n",
    "   - Provide translation invariance.\n",
    "   - Help control overfitting by reducing the number of parameters.\n",
    "\n",
    "3. **Fully Connected Layers**:\n",
    "   - Interpret the features extracted by convolutional layers.\n",
    "   - Act as a classifier or regressor depending on the task.\n",
    "   - In some modern architectures, fully connected layers are replaced or minimized in favor of global pooling to reduce parameters.\n",
    "\n",
    "4. **Normalization Layers**:\n",
    "   - Batch Normalization stabilizes and accelerates training by normalizing inputs to layers.\n",
    "\n",
    "5. **Regularization Techniques**:\n",
    "   - **Dropout**: Randomly sets a fraction of inputs to zero during training to prevent overfitting.\n",
    "   - **Weight Decay**: Adds a penalty term to the loss function to discourage large weights.\n",
    "\n",
    "6. **Output Layer**:\n",
    "   - For classification tasks, a softmax activation provides class probabilities.\n",
    "   - For regression tasks, a linear activation outputs continuous values.\n",
    "\n",
    "7. **Loss Function**:\n",
    "   - Guides the optimization by quantifying the difference between predictions and ground truth.\n",
    "   - Common losses include cross-entropy for classification and mean squared error for regression.\n",
    "\n",
    "## Advanced CNN Architectures\n",
    "\n",
    "Modern CNNs incorporate various architectural innovations to enhance performance:\n",
    "\n",
    "- **Deep Networks**: Stacking more layers allows for learning more complex representations. Techniques like residual connections (ResNets) help mitigate issues like vanishing gradients.\n",
    "\n",
    "- **Inception Modules**: Utilize multiple convolutional kernels of different sizes in parallel to capture features at various scales.\n",
    "\n",
    "- **Dilated Convolutions**: Increase the receptive field without increasing the number of parameters by introducing gaps within the kernels.\n",
    "\n",
    "- **Attention Mechanisms**: Allow the network to focus on important regions of the input.\n",
    "\n",
    "- **Encoder-Decoder Architectures**: Used in tasks like image segmentation, where the encoder captures context, and the decoder reconstructs the spatial details.\n",
    "\n",
    "## Advantages of CNNs\n",
    "\n",
    "- **Parameter Efficiency**: Fewer parameters than fully connected networks for high-dimensional inputs.\n",
    "\n",
    "- **Spatial Hierarchies**: Ability to capture local to global patterns through layered convolutions.\n",
    "\n",
    "- **Translation Equivariance and Invariance**: Convolutional operations are naturally equivariant to translation, and pooling introduces invariance.\n",
    "\n",
    "- **Effective Inductive Bias**: The architectural design incorporates assumptions about the spatial structure of images, aiding generalization.\n",
    "\n",
    "- **Scalability**: Can be scaled to very deep networks, leveraging modern computational resources and optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:08<00:00, 1171416.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 1458324.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1111153.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 907859.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "step=0, train_loss=2.302813768386841, test_loss=2.3030264377593994, test_accuracy=0.16291798651218414\n",
      "step=30, train_loss=2.018705368041992, test_loss=2.0125811100006104, test_accuracy=0.5482683181762695\n",
      "step=60, train_loss=1.7050797939300537, test_loss=1.6047518253326416, test_accuracy=0.6208200454711914\n",
      "step=90, train_loss=1.1989474296569824, test_loss=1.201015830039978, test_accuracy=0.7296974658966064\n",
      "step=120, train_loss=1.067009449005127, test_loss=0.8721583485603333, test_accuracy=0.8121019005775452\n",
      "step=150, train_loss=0.6692885160446167, test_loss=0.665345311164856, test_accuracy=0.8558917045593262\n",
      "step=180, train_loss=0.6297428011894226, test_loss=0.5339710116386414, test_accuracy=0.8737062215805054\n",
      "step=210, train_loss=0.4877898395061493, test_loss=0.4716041088104248, test_accuracy=0.8772889971733093\n",
      "step=240, train_loss=0.5786195397377014, test_loss=0.40109315514564514, test_accuracy=0.8961982727050781\n",
      "step=270, train_loss=0.49600231647491455, test_loss=0.35962775349617004, test_accuracy=0.9079418778419495\n",
      "step=299, train_loss=0.49557751417160034, test_loss=0.3242952227592468, test_accuracy=0.9128184914588928\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"567.95679pt\" height=\"290.225455pt\" viewBox=\"0 0 567.95679 290.225455\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-15T23:21:43.169420</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 290.225455 \n",
       "L 567.95679 290.225455 \n",
       "L 567.95679 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 252.669205 \n",
       "L 282.18125 252.669205 \n",
       "L 282.18125 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mffdab4e761\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mffdab4e761\" x=\"60.690909\" y=\"252.669205\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(57.509659 267.267642) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-30\" d=\"M 2034 219 \n",
       "Q 2513 219 2750 744 \n",
       "Q 2988 1269 2988 2328 \n",
       "Q 2988 3391 2750 3916 \n",
       "Q 2513 4441 2034 4441 \n",
       "Q 1556 4441 1318 3916 \n",
       "Q 1081 3391 1081 2328 \n",
       "Q 1081 1269 1318 744 \n",
       "Q 1556 219 2034 219 \n",
       "z\n",
       "M 2034 -91 \n",
       "Q 1275 -91 848 546 \n",
       "Q 422 1184 422 2328 \n",
       "Q 422 3475 848 4112 \n",
       "Q 1275 4750 2034 4750 \n",
       "Q 2797 4750 3222 4112 \n",
       "Q 3647 3475 3647 2328 \n",
       "Q 3647 1184 3222 546 \n",
       "Q 2797 -91 2034 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mffdab4e761\" x=\"95.965689\" y=\"252.669205\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(89.603189 267.267642) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-35\" d=\"M 3219 4666 \n",
       "L 3219 4153 \n",
       "L 1081 4153 \n",
       "L 1081 2816 \n",
       "Q 1244 2928 1461 2984 \n",
       "Q 1678 3041 1947 3041 \n",
       "Q 2703 3041 3140 2622 \n",
       "Q 3578 2203 3578 1478 \n",
       "Q 3578 738 3136 323 \n",
       "Q 2694 -91 1894 -91 \n",
       "Q 1572 -91 1234 -12 \n",
       "Q 897 66 544 225 \n",
       "L 544 1131 \n",
       "L 897 1131 \n",
       "Q 925 688 1179 453 \n",
       "Q 1434 219 1894 219 \n",
       "Q 2388 219 2653 544 \n",
       "Q 2919 869 2919 1478 \n",
       "Q 2919 2084 2655 2407 \n",
       "Q 2391 2731 1894 2731 \n",
       "Q 1613 2731 1398 2631 \n",
       "Q 1184 2531 1019 2322 \n",
       "L 750 2322 \n",
       "L 750 4666 \n",
       "L 3219 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mffdab4e761\" x=\"131.240468\" y=\"252.669205\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(121.696718 267.267642) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-31\" d=\"M 909 0 \n",
       "L 909 331 \n",
       "L 1722 331 \n",
       "L 1722 4213 \n",
       "L 781 3603 \n",
       "L 781 4013 \n",
       "L 1919 4750 \n",
       "L 2350 4750 \n",
       "L 2350 331 \n",
       "L 3163 331 \n",
       "L 3163 0 \n",
       "L 909 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mffdab4e761\" x=\"166.515248\" y=\"252.669205\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(156.971498 267.267642) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mffdab4e761\" x=\"201.790027\" y=\"252.669205\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(192.246277 267.267642) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-32\" d=\"M 819 3553 \n",
       "L 469 3553 \n",
       "L 469 4384 \n",
       "Q 803 4563 1142 4656 \n",
       "Q 1481 4750 1806 4750 \n",
       "Q 2534 4750 2956 4397 \n",
       "Q 3378 4044 3378 3438 \n",
       "Q 3378 2753 2422 1800 \n",
       "Q 2347 1728 2309 1691 \n",
       "L 1131 513 \n",
       "L 3078 513 \n",
       "L 3078 1088 \n",
       "L 3444 1088 \n",
       "L 3444 0 \n",
       "L 434 0 \n",
       "L 434 341 \n",
       "L 1850 1753 \n",
       "Q 2319 2222 2519 2614 \n",
       "Q 2719 3006 2719 3438 \n",
       "Q 2719 3909 2473 4175 \n",
       "Q 2228 4441 1797 4441 \n",
       "Q 1350 4441 1106 4219 \n",
       "Q 863 3997 819 3553 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mffdab4e761\" x=\"237.064807\" y=\"252.669205\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(227.521057 267.267642) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mffdab4e761\" x=\"272.339587\" y=\"252.669205\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(262.795837 267.267642) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-33\" d=\"M 622 4469 \n",
       "Q 988 4606 1323 4678 \n",
       "Q 1659 4750 1953 4750 \n",
       "Q 2638 4750 3022 4454 \n",
       "Q 3406 4159 3406 3634 \n",
       "Q 3406 3213 3140 2930 \n",
       "Q 2875 2647 2388 2547 \n",
       "Q 2963 2466 3280 2130 \n",
       "Q 3597 1794 3597 1259 \n",
       "Q 3597 606 3158 257 \n",
       "Q 2719 -91 1894 -91 \n",
       "Q 1528 -91 1179 -12 \n",
       "Q 831 66 488 225 \n",
       "L 488 1131 \n",
       "L 838 1131 \n",
       "Q 869 681 1141 450 \n",
       "Q 1413 219 1906 219 \n",
       "Q 2384 219 2661 495 \n",
       "Q 2938 772 2938 1253 \n",
       "Q 2938 1803 2653 2086 \n",
       "Q 2369 2369 1819 2369 \n",
       "L 1522 2369 \n",
       "L 1522 2688 \n",
       "L 1678 2688 \n",
       "Q 2225 2688 2498 2914 \n",
       "Q 2772 3141 2772 3597 \n",
       "Q 2772 4006 2547 4223 \n",
       "Q 2322 4441 1900 4441 \n",
       "Q 1478 4441 1245 4241 \n",
       "Q 1013 4041 972 3647 \n",
       "L 622 3647 \n",
       "L 622 4469 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- Step -->\n",
       "     <g transform=\"translate(154.567969 280.945767) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-53\" d=\"M 594 225 \n",
       "L 594 1288 \n",
       "L 953 1284 \n",
       "Q 969 753 1261 498 \n",
       "Q 1553 244 2150 244 \n",
       "Q 2706 244 2998 464 \n",
       "Q 3291 684 3291 1106 \n",
       "Q 3291 1444 3114 1625 \n",
       "Q 2938 1806 2369 1978 \n",
       "L 1753 2163 \n",
       "Q 1084 2366 811 2669 \n",
       "Q 538 2972 538 3500 \n",
       "Q 538 4094 959 4422 \n",
       "Q 1381 4750 2144 4750 \n",
       "Q 2469 4750 2856 4679 \n",
       "Q 3244 4609 3681 4475 \n",
       "L 3681 3481 \n",
       "L 3328 3481 \n",
       "Q 3275 3975 2998 4195 \n",
       "Q 2722 4416 2156 4416 \n",
       "Q 1663 4416 1405 4214 \n",
       "Q 1147 4013 1147 3628 \n",
       "Q 1147 3294 1340 3103 \n",
       "Q 1534 2913 2163 2725 \n",
       "L 2741 2553 \n",
       "Q 3375 2363 3645 2067 \n",
       "Q 3916 1772 3916 1275 \n",
       "Q 3916 597 3481 253 \n",
       "Q 3047 -91 2188 -91 \n",
       "Q 1803 -91 1404 -12 \n",
       "Q 1006 66 594 225 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-74\" d=\"M 691 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 691 3322 \n",
       "L 691 4353 \n",
       "L 1269 4353 \n",
       "L 1269 3322 \n",
       "L 2350 3322 \n",
       "L 2350 2988 \n",
       "L 1269 2988 \n",
       "L 1269 878 \n",
       "Q 1269 456 1350 337 \n",
       "Q 1431 219 1650 219 \n",
       "Q 1875 219 1978 351 \n",
       "Q 2081 484 2088 781 \n",
       "L 2522 781 \n",
       "Q 2497 328 2275 118 \n",
       "Q 2053 -91 1600 -91 \n",
       "Q 1103 -91 897 129 \n",
       "Q 691 350 691 878 \n",
       "L 691 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-65\" d=\"M 3469 1600 \n",
       "L 991 1600 \n",
       "L 991 1575 \n",
       "Q 991 903 1244 561 \n",
       "Q 1497 219 1991 219 \n",
       "Q 2369 219 2611 417 \n",
       "Q 2853 616 2950 1006 \n",
       "L 3413 1006 \n",
       "Q 3275 459 2904 184 \n",
       "Q 2534 -91 1931 -91 \n",
       "Q 1203 -91 761 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2450 753 2931 \n",
       "Q 1188 3413 1894 3413 \n",
       "Q 2647 3413 3050 2948 \n",
       "Q 3453 2484 3469 1600 \n",
       "z\n",
       "M 2791 1931 \n",
       "Q 2772 2513 2545 2808 \n",
       "Q 2319 3103 1894 3103 \n",
       "Q 1497 3103 1269 2806 \n",
       "Q 1041 2509 991 1931 \n",
       "L 2791 1931 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-70\" d=\"M 1313 1825 \n",
       "L 1313 1497 \n",
       "Q 1313 897 1542 583 \n",
       "Q 1772 269 2209 269 \n",
       "Q 2650 269 2876 622 \n",
       "Q 3103 975 3103 1663 \n",
       "Q 3103 2353 2876 2703 \n",
       "Q 2650 3053 2209 3053 \n",
       "Q 1772 3053 1542 2737 \n",
       "Q 1313 2422 1313 1825 \n",
       "z\n",
       "M 738 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 1313 3322 \n",
       "L 1313 2803 \n",
       "Q 1481 3116 1742 3264 \n",
       "Q 2003 3413 2388 3413 \n",
       "Q 3000 3413 3387 2928 \n",
       "Q 3775 2444 3775 1663 \n",
       "Q 3775 881 3387 395 \n",
       "Q 3000 -91 2388 -91 \n",
       "Q 2003 -91 1742 57 \n",
       "Q 1481 206 1313 519 \n",
       "L 1313 -997 \n",
       "L 1856 -997 \n",
       "L 1856 -1331 \n",
       "L 184 -1331 \n",
       "L 184 -997 \n",
       "L 738 -997 \n",
       "L 738 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-53\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"68.505859\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"108.691406\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-70\" x=\"167.871094\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path id=\"m24c9ebcaa8\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"250.0614\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(20.878125 253.860619) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-2e\" d=\"M 603 325 \n",
       "Q 603 500 722 622 \n",
       "Q 841 744 1019 744 \n",
       "Q 1191 744 1312 622 \n",
       "Q 1434 500 1434 325 \n",
       "Q 1434 153 1312 31 \n",
       "Q 1191 -91 1019 -91 \n",
       "Q 841 -91 722 29 \n",
       "Q 603 150 603 325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"223.60378\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(20.878125 227.402999) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"197.14616\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.75 -->\n",
       "      <g transform=\"translate(20.878125 200.945379) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-37\" d=\"M 3609 4347 \n",
       "L 1784 0 \n",
       "L 1319 0 \n",
       "L 3059 4153 \n",
       "L 903 4153 \n",
       "L 903 3578 \n",
       "L 538 3578 \n",
       "L 538 4666 \n",
       "L 3609 4666 \n",
       "L 3609 4347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"170.688541\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.00 -->\n",
       "      <g transform=\"translate(20.878125 174.487759) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"144.230921\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 1.25 -->\n",
       "      <g transform=\"translate(20.878125 148.03014) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"117.773301\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 1.50 -->\n",
       "      <g transform=\"translate(20.878125 121.57252) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"91.315681\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 1.75 -->\n",
       "      <g transform=\"translate(20.878125 95.1149) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"64.858062\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 2.00 -->\n",
       "      <g transform=\"translate(20.878125 68.65728) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m24c9ebcaa8\" x=\"50.14375\" y=\"38.400442\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 2.25 -->\n",
       "      <g transform=\"translate(20.878125 42.199661) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- Loss -->\n",
       "     <g transform=\"translate(14.798438 148.955384) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-4c\" d=\"M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2175 4666 \n",
       "L 2175 4331 \n",
       "L 1581 4331 \n",
       "L 1581 384 \n",
       "L 3713 384 \n",
       "L 3713 1166 \n",
       "L 4097 1166 \n",
       "L 4097 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-6f\" d=\"M 1925 219 \n",
       "Q 2388 219 2623 584 \n",
       "Q 2859 950 2859 1663 \n",
       "Q 2859 2375 2623 2739 \n",
       "Q 2388 3103 1925 3103 \n",
       "Q 1463 3103 1227 2739 \n",
       "Q 991 2375 991 1663 \n",
       "Q 991 950 1228 584 \n",
       "Q 1466 219 1925 219 \n",
       "z\n",
       "M 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2456 758 2934 \n",
       "Q 1197 3413 1925 3413 \n",
       "Q 2653 3413 3092 2934 \n",
       "Q 3531 2456 3531 1663 \n",
       "Q 3531 869 3092 389 \n",
       "Q 2653 -91 1925 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-73\" d=\"M 359 184 \n",
       "L 359 959 \n",
       "L 691 959 \n",
       "Q 703 588 923 403 \n",
       "Q 1144 219 1575 219 \n",
       "Q 1963 219 2166 364 \n",
       "Q 2369 509 2369 788 \n",
       "Q 2369 1006 2220 1140 \n",
       "Q 2072 1275 1594 1428 \n",
       "L 1178 1569 \n",
       "Q 750 1706 558 1912 \n",
       "Q 366 2119 366 2438 \n",
       "Q 366 2894 700 3153 \n",
       "Q 1034 3413 1625 3413 \n",
       "Q 1888 3413 2178 3344 \n",
       "Q 2469 3275 2778 3144 \n",
       "L 2778 2419 \n",
       "L 2447 2419 \n",
       "Q 2434 2741 2221 2922 \n",
       "Q 2009 3103 1644 3103 \n",
       "Q 1281 3103 1095 2975 \n",
       "Q 909 2847 909 2591 \n",
       "Q 909 2381 1050 2254 \n",
       "Q 1191 2128 1613 1997 \n",
       "L 2069 1856 \n",
       "Q 2541 1709 2748 1489 \n",
       "Q 2956 1269 2956 922 \n",
       "Q 2956 450 2595 179 \n",
       "Q 2234 -91 1600 -91 \n",
       "Q 1278 -91 972 -22 \n",
       "Q 666 47 359 184 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"66.40625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"126.611328\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"177.929688\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 60.690909 32.811136 \n",
       "L 81.855777 62.878464 \n",
       "L 103.020645 96.069608 \n",
       "L 124.185512 149.633839 \n",
       "L 145.35038 163.596899 \n",
       "L 166.515248 205.687895 \n",
       "L 187.680116 209.873037 \n",
       "L 208.844983 224.895987 \n",
       "L 230.009851 215.283437 \n",
       "L 251.174719 224.026857 \n",
       "L 271.634091 224.071814 \n",
       "\" clip-path=\"url(#pae96e05a02)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_18\">\n",
       "    <path d=\"M 60.690909 32.788629 \n",
       "L 81.855777 63.526597 \n",
       "L 103.020645 106.687365 \n",
       "L 124.185512 149.414939 \n",
       "L 145.35038 184.218084 \n",
       "L 166.515248 206.105207 \n",
       "L 187.680116 220.008612 \n",
       "L 208.844983 226.608931 \n",
       "L 230.009851 234.071139 \n",
       "L 251.174719 238.459442 \n",
       "L 271.634091 242.198701 \n",
       "\" clip-path=\"url(#pae96e05a02)\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 252.669205 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 282.18125 252.669205 \n",
       "L 282.18125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 252.669205 \n",
       "L 282.18125 252.669205 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 282.18125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 190.028125 59.674375 \n",
       "L 275.18125 59.674375 \n",
       "Q 277.18125 59.674375 277.18125 57.674375 \n",
       "L 277.18125 29.318125 \n",
       "Q 277.18125 27.318125 275.18125 27.318125 \n",
       "L 190.028125 27.318125 \n",
       "Q 188.028125 27.318125 188.028125 29.318125 \n",
       "L 188.028125 57.674375 \n",
       "Q 188.028125 59.674375 190.028125 59.674375 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 192.028125 35.416563 \n",
       "L 202.028125 35.416563 \n",
       "L 212.028125 35.416563 \n",
       "\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- Train Loss -->\n",
       "     <g transform=\"translate(220.028125 38.916563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-54\" d=\"M 1222 0 \n",
       "L 1222 331 \n",
       "L 1819 331 \n",
       "L 1819 4294 \n",
       "L 447 4294 \n",
       "L 447 3566 \n",
       "L 63 3566 \n",
       "L 63 4666 \n",
       "L 4206 4666 \n",
       "L 4206 3566 \n",
       "L 3822 3566 \n",
       "L 3822 4294 \n",
       "L 2450 4294 \n",
       "L 2450 331 \n",
       "L 3047 331 \n",
       "L 3047 0 \n",
       "L 1222 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-72\" d=\"M 3059 3328 \n",
       "L 3059 2497 \n",
       "L 2728 2497 \n",
       "Q 2713 2744 2591 2866 \n",
       "Q 2469 2988 2234 2988 \n",
       "Q 1809 2988 1582 2694 \n",
       "Q 1356 2400 1356 1850 \n",
       "L 1356 331 \n",
       "L 2022 331 \n",
       "L 2022 0 \n",
       "L 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2994 \n",
       "L 231 2994 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1525 3078 1790 3245 \n",
       "Q 2056 3413 2438 3413 \n",
       "Q 2578 3413 2733 3391 \n",
       "Q 2888 3369 3059 3328 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-61\" d=\"M 2547 1044 \n",
       "L 2547 1747 \n",
       "L 1806 1747 \n",
       "Q 1378 1747 1168 1562 \n",
       "Q 959 1378 959 997 \n",
       "Q 959 650 1171 447 \n",
       "Q 1384 244 1747 244 \n",
       "Q 2106 244 2326 466 \n",
       "Q 2547 688 2547 1044 \n",
       "z\n",
       "M 3122 2075 \n",
       "L 3122 331 \n",
       "L 3634 331 \n",
       "L 3634 0 \n",
       "L 2547 0 \n",
       "L 2547 359 \n",
       "Q 2356 128 2106 18 \n",
       "Q 1856 -91 1522 -91 \n",
       "Q 969 -91 644 203 \n",
       "Q 319 497 319 997 \n",
       "Q 319 1513 691 1797 \n",
       "Q 1063 2081 1741 2081 \n",
       "L 2547 2081 \n",
       "L 2547 2309 \n",
       "Q 2547 2688 2317 2895 \n",
       "Q 2088 3103 1672 3103 \n",
       "Q 1328 3103 1125 2947 \n",
       "Q 922 2791 872 2484 \n",
       "L 575 2484 \n",
       "L 575 3156 \n",
       "Q 875 3284 1158 3348 \n",
       "Q 1441 3413 1709 3413 \n",
       "Q 2400 3413 2761 3070 \n",
       "Q 3122 2728 3122 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-69\" d=\"M 622 4353 \n",
       "Q 622 4497 726 4603 \n",
       "Q 831 4709 978 4709 \n",
       "Q 1122 4709 1226 4603 \n",
       "Q 1331 4497 1331 4353 \n",
       "Q 1331 4206 1228 4103 \n",
       "Q 1125 4000 978 4000 \n",
       "Q 831 4000 726 4103 \n",
       "Q 622 4206 622 4353 \n",
       "z\n",
       "M 1356 331 \n",
       "L 1900 331 \n",
       "L 1900 0 \n",
       "L 231 0 \n",
       "L 231 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 331 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-6e\" d=\"M 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1516 3069 1770 3241 \n",
       "Q 2025 3413 2363 3413 \n",
       "Q 2913 3413 3172 3097 \n",
       "Q 3431 2781 3431 2113 \n",
       "L 3431 331 \n",
       "L 3944 331 \n",
       "L 3944 0 \n",
       "L 2356 0 \n",
       "L 2356 331 \n",
       "L 2853 331 \n",
       "L 2853 1931 \n",
       "Q 2853 2541 2703 2767 \n",
       "Q 2553 2994 2175 2994 \n",
       "Q 1775 2994 1565 2701 \n",
       "Q 1356 2409 1356 1850 \n",
       "L 1356 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 263 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-20\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"66.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"114.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-69\" x=\"174.121094\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6e\" x=\"206.103516\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"270.507812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\" x=\"302.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"368.701172\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"428.90625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"480.224609\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\">\n",
       "     <path d=\"M 192.028125 50.094688 \n",
       "L 202.028125 50.094688 \n",
       "L 212.028125 50.094688 \n",
       "\" style=\"fill: none; stroke: #ff0000; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- Test Loss -->\n",
       "     <g transform=\"translate(220.028125 53.594688) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"58.949219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"118.128906\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"169.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"209.632812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\" x=\"241.419922\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"307.826172\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"368.03125\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"419.349609\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_2\">\n",
       "   <g clip-path=\"url(#p16730312f0)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAUAAAAFACAYAAADNkKWqAAAFt0lEQVR4nO3dPUpcbRiA4TP6IVGIpgsWYiXYCGlciD+QDbiqIJZpA9ZuQrBxAVbCNBZ2TpYw54DD+ch9XfXDy1sM9zzNy1kMw7AaAIK25r4AwFwEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsv6b+wK0XVxcjJ69vb2ddPbJycno2eVyOels/g02QCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgazFMAyruS9B15QnaAcHB5POnvLM7v7+ftLZ/BtsgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGT5LCaz+vbt2+jZ1cqzdT6XDRDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMjyVThmtVgs5r4CYTZAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEshbDMKzmvgRdHx8fo2dfX18nnf39+/ep1yHGBghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJDls5h8uqurq7mvAKPYAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMjyFphPt7+/v5FzHx4eNnIuXTZAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLE/h+HTn5+cbOffp6Wkj59JlAwSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBrMQzDau5L8P92fHw8af7x8XH07MHBwejZ09PTSfd4fn6eNE+PDRDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCDLZzFZa3d3d9L8169fR8+uVl5iMh8bIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJancMzq5eVl9OxyudzgTSiyAQJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJDlLTBrHR0dbezsL1++jJ7d2dnZ2D1osgECWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkeQrHWpeXlxs7++3tbfTs+/v7xu5Bkw0QyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLG+BWev6+nrS/GKxGD378vIyena5XE66B6xjAwSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMjyFC7qx48fo2f39vYmnb1arUbP3t3dTTobPpMNEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSxvgaMODw9Hz+7s7GzsHn/+/NnY2bCODRDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBrMQzD+G8YkvT79+9J8z9//hw9u729PfU68GlsgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlm+CsdaZ2dnc18BNsIGCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJbPYrLWzc3NpPlfv36Nnt3a8h/MfPz6gCwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsjyFA7JsgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQNZf3KBQDCQ3q0cAAAAASUVORK5CYII=\" id=\"image16a2179af9\" transform=\"scale(1 -1) translate(0 -230.4)\" x=\"330.40571\" y=\"-22.269205\" width=\"230.4\" height=\"230.4\"/>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- Predicted: 1, Actual: 1 -->\n",
       "    <g transform=\"translate(377.431562 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSerif-50\" d=\"M 1581 2375 \n",
       "L 2406 2375 \n",
       "Q 2872 2375 3115 2626 \n",
       "Q 3359 2878 3359 3353 \n",
       "Q 3359 3831 3115 4081 \n",
       "Q 2872 4331 2406 4331 \n",
       "L 1581 4331 \n",
       "L 1581 2375 \n",
       "z\n",
       "M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2559 4666 \n",
       "Q 3259 4666 3668 4311 \n",
       "Q 4078 3956 4078 3353 \n",
       "Q 4078 2753 3668 2397 \n",
       "Q 3259 2041 2559 2041 \n",
       "L 1581 2041 \n",
       "L 1581 331 \n",
       "L 2303 331 \n",
       "L 2303 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-64\" d=\"M 3359 331 \n",
       "L 3909 331 \n",
       "L 3909 0 \n",
       "L 2784 0 \n",
       "L 2784 519 \n",
       "Q 2616 206 2355 57 \n",
       "Q 2094 -91 1709 -91 \n",
       "Q 1097 -91 708 395 \n",
       "Q 319 881 319 1663 \n",
       "Q 319 2444 706 2928 \n",
       "Q 1094 3413 1709 3413 \n",
       "Q 2094 3413 2355 3264 \n",
       "Q 2616 3116 2784 2803 \n",
       "L 2784 4531 \n",
       "L 2241 4531 \n",
       "L 2241 4863 \n",
       "L 3359 4863 \n",
       "L 3359 331 \n",
       "z\n",
       "M 2784 1497 \n",
       "L 2784 1825 \n",
       "Q 2784 2422 2554 2737 \n",
       "Q 2325 3053 1888 3053 \n",
       "Q 1444 3053 1217 2703 \n",
       "Q 991 2353 991 1663 \n",
       "Q 991 975 1217 622 \n",
       "Q 1444 269 1888 269 \n",
       "Q 2325 269 2554 583 \n",
       "Q 2784 897 2784 1497 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-63\" d=\"M 3291 997 \n",
       "Q 3169 466 2822 187 \n",
       "Q 2475 -91 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2459 759 2936 \n",
       "Q 1200 3413 1925 3413 \n",
       "Q 2241 3413 2553 3339 \n",
       "Q 2866 3266 3181 3116 \n",
       "L 3181 2266 \n",
       "L 2847 2266 \n",
       "Q 2781 2703 2561 2903 \n",
       "Q 2341 3103 1931 3103 \n",
       "Q 1466 3103 1228 2742 \n",
       "Q 991 2381 991 1663 \n",
       "Q 991 944 1227 581 \n",
       "Q 1463 219 1931 219 \n",
       "Q 2303 219 2525 412 \n",
       "Q 2747 606 2828 997 \n",
       "L 3291 997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-3a\" d=\"M 666 325 \n",
       "Q 666 500 786 622 \n",
       "Q 906 744 1081 744 \n",
       "Q 1256 744 1376 622 \n",
       "Q 1497 500 1497 325 \n",
       "Q 1497 150 1378 29 \n",
       "Q 1259 -91 1081 -91 \n",
       "Q 903 -91 784 29 \n",
       "Q 666 150 666 325 \n",
       "z\n",
       "M 666 2363 \n",
       "Q 666 2538 786 2658 \n",
       "Q 906 2778 1081 2778 \n",
       "Q 1259 2778 1378 2659 \n",
       "Q 1497 2541 1497 2363 \n",
       "Q 1497 2184 1378 2065 \n",
       "Q 1259 1947 1081 1947 \n",
       "Q 906 1947 786 2067 \n",
       "Q 666 2188 666 2363 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-2c\" d=\"M 231 -622 \n",
       "Q 525 -406 662 -114 \n",
       "Q 800 178 800 594 \n",
       "L 800 709 \n",
       "L 1416 709 \n",
       "Q 1391 175 1164 -208 \n",
       "Q 938 -591 481 -872 \n",
       "L 231 -622 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-41\" d=\"M 1281 1691 \n",
       "L 2994 1691 \n",
       "L 2138 3909 \n",
       "L 1281 1691 \n",
       "z\n",
       "M -38 0 \n",
       "L -38 331 \n",
       "L 372 331 \n",
       "L 2034 4666 \n",
       "L 2559 4666 \n",
       "L 4225 331 \n",
       "L 4684 331 \n",
       "L 4684 0 \n",
       "L 2988 0 \n",
       "L 2988 331 \n",
       "L 3506 331 \n",
       "L 3116 1356 \n",
       "L 1153 1356 \n",
       "L 763 331 \n",
       "L 1275 331 \n",
       "L 1275 0 \n",
       "L -38 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-75\" d=\"M 2266 3322 \n",
       "L 3341 3322 \n",
       "L 3341 331 \n",
       "L 3884 331 \n",
       "L 3884 0 \n",
       "L 2766 0 \n",
       "L 2766 588 \n",
       "Q 2606 256 2353 82 \n",
       "Q 2100 -91 1766 -91 \n",
       "Q 1213 -91 952 223 \n",
       "Q 691 538 691 1209 \n",
       "L 691 2988 \n",
       "L 172 2988 \n",
       "L 172 3322 \n",
       "L 1269 3322 \n",
       "L 1269 1388 \n",
       "Q 1269 781 1417 556 \n",
       "Q 1566 331 1947 331 \n",
       "Q 2347 331 2556 625 \n",
       "Q 2766 919 2766 1478 \n",
       "L 2766 2988 \n",
       "L 2266 2988 \n",
       "L 2266 3322 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-6c\" d=\"M 1313 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 184 0 \n",
       "L 184 331 \n",
       "L 738 331 \n",
       "L 738 4531 \n",
       "L 184 4531 \n",
       "L 184 4863 \n",
       "L 1313 4863 \n",
       "L 1313 331 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSerif-50\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-72\" x=\"67.285156\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-65\" x=\"115.087891\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-64\" x=\"174.267578\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-69\" x=\"238.28125\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-63\" x=\"270.263672\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-74\" x=\"326.269531\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-65\" x=\"366.455078\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-64\" x=\"425.634766\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-3a\" x=\"489.648438\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-20\" x=\"523.339844\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-31\" x=\"555.126953\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-2c\" x=\"618.75\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-20\" x=\"650.537109\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-41\" x=\"682.324219\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-63\" x=\"754.541016\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-74\" x=\"810.546875\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-75\" x=\"850.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-61\" x=\"915.136719\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6c\" x=\"974.755859\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-3a\" x=\"1006.738281\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-20\" x=\"1040.429688\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-31\" x=\"1072.216797\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       "  <g id=\"axes_3\">\n",
       "   <g id=\"matplotlib.axis_3\">\n",
       "    <g id=\"ytick_10\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <defs>\n",
       "       <path id=\"m0a48662b8c\" d=\"M 0 0 \n",
       "L 3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"231.843531\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_22\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(289.18125 235.64275) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_11\">\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"203.918484\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_23\">\n",
       "      <!-- 0.3 -->\n",
       "      <g transform=\"translate(289.18125 207.717702) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-33\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_12\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"175.993436\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_24\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(289.18125 179.792655) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-34\" d=\"M 2234 1581 \n",
       "L 2234 4063 \n",
       "L 641 1581 \n",
       "L 2234 1581 \n",
       "z\n",
       "M 3609 0 \n",
       "L 1484 0 \n",
       "L 1484 331 \n",
       "L 2234 331 \n",
       "L 2234 1247 \n",
       "L 197 1247 \n",
       "L 197 1588 \n",
       "L 2241 4750 \n",
       "L 2859 4750 \n",
       "L 2859 1581 \n",
       "L 3750 1581 \n",
       "L 3750 1247 \n",
       "L 2859 1247 \n",
       "L 2859 331 \n",
       "L 3609 331 \n",
       "L 3609 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_13\">\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"148.068389\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_25\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(289.18125 151.867607) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_14\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"120.143341\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_26\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(289.18125 123.94256) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-36\" d=\"M 2094 219 \n",
       "Q 2534 219 2771 542 \n",
       "Q 3009 866 3009 1472 \n",
       "Q 3009 2078 2771 2401 \n",
       "Q 2534 2725 2094 2725 \n",
       "Q 1647 2725 1412 2412 \n",
       "Q 1178 2100 1178 1509 \n",
       "Q 1178 888 1415 553 \n",
       "Q 1653 219 2094 219 \n",
       "z\n",
       "M 1075 2569 \n",
       "Q 1288 2803 1556 2918 \n",
       "Q 1825 3034 2163 3034 \n",
       "Q 2859 3034 3264 2615 \n",
       "Q 3669 2197 3669 1472 \n",
       "Q 3669 763 3233 336 \n",
       "Q 2797 -91 2069 -91 \n",
       "Q 1278 -91 853 498 \n",
       "Q 428 1088 428 2181 \n",
       "Q 428 3406 931 4078 \n",
       "Q 1434 4750 2350 4750 \n",
       "Q 2597 4750 2869 4703 \n",
       "Q 3141 4656 3425 4563 \n",
       "L 3425 3794 \n",
       "L 3072 3794 \n",
       "Q 3034 4109 2831 4275 \n",
       "Q 2628 4441 2284 4441 \n",
       "Q 1678 4441 1381 3981 \n",
       "Q 1084 3522 1075 2569 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_15\">\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"92.218293\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_27\">\n",
       "      <!-- 0.7 -->\n",
       "      <g transform=\"translate(289.18125 96.017512) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_16\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"64.293246\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_28\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(289.18125 68.092465) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-38\" d=\"M 2981 1275 \n",
       "Q 2981 1775 2732 2051 \n",
       "Q 2484 2328 2034 2328 \n",
       "Q 1584 2328 1336 2051 \n",
       "Q 1088 1775 1088 1275 \n",
       "Q 1088 772 1336 495 \n",
       "Q 1584 219 2034 219 \n",
       "Q 2484 219 2732 495 \n",
       "Q 2981 772 2981 1275 \n",
       "z\n",
       "M 2853 3541 \n",
       "Q 2853 3966 2637 4203 \n",
       "Q 2422 4441 2034 4441 \n",
       "Q 1650 4441 1433 4203 \n",
       "Q 1216 3966 1216 3541 \n",
       "Q 1216 3113 1433 2875 \n",
       "Q 1650 2638 2034 2638 \n",
       "Q 2422 2638 2637 2875 \n",
       "Q 2853 3113 2853 3541 \n",
       "z\n",
       "M 2516 2484 \n",
       "Q 3047 2413 3344 2092 \n",
       "Q 3641 1772 3641 1275 \n",
       "Q 3641 619 3225 264 \n",
       "Q 2809 -91 2034 -91 \n",
       "Q 1263 -91 845 264 \n",
       "Q 428 619 428 1275 \n",
       "Q 428 1772 725 2092 \n",
       "Q 1022 2413 1556 2484 \n",
       "Q 1084 2569 832 2842 \n",
       "Q 581 3116 581 3541 \n",
       "Q 581 4103 968 4426 \n",
       "Q 1356 4750 2034 4750 \n",
       "Q 2713 4750 3100 4426 \n",
       "Q 3488 4103 3488 3541 \n",
       "Q 3488 3116 3236 2842 \n",
       "Q 2984 2569 2516 2484 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_17\">\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m0a48662b8c\" x=\"282.18125\" y=\"36.368198\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_29\">\n",
       "      <!-- 0.9 -->\n",
       "      <g transform=\"translate(289.18125 40.167417) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-39\" d=\"M 2994 2091 \n",
       "Q 2784 1856 2512 1740 \n",
       "Q 2241 1625 1900 1625 \n",
       "Q 1206 1625 804 2044 \n",
       "Q 403 2463 403 3188 \n",
       "Q 403 3897 839 4323 \n",
       "Q 1275 4750 2003 4750 \n",
       "Q 2794 4750 3217 4161 \n",
       "Q 3641 3572 3641 2478 \n",
       "Q 3641 1253 3137 581 \n",
       "Q 2634 -91 1722 -91 \n",
       "Q 1475 -91 1203 -44 \n",
       "Q 931 3 647 97 \n",
       "L 647 872 \n",
       "L 997 872 \n",
       "Q 1038 556 1241 387 \n",
       "Q 1444 219 1784 219 \n",
       "Q 2391 219 2687 676 \n",
       "Q 2984 1134 2994 2091 \n",
       "z\n",
       "M 1978 4441 \n",
       "Q 1534 4441 1298 4117 \n",
       "Q 1063 3794 1063 3188 \n",
       "Q 1063 2581 1298 2256 \n",
       "Q 1534 1931 1978 1931 \n",
       "Q 2422 1931 2658 2245 \n",
       "Q 2894 2559 2894 3150 \n",
       "Q 2894 3772 2656 4106 \n",
       "Q 2419 4441 1978 4441 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-39\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_30\">\n",
       "     <!-- Accuracy -->\n",
       "     <g transform=\"translate(316.540625 160.921009) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-79\" d=\"M 1381 -609 \n",
       "L 1600 -56 \n",
       "L 359 2988 \n",
       "L -19 2988 \n",
       "L -19 3322 \n",
       "L 1509 3322 \n",
       "L 1509 2988 \n",
       "L 978 2988 \n",
       "L 1913 703 \n",
       "L 2847 2988 \n",
       "L 2350 2988 \n",
       "L 2350 3322 \n",
       "L 3597 3322 \n",
       "L 3597 2988 \n",
       "L 3225 2988 \n",
       "L 1703 -750 \n",
       "Q 1547 -1138 1356 -1280 \n",
       "Q 1166 -1422 819 -1422 \n",
       "Q 672 -1422 517 -1397 \n",
       "Q 363 -1372 206 -1325 \n",
       "L 206 -691 \n",
       "L 500 -691 \n",
       "Q 519 -903 608 -995 \n",
       "Q 697 -1088 884 -1088 \n",
       "Q 1056 -1088 1161 -992 \n",
       "Q 1266 -897 1381 -609 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-41\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"72.216797\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"128.222656\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-75\" x=\"184.228516\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"248.632812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"296.435547\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"356.054688\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-79\" x=\"412.060547\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_29\">\n",
       "    <path d=\"M 60.690909 242.198701 \n",
       "L 81.855777 134.589438 \n",
       "L 103.020645 114.329333 \n",
       "L 124.185512 83.925262 \n",
       "L 145.35038 60.913784 \n",
       "L 166.515248 48.685461 \n",
       "L 187.680116 43.710749 \n",
       "L 208.844983 42.710257 \n",
       "L 230.009851 37.429833 \n",
       "L 251.174719 34.150425 \n",
       "L 271.634091 32.788629 \n",
       "\" clip-path=\"url(#pae96e05a02)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 50.14375 252.669205 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 282.18125 252.669205 \n",
       "L 282.18125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 50.14375 252.669205 \n",
       "L 282.18125 252.669205 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 282.18125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_2\">\n",
       "    <g id=\"patch_12\">\n",
       "     <path d=\"M 172.184375 146.403821 \n",
       "L 275.18125 146.403821 \n",
       "Q 277.18125 146.403821 277.18125 144.403821 \n",
       "L 277.18125 130.583509 \n",
       "Q 277.18125 128.583509 275.18125 128.583509 \n",
       "L 172.184375 128.583509 \n",
       "Q 170.184375 128.583509 170.184375 130.583509 \n",
       "L 170.184375 144.403821 \n",
       "Q 170.184375 146.403821 172.184375 146.403821 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_30\">\n",
       "     <path d=\"M 174.184375 136.681946 \n",
       "L 184.184375 136.681946 \n",
       "L 194.184375 136.681946 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_31\">\n",
       "     <!-- Test Accuracy -->\n",
       "     <g transform=\"translate(202.184375 140.181946) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"58.949219\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"118.128906\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"169.447266\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-20\" x=\"209.632812\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-41\" x=\"241.419922\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"313.636719\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"369.642578\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-75\" x=\"425.648438\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-72\" x=\"490.052734\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-61\" x=\"537.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-63\" x=\"597.474609\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-79\" x=\"653.480469\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pae96e05a02\">\n",
       "   <rect x=\"50.14375\" y=\"22.318125\" width=\"232.0375\" height=\"230.35108\"/>\n",
       "  </clipPath>\n",
       "  <clipPath id=\"p16730312f0\">\n",
       "   <rect x=\"330.40571\" y=\"22.318125\" width=\"230.35108\" height=\"230.35108\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 800x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64  # Batch size for training and testing\n",
    "\n",
    "key = jax.random.PRNGKey(0)  # Random key for reproducibility\n",
    "\n",
    "# Data normalization and transformation\n",
    "normalise_data = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\"MNIST\", train=True, download=True, transform=normalise_data)\n",
    "test_dataset = torchvision.datasets.MNIST(\"MNIST\", train=False, download=True, transform=normalise_data)\n",
    "\n",
    "# Data loaders\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model\n",
    "class CNN(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        # Standard CNN setup: convolutional layer, followed by flattening, with a small MLP on top.\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(1, 3, kernel_size=4, key=key1),\n",
    "            eqx.nn.MaxPool2d(kernel_size=2),\n",
    "            jax.nn.relu,\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(1728, 512, key=key2),\n",
    "            jax.nn.sigmoid,\n",
    "            eqx.nn.Linear(512, 64, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, 10, key=key4),\n",
    "            jax.nn.log_softmax,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"1 28 28\"]) -> Float[Array, \"10\"]:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "model = CNN(subkey)\n",
    "\n",
    "# Loss & Inference & Evaluation\n",
    "@eqx.filter_jit\n",
    "def loss(model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \"batch\"]) -> Float[Array, \"\"]:\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return cross_entropy(y, pred_y)\n",
    "\n",
    "def cross_entropy(y: Int[Array, \"batch\"], pred_y: Float[Array, \"batch 10\"]) -> Float[Array, \"\"]:\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(model: CNN, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \"batch\"]) -> Float[Array, \"\"]:\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)\n",
    "\n",
    "def evaluate(model: CNN, testloader: torch.utils.data.DataLoader):\n",
    "    avg_loss, avg_acc = 0, 0\n",
    "    for x, y in testloader:\n",
    "        avg_loss += loss(model, x.numpy(), y.numpy())\n",
    "        avg_acc += compute_accuracy(model, x.numpy(), y.numpy())\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)\n",
    "\n",
    "# Training\n",
    "STEPS = 300  # Number of training steps\n",
    "PRINT_EVERY = 30  # Print every 30 steps\n",
    "LEARNING_RATE = 3e-4  # Learning rate\n",
    "\n",
    "optim = optax.adamw(LEARNING_RATE)  # Adam optimizer with weight decay\n",
    "\n",
    "def train(model, trainloader, testloader, optim, steps, print_every):\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model: CNN, opt_state: PyTree, x: Float[Array, \"batch 1 28 28\"], y: Int[Array, \"batch\"]):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from trainloader\n",
    "\n",
    "    lsteps, train_losses, test_losses, test_accuracies = [], [], [], []\n",
    "\n",
    "    for step, (x, y) in zip(range(steps), infinite_trainloader()):\n",
    "        # PyTorch dataloaders give PyTorch tensors by default, so convert them to NumPy arrays.\n",
    "        x, y = x.numpy(), y.numpy()\n",
    "\n",
    "        model, opt_state, train_loss = make_step(model, opt_state, x, y)\n",
    "\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "            test_loss, test_accuracy = evaluate(model, testloader)\n",
    "            lsteps.append(step)\n",
    "            train_losses.append(train_loss.item())\n",
    "            test_losses.append(test_loss.item())\n",
    "            test_accuracies.append(test_accuracy.item())\n",
    "            print(f\"{step=}, train_loss={train_loss.item()}, test_loss={test_loss.item()}, test_accuracy={test_accuracy.item()}\")\n",
    "\n",
    "    return model, lsteps, train_losses, test_losses, test_accuracies\n",
    "\n",
    "model, lsteps, train_losses, test_losses, test_accuracies = train(model, trainloader, testloader, optim, STEPS, PRINT_EVERY)\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "\n",
    "# Plot training and testing loss in the first subplot\n",
    "axs[0].plot(lsteps, train_losses, 'b-', label='Train Loss')\n",
    "axs[0].plot(lsteps, test_losses, 'r-', label='Test Loss')\n",
    "axs[0].set_xlabel('Step')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot testing accuracy in the second subplot\n",
    "ax0 = axs[0].twinx()\n",
    "ax0.plot(lsteps, test_accuracies, 'g--', label='Test Accuracy')\n",
    "ax0.set_xlabel('Step')\n",
    "ax0.set_ylabel('Accuracy')\n",
    "ax0.legend(loc='center right')\n",
    "\n",
    "# Plot a sample from the MNIST dataset in the third subplot\n",
    "sample_x, sample_y = next(iter(testloader))\n",
    "sample_x, sample_y = sample_x.numpy()[0], sample_y.numpy()[0]\n",
    "sample_pred_y = model(sample_x).argmax()\n",
    "axs[1].imshow(sample_x.squeeze(), cmap='gray')\n",
    "axs[1].set_title(f'Predicted: {sample_pred_y}, Actual: {sample_y}')\n",
    "axs[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks: Addressing the Vanishing Gradient Problem\n",
    "\n",
    "Residual Networks (ResNets) are a class of deep neural network architectures that have significantly advanced the field of deep learning. They enable the training of much deeper networks by effectively mitigating issues like vanishing and exploding gradients. This comprehensive explanation covers the challenges associated with training deep networks, the concept of residual blocks, the role of batch normalization, and how these elements contribute to smoother loss surfaces and improved network performance.\n",
    "\n",
    "## The Vanishing and Exploding Gradient Problem\n",
    "\n",
    "### Understanding Gradients in Deep Networks\n",
    "\n",
    "In neural networks, training involves adjusting the network's weights to minimize a loss function that measures the difference between predicted outputs and actual targets. This adjustment relies on gradients—partial derivatives of the loss function with respect to each weight—calculated through backpropagation. Gradients indicate the direction and magnitude by which weights should be updated to reduce the loss.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "In very deep networks, gradients computed for earlier layers can become exceedingly small, effectively vanishing. This occurs because backpropagation involves the multiplication of many small derivatives (from activation functions like sigmoid or tanh) as gradients are propagated backward through the network layers. When these derivatives are less than one, their repeated multiplication results in a gradient that approaches zero exponentially with network depth.\n",
    "\n",
    "**Impact**:\n",
    "- Early layers learn very slowly or not at all.\n",
    "- The network struggles to capture low-level features essential for deep learning tasks.\n",
    "- Training becomes inefficient and may not converge to an optimal solution.\n",
    "\n",
    "### Exploding Gradients\n",
    "\n",
    "Conversely, gradients can also become excessively large, or explode, when the derivatives during backpropagation are greater than one. This leads to numerical instability and erratic weight updates, making it difficult for the network to converge.\n",
    "\n",
    "**Impact**:\n",
    "- Weight updates become too large, causing the loss function to oscillate or diverge.\n",
    "- Training may fail entirely due to numerical overflow or instability.\n",
    "\n",
    "### Shattered Gradients Phenomenon\n",
    "\n",
    "In addition to vanishing and exploding gradients, very deep networks can suffer from \"shattered gradients,\" where the gradient mappings become highly irregular. Small changes in inputs or parameters can cause large, unpredictable changes in gradients, making optimization via gradient descent methods challenging.\n",
    "\n",
    "## Residual Blocks: The Core of Residual Networks\n",
    "\n",
    "### The Concept of Residual Learning\n",
    "\n",
    "Residual learning addresses the vanishing gradient problem by allowing layers to learn modifications (residuals) to the identity mapping rather than complete transformations. Instead of each layer attempting to learn a direct mapping $ \\mathcal{H}(\\mathbf{x}) $ of the input $ \\mathbf{x} $, it learns a residual function $ \\mathcal{F}(\\mathbf{x}) = \\mathcal{H}(\\mathbf{x}) - \\mathbf{x} $, or equivalently, $ \\mathcal{H}(\\mathbf{x}) = \\mathbf{x} + \\mathcal{F}(\\mathbf{x}) $.\n",
    "\n",
    "### Structure of a Residual Block\n",
    "\n",
    "A residual block typically consists of:\n",
    "\n",
    "- **Convolutional Layers**: One or more layers that process the input to produce an intermediate output.\n",
    "- **Shortcut Connection (Skip Connection)**: A direct path that adds the input $ \\mathbf{x} $ to the output of the convolutional layers.\n",
    "- **Activation Function**: Applied after the addition to introduce non-linearity.\n",
    "\n",
    "The output of a residual block is:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{x} + \\mathcal{F}(\\mathbf{x}, \\{W_i\\})\n",
    "$$\n",
    "\n",
    "where $ \\mathcal{F} $ represents the residual function learned by the convolutional layers with weights $ \\{W_i\\} $.\n",
    "\n",
    "### Benefits of Residual Blocks\n",
    "\n",
    "1. **Improved Gradient Flow**: Skip connections provide alternate paths for gradients to backpropagate, mitigating the vanishing gradient problem.\n",
    "\n",
    "2. **Ease of Optimization**: Learning residuals simplifies the optimization process, as it is often easier to learn the residual mapping $ \\mathcal{F}(\\mathbf{x}) $ than the original mapping $ \\mathcal{H}(\\mathbf{x}) $.\n",
    "\n",
    "3. **Identity Mapping**: If the optimal mapping is an identity function, residual blocks can easily approximate it by driving $ \\mathcal{F}(\\mathbf{x}) $ to zero.\n",
    "\n",
    "4. **Deep Network Training**: Residual blocks enable the training of much deeper networks (e.g., over 100 layers) without performance degradation.\n",
    "\n",
    "### Understanding Residual Blocks as Ensembles\n",
    "\n",
    "Residual networks can be viewed as ensembles of networks of varying depths. The skip connections allow the network to combine outputs from multiple paths, effectively integrating features learned at different levels.\n",
    "\n",
    "## Batch Normalization: Stabilizing Deep Network Training\n",
    "\n",
    "### The Need for Batch Normalization\n",
    "\n",
    "As networks deepen, they become more sensitive to the initialization of weights and the distribution of inputs to each layer. Internal covariate shift—changes in the distribution of layer inputs during training—can slow down training and require careful weight initialization and low learning rates.\n",
    "\n",
    "### How Batch Normalization Works\n",
    "\n",
    "Batch normalization standardizes the inputs to each layer by adjusting and scaling the activations:\n",
    "\n",
    "1. **Compute Mean and Variance**:\n",
    "\n",
    "   For each feature over a mini-batch $ B $:\n",
    "\n",
    "   $$\n",
    "   \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} h_i, \\quad \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (h_i - \\mu_B)^2\n",
    "   $$\n",
    "\n",
    "2. **Normalize**:\n",
    "\n",
    "   $$\n",
    "   \\hat{h}_i = \\frac{h_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "   where $ \\epsilon $ is a small constant to prevent division by zero.\n",
    "\n",
    "3. **Scale and Shift**:\n",
    "\n",
    "   Introduce learnable parameters $ \\gamma $ and $ \\beta $ to allow the network to restore the original distribution if needed:\n",
    "\n",
    "   $$\n",
    "   h_i^{\\text{BN}} = \\gamma \\hat{h}_i + \\beta\n",
    "   $$\n",
    "\n",
    "### Benefits of Batch Normalization\n",
    "\n",
    "1. **Accelerated Training**: Allows for higher learning rates by reducing the risk of exploding gradients.\n",
    "\n",
    "2. **Reduced Sensitivity to Initialization**: Less dependence on careful weight initialization, making the network more robust.\n",
    "\n",
    "3. **Regularization Effect**: Acts as a form of regularization by adding noise to the activations due to batch statistics, potentially reducing overfitting.\n",
    "\n",
    "4. **Stabilized Activations**: Maintains consistent distributions of layer inputs, which helps in training very deep networks.\n",
    "\n",
    "## Combining Residual Blocks and Batch Normalization\n",
    "\n",
    "### Addressing Vanishing and Exploding Gradients\n",
    "\n",
    "- **Residual Blocks**: Mitigate vanishing gradients by providing direct pathways for gradient flow, ensuring that earlier layers receive sufficient gradient signals.\n",
    "- **Batch Normalization**: Addresses exploding gradients by normalizing activations, keeping them within a stable range and preventing large updates.\n",
    "\n",
    "### Smoother Loss Surface\n",
    "\n",
    "Both residual connections and batch normalization contribute to a smoother loss surface:\n",
    "\n",
    "- **Residual Connections**: Simplify the loss landscape by allowing the network to learn identity mappings more easily and by reducing the depth that gradients need to traverse without attenuation.\n",
    "- **Batch Normalization**: Ensures that the activation distributions remain stable across layers and iterations, smoothing the optimization landscape.\n",
    "\n",
    "A smoother loss surface facilitates more effective optimization, allowing gradient descent algorithms to find better minima more reliably.\n",
    "\n",
    "## Practical Insights into Residual Networks\n",
    "\n",
    "### Training Very Deep Networks\n",
    "\n",
    "Residual networks enable the training of networks with significantly more layers than previously possible. For example, ResNets with 50, 101, or even 152 layers have been successfully trained and have achieved state-of-the-art performance on various tasks.\n",
    "\n",
    "### Performance Improvements\n",
    "\n",
    "- **Image Classification**: ResNets have achieved top results on datasets like ImageNet and CIFAR-10.\n",
    "- **Object Detection and Segmentation**: Deeper residual networks improve feature representation, benefiting tasks requiring detailed understanding of images.\n",
    "- **Transfer Learning**: Pre-trained ResNets serve as powerful feature extractors for various downstream applications.\n",
    "\n",
    "### Architectural Variations\n",
    "\n",
    "- **Bottleneck Blocks**: Introduce a three-layer block with $ 1 \\times 1 $, $ 3 \\times 3 $, and $ 1 \\times 1 $ convolutions to reduce computation while maintaining depth.\n",
    "- **Identity vs. Projection Shortcuts**: When the dimensions of input and output differ, projection shortcuts (using $ 1 \\times 1 $ convolutions) adjust dimensions to enable addition.\n",
    "- **Pre-Activation ResNets**: Modify the order of operations within a residual block (batch normalization and activation before convolution) to improve gradient flow.\n",
    "\n",
    "## Theoretical Understanding\n",
    "\n",
    "### Why Residual Networks Work\n",
    "\n",
    "1. **Identity Paths**: The skip connections create paths in the network where information and gradients can flow without attenuation, effectively creating an ensemble of shallower networks within the deep architecture.\n",
    "\n",
    "2. **Ease of Learning**: Learning residuals is theoretically easier than learning unreferenced functions because the network can focus on learning the changes required from the identity mapping.\n",
    "\n",
    "3. **Linear Behavior**: In the extreme case where residual functions are zero, the network behaves like an identity function, which is easy to optimize and serves as a good initialization point.\n",
    "\n",
    "### Limitations and Considerations\n",
    "\n",
    "- **Overfitting**: Deeper networks have more parameters and may overfit if not properly regularized.\n",
    "- **Computational Resources**: Very deep networks require significant computational power and memory.\n",
    "\n",
    "## Implementation Guidelines\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "- **He Initialization**: Initialize weights to account for ReLU activations, helping to maintain variance through layers.\n",
    "- **Zero Initialization of Last Layer in Residual Block**: Initializing the last layer of a residual block to zero can help the network start as an identity function, which can improve training stability.\n",
    "\n",
    "### Learning Rate and Optimization\n",
    "\n",
    "- **Learning Rate Schedules**: Use learning rate decay strategies to improve convergence.\n",
    "- **Optimizers**: Adaptive optimizers like Adam can be used, but standard SGD with momentum often works well with batch normalization.\n",
    "\n",
    "### Regularization Techniques\n",
    "\n",
    "- **Data Augmentation**: Enhances generalization by increasing the diversity of training data.\n",
    "- **Dropout**: May not be necessary due to the regularization effect of batch normalization and residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data by incorporating temporal dependencies. Unlike feedforward neural networks, RNNs maintain a hidden state that captures information about previous inputs, enabling them to model sequences and time-dependent phenomena effectively. This comprehensive explanation covers the fundamentals of RNNs, various architectures for sequence processing, training methodologies, challenges like vanishing and exploding gradients, and advanced units like Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) networks.\n",
    "\n",
    "## Fundamentals of Recurrent Neural Networks\n",
    "\n",
    "### Sequence Modeling with RNNs\n",
    "\n",
    "RNNs are particularly suited for tasks where the input data is sequential, and the order of the data points matters. Examples include:\n",
    "\n",
    "- **Natural Language Processing**: Sentences are sequences of words where context and order are crucial.\n",
    "- **Time Series Analysis**: Financial data, sensor readings, or any data that varies over time.\n",
    "- **Speech Recognition**: Audio signals processed over time.\n",
    "\n",
    "An RNN processes an input sequence $ \\mathbf{x}_{1:T} = (\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_T) $ by recursively applying a transition function to update its hidden state $ \\mathbf{h}_t $, which serves as a memory of the past inputs.\n",
    "\n",
    "### The Recurrent Relation\n",
    "\n",
    "The hidden state at time $ t $ is computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\mathbf{x}_t; \\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\mathbf{h}_{t-1} $ is the hidden state from the previous time step.\n",
    "- $ \\mathbf{x}_t $ is the input at time $ t $.\n",
    "- $ f $ is a nonlinear activation function (e.g., $ \\tanh $, ReLU).\n",
    "- $ \\boldsymbol{\\theta} $ represents the parameters (weights and biases) of the network.\n",
    "\n",
    "The output $ \\mathbf{y}_t $ at each time step can be computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = g(\\mathbf{h}_t; \\boldsymbol{\\phi})\n",
    "$$\n",
    "\n",
    "where $ g $ is an output function (e.g., a linear transformation followed by a softmax for classification), and $ \\boldsymbol{\\phi} $ are the output layer parameters.\n",
    "\n",
    "## Vec2Seq: Sequence Generation\n",
    "\n",
    "Vec2Seq models map a fixed-length input vector to a variable-length output sequence. They are useful in tasks like:\n",
    "\n",
    "- **Image Captioning**: Generating a sequence of words (caption) from an image feature vector.\n",
    "- **Text Generation**: Generating text based on a latent representation.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Given an input vector $ \\mathbf{x} \\in \\mathbb{R}^D $, the RNN generates an output sequence $ \\mathbf{y}_{1:T} $ by:\n",
    "\n",
    "1. **Initializing the Hidden State**:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{h}_0 = \\phi_{\\text{init}}(\\mathbf{x})\n",
    "   $$\n",
    "\n",
    "   where $ \\phi_{\\text{init}} $ is a function that initializes the hidden state based on the input vector.\n",
    "\n",
    "2. **Recursively Generating Outputs**:\n",
    "\n",
    "   For $ t = 1 $ to $ T $:\n",
    "\n",
    "   - **Compute Hidden State**:\n",
    "\n",
    "     $$\n",
    "     \\mathbf{h}_t = f(\\mathbf{h}_{t-1}, \\mathbf{y}_{t-1}; \\boldsymbol{\\theta})\n",
    "     $$\n",
    "\n",
    "     where $ \\mathbf{y}_{t-1} $ is the output from the previous time step (during training, the actual previous output is used; during inference, the generated output is used).\n",
    "\n",
    "   - **Generate Output**:\n",
    "\n",
    "     $$\n",
    "     \\mathbf{y}_t \\sim p(\\mathbf{y}_t | \\mathbf{h}_t)\n",
    "     $$\n",
    "\n",
    "     where $ p $ is a probability distribution over possible outputs, often modeled using a softmax function.\n",
    "\n",
    "### Conditional Generative Modeling\n",
    "\n",
    "The model defines a conditional probability distribution over sequences:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}_{1:T} | \\mathbf{x}) = \\prod_{t=1}^T p(\\mathbf{y}_t | \\mathbf{h}_t)\n",
    "$$\n",
    "\n",
    "This captures dependencies between the output tokens, allowing the model to generate coherent sequences.\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "The model is trained to maximize the likelihood of the observed sequences in the training data. The loss function is typically the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^T \\log p(\\mathbf{y}_t | \\mathbf{h}_t)\n",
    "$$\n",
    "\n",
    "## Seq2Vec: Sequence Classification\n",
    "\n",
    "Seq2Vec models map a variable-length input sequence to a fixed-length output vector, suitable for tasks like:\n",
    "\n",
    "- **Sentiment Analysis**: Classifying the sentiment of a text sequence.\n",
    "- **Sequence Classification**: Assigning a label to an entire sequence.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "1. **Encoding the Input Sequence**:\n",
    "\n",
    "   - **Forward Pass**:\n",
    "\n",
    "     $$\n",
    "     \\mathbf{h}_t^{\\rightarrow} = f_{\\text{forward}}(\\mathbf{h}_{t-1}^{\\rightarrow}, \\mathbf{x}_t)\n",
    "     $$\n",
    "\n",
    "   - **Backward Pass** (for bidirectional RNNs):\n",
    "\n",
    "     $$\n",
    "     \\mathbf{h}_t^{\\leftarrow} = f_{\\text{backward}}(\\mathbf{h}_{t+1}^{\\leftarrow}, \\mathbf{x}_t)\n",
    "     $$\n",
    "\n",
    "   - **Combined Hidden State**:\n",
    "\n",
    "     $$\n",
    "     \\mathbf{h}_t = [\\mathbf{h}_t^{\\rightarrow}; \\mathbf{h}_t^{\\leftarrow}]\n",
    "     $$\n",
    "\n",
    "2. **Pooling**:\n",
    "\n",
    "   Aggregate the hidden states across time to obtain a fixed-length vector:\n",
    "\n",
    "   - **Average Pooling**:\n",
    "\n",
    "     $$\n",
    "     \\overline{\\mathbf{h}} = \\frac{1}{T} \\sum_{t=1}^T \\mathbf{h}_t\n",
    "     $$\n",
    "\n",
    "   - **Max Pooling**:\n",
    "\n",
    "     $$\n",
    "     \\overline{\\mathbf{h}} = \\max_{t} \\mathbf{h}_t\n",
    "     $$\n",
    "\n",
    "3. **Classification**:\n",
    "\n",
    "   - Apply a fully connected layer and softmax activation:\n",
    "\n",
    "     $$\n",
    "     p(y | \\mathbf{x}_{1:T}) = \\text{softmax}(\\mathbf{W} \\overline{\\mathbf{h}} + \\mathbf{b})\n",
    "     $$\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "The model is trained using a cross-entropy loss between the predicted class probabilities and the true labels.\n",
    "\n",
    "## Seq2Seq: Sequence Translation\n",
    "\n",
    "Seq2Seq models map a variable-length input sequence to a variable-length output sequence, potentially of different lengths. They are widely used in:\n",
    "\n",
    "- **Machine Translation**: Translating sentences from one language to another.\n",
    "- **Speech Recognition**: Converting audio signals to text sequences.\n",
    "\n",
    "### Encoder-Decoder Architecture\n",
    "\n",
    "1. **Encoder**:\n",
    "\n",
    "   - Processes the input sequence and summarizes it into a context vector.\n",
    "\n",
    "     For $ t = 1 $ to $ T $:\n",
    "\n",
    "     $$\n",
    "     \\mathbf{h}_t^{\\text{enc}} = f_{\\text{enc}}(\\mathbf{h}_{t-1}^{\\text{enc}}, \\mathbf{x}_t)\n",
    "     $$\n",
    "\n",
    "   - The context vector $ \\mathbf{c} $ can be the final hidden state $ \\mathbf{h}_T^{\\text{enc}} $ or a combination of all hidden states (e.g., through attention mechanisms).\n",
    "\n",
    "2. **Decoder**:\n",
    "\n",
    "   - Generates the output sequence based on the context vector.\n",
    "\n",
    "     For $ t = 1 $ to $ T' $:\n",
    "\n",
    "     $$\n",
    "     \\mathbf{h}_t^{\\text{dec}} = f_{\\text{dec}}(\\mathbf{h}_{t-1}^{\\text{dec}}, \\mathbf{y}_{t-1}, \\mathbf{c})\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\mathbf{y}_t \\sim p(\\mathbf{y}_t | \\mathbf{h}_t^{\\text{dec}})\n",
    "     $$\n",
    "\n",
    "### Aligned Case (Sequence Labeling)\n",
    "\n",
    "In tasks where the input and output sequences are aligned (same length), such as:\n",
    "\n",
    "- **Part-of-Speech Tagging**\n",
    "- **Named Entity Recognition**\n",
    "\n",
    "The model predicts an output at each time step:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}_{1:T} | \\mathbf{x}_{1:T}) = \\prod_{t=1}^T p(\\mathbf{y}_t | \\mathbf{h}_t)\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{h}_t $ depends on $ \\mathbf{x}_{1:t} $ or both past and future inputs in bidirectional RNNs.\n",
    "\n",
    "### Unaligned Case\n",
    "\n",
    "In tasks where the input and output sequences differ in length, the encoder-decoder framework is used, sometimes enhanced with attention mechanisms to allow the decoder to focus on different parts of the input sequence at each time step.\n",
    "\n",
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "Training RNNs involves computing gradients of the loss function with respect to the network parameters, which requires backpropagating errors through time.\n",
    "\n",
    "### Unfolding the Network\n",
    "\n",
    "An RNN can be unfolded over time to represent the sequential dependencies explicitly. Each time step is treated as a layer in a deep network, sharing parameters across time.\n",
    "\n",
    "### Computing Gradients\n",
    "\n",
    "The gradient of the loss $ \\mathcal{L} $ with respect to the parameters $ \\boldsymbol{\\theta} $ involves summing over time steps:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\theta}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_t} \\frac{\\partial \\mathbf{h}_t}{\\partial \\boldsymbol{\\theta}}\n",
    "$$\n",
    "\n",
    "This recursive dependency requires careful computation, as errors must be propagated back through all previous time steps.\n",
    "\n",
    "### Truncated BPTT\n",
    "\n",
    "Due to computational constraints and vanishing gradients over long sequences, BPTT is often truncated to a fixed number of time steps $ K $:\n",
    "\n",
    "- Only backpropagate errors for $ K $ steps.\n",
    "- Helps manage memory and computational resources.\n",
    "- May limit the ability to capture long-term dependencies.\n",
    "\n",
    "## Vanishing and Exploding Gradients\n",
    "\n",
    "### The Problem\n",
    "\n",
    "- **Vanishing Gradients**: Gradients diminish exponentially as they are backpropagated through many time steps, hindering the learning of long-range dependencies.\n",
    "- **Exploding Gradients**: Gradients can grow exponentially, leading to numerical instability and difficulty in convergence.\n",
    "\n",
    "### Causes\n",
    "\n",
    "- Repeated multiplication by weights and activation derivatives less than or greater than one.\n",
    "- The eigenvalues (spectral radius) of the recurrent weight matrix influence the gradient behavior.\n",
    "\n",
    "### Mitigation Strategies\n",
    "\n",
    "1. **Gradient Clipping**:\n",
    "\n",
    "   - Limit the gradient norms to prevent explosion.\n",
    "   - For gradient $ \\mathbf{g} $:\n",
    "\n",
    "     $$\n",
    "     \\mathbf{g} \\leftarrow \\mathbf{g} \\cdot \\min\\left(1, \\frac{\\tau}{\\|\\mathbf{g}\\|}\\right)\n",
    "     $$\n",
    "\n",
    "     where $ \\tau $ is a threshold.\n",
    "\n",
    "2. **Weight Initialization**:\n",
    "\n",
    "   - Initialize weights to ensure that the initial spectral radius is close to one.\n",
    "\n",
    "3. **Regularization Techniques**:\n",
    "\n",
    "   - L1 or L2 regularization to penalize large weights.\n",
    "\n",
    "4. **Advanced RNN Units**:\n",
    "\n",
    "   - Use architectures like GRU or LSTM that are designed to handle these issues.\n",
    "\n",
    "## Gating Mechanisms and Long-Term Memory\n",
    "\n",
    "To address the limitations of standard RNNs in capturing long-term dependencies, gating mechanisms are introduced to control the flow of information.\n",
    "\n",
    "### Gated Recurrent Units (GRU)\n",
    "\n",
    "GRUs are a type of RNN that uses gating mechanisms to regulate information flow without separate memory cells.\n",
    "\n",
    "#### Components\n",
    "\n",
    "1. **Reset Gate ($ \\mathbf{r}_t $)**:\n",
    "\n",
    "   - Determines how to combine the new input with the previous memory.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{r}_t = \\sigma(\\mathbf{W}_r \\mathbf{x}_t + \\mathbf{U}_r \\mathbf{h}_{t-1} + \\mathbf{b}_r)\n",
    "   $$\n",
    "\n",
    "2. **Update Gate ($ \\mathbf{z}_t $)**:\n",
    "\n",
    "   - Controls how much of the previous memory to keep.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{z}_t = \\sigma(\\mathbf{W}_z \\mathbf{x}_t + \\mathbf{U}_z \\mathbf{h}_{t-1} + \\mathbf{b}_z)\n",
    "   $$\n",
    "\n",
    "3. **Candidate Activation ($ \\tilde{\\mathbf{h}}_t $)**:\n",
    "\n",
    "   - Proposed new memory content.\n",
    "\n",
    "   $$\n",
    "   \\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W}_h \\mathbf{x}_t + \\mathbf{U}_h (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1}) + \\mathbf{b}_h)\n",
    "   $$\n",
    "\n",
    "4. **Final Memory at Time $ t $ ($ \\mathbf{h}_t $)**:\n",
    "\n",
    "   - Combines the previous memory and the candidate activation.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{h}_t = (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t\n",
    "   $$\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- **Simpler Structure**: Fewer gates than LSTM, making it computationally efficient.\n",
    "- **Capability**: Effectively captures dependencies over long sequences.\n",
    "\n",
    "### Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTMs introduce memory cells and multiple gates to regulate the flow of information, specifically designed to overcome vanishing gradient problems.\n",
    "\n",
    "#### Components\n",
    "\n",
    "1. **Input Gate ($ \\mathbf{i}_t $)**:\n",
    "\n",
    "   - Controls the extent to which new information flows into the cell.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\mathbf{x}_t + \\mathbf{U}_i \\mathbf{h}_{t-1} + \\mathbf{b}_i)\n",
    "   $$\n",
    "\n",
    "2. **Forget Gate ($ \\mathbf{f}_t $)**:\n",
    "\n",
    "   - Determines what information to discard from the cell.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\mathbf{x}_t + \\mathbf{U}_f \\mathbf{h}_{t-1} + \\mathbf{b}_f)\n",
    "   $$\n",
    "\n",
    "3. **Output Gate ($ \\mathbf{o}_t $)**:\n",
    "\n",
    "   - Controls the output of the cell to the hidden state.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{o}_t = \\sigma(\\mathbf{W}_o \\mathbf{x}_t + \\mathbf{U}_o \\mathbf{h}_{t-1} + \\mathbf{b}_o)\n",
    "   $$\n",
    "\n",
    "4. **Cell Candidate ($ \\tilde{\\mathbf{c}}_t $)**:\n",
    "\n",
    "   - New candidate values for the cell state.\n",
    "\n",
    "   $$\n",
    "   \\tilde{\\mathbf{c}}_t = \\tanh(\\mathbf{W}_c \\mathbf{x}_t + \\mathbf{U}_c \\mathbf{h}_{t-1} + \\mathbf{b}_c)\n",
    "   $$\n",
    "\n",
    "5. **Cell State Update ($ \\mathbf{c}_t $)**:\n",
    "\n",
    "   - Updates the cell state by combining previous state and candidate.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t\n",
    "   $$\n",
    "\n",
    "6. **Hidden State ($ \\mathbf{h}_t $)**:\n",
    "\n",
    "   - Computes the hidden state based on the cell state.\n",
    "\n",
    "   $$\n",
    "   \\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n",
    "   $$\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- **Long-Term Dependencies**: Capable of learning dependencies over long sequences.\n",
    "- **Flexible Memory**: The cell state acts as an explicit memory, modulated by gates.\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "- **Initialization**: Proper weight initialization can improve convergence.\n",
    "- **Regularization**: Techniques like dropout (applied to recurrent connections) can prevent overfitting.\n",
    "- **Optimization Algorithms**: Adaptive optimizers like Adam are commonly used.\n",
    "- **Sequence Padding**: Handle variable-length sequences by padding and masking.\n",
    "- **Batch Processing**: Efficient batching requires careful handling of sequences of different lengths.\n",
    "\n",
    "By mastering these concepts and techniques, one can effectively design and train RNNs to model complex sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=0.6979988217353821\n",
      "step=50, loss=0.689518928527832\n",
      "step=100, loss=0.695279598236084\n",
      "step=150, loss=0.04843534156680107\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"424.8pt\" height=\"280.8pt\" viewBox=\"0 0 424.8 280.8\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-15T23:21:44.666374</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 280.8 \n",
       "L 424.8 280.8 \n",
       "L 424.8 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 243.24375 \n",
       "L 417.6 243.24375 \n",
       "L 417.6 22.318125 \n",
       "L 43.78125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m275294f6e9\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"60.773011\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(57.591761 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-30\" d=\"M 2034 219 \n",
       "Q 2513 219 2750 744 \n",
       "Q 2988 1269 2988 2328 \n",
       "Q 2988 3391 2750 3916 \n",
       "Q 2513 4441 2034 4441 \n",
       "Q 1556 4441 1318 3916 \n",
       "Q 1081 3391 1081 2328 \n",
       "Q 1081 1269 1318 744 \n",
       "Q 1556 219 2034 219 \n",
       "z\n",
       "M 2034 -91 \n",
       "Q 1275 -91 848 546 \n",
       "Q 422 1184 422 2328 \n",
       "Q 422 3475 848 4112 \n",
       "Q 1275 4750 2034 4750 \n",
       "Q 2797 4750 3222 4112 \n",
       "Q 3647 3475 3647 2328 \n",
       "Q 3647 1184 3222 546 \n",
       "Q 2797 -91 2034 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"103.465879\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(97.103379 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-32\" d=\"M 819 3553 \n",
       "L 469 3553 \n",
       "L 469 4384 \n",
       "Q 803 4563 1142 4656 \n",
       "Q 1481 4750 1806 4750 \n",
       "Q 2534 4750 2956 4397 \n",
       "Q 3378 4044 3378 3438 \n",
       "Q 3378 2753 2422 1800 \n",
       "Q 2347 1728 2309 1691 \n",
       "L 1131 513 \n",
       "L 3078 513 \n",
       "L 3078 1088 \n",
       "L 3444 1088 \n",
       "L 3444 0 \n",
       "L 434 0 \n",
       "L 434 341 \n",
       "L 1850 1753 \n",
       "Q 2319 2222 2519 2614 \n",
       "Q 2719 3006 2719 3438 \n",
       "Q 2719 3909 2473 4175 \n",
       "Q 2228 4441 1797 4441 \n",
       "Q 1350 4441 1106 4219 \n",
       "Q 863 3997 819 3553 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSerif-35\" d=\"M 3219 4666 \n",
       "L 3219 4153 \n",
       "L 1081 4153 \n",
       "L 1081 2816 \n",
       "Q 1244 2928 1461 2984 \n",
       "Q 1678 3041 1947 3041 \n",
       "Q 2703 3041 3140 2622 \n",
       "Q 3578 2203 3578 1478 \n",
       "Q 3578 738 3136 323 \n",
       "Q 2694 -91 1894 -91 \n",
       "Q 1572 -91 1234 -12 \n",
       "Q 897 66 544 225 \n",
       "L 544 1131 \n",
       "L 897 1131 \n",
       "Q 925 688 1179 453 \n",
       "Q 1434 219 1894 219 \n",
       "Q 2388 219 2653 544 \n",
       "Q 2919 869 2919 1478 \n",
       "Q 2919 2084 2655 2407 \n",
       "Q 2391 2731 1894 2731 \n",
       "Q 1613 2731 1398 2631 \n",
       "Q 1184 2531 1019 2322 \n",
       "L 750 2322 \n",
       "L 750 4666 \n",
       "L 3219 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"146.158747\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(139.796247 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"188.851615\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 75 -->\n",
       "      <g transform=\"translate(182.489115 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-37\" d=\"M 3609 4347 \n",
       "L 1784 0 \n",
       "L 1319 0 \n",
       "L 3059 4153 \n",
       "L 903 4153 \n",
       "L 903 3578 \n",
       "L 538 3578 \n",
       "L 538 4666 \n",
       "L 3609 4666 \n",
       "L 3609 4347 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"231.544482\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(222.000732 257.842188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-31\" d=\"M 909 0 \n",
       "L 909 331 \n",
       "L 1722 331 \n",
       "L 1722 4213 \n",
       "L 781 3603 \n",
       "L 781 4013 \n",
       "L 1919 4750 \n",
       "L 2350 4750 \n",
       "L 2350 331 \n",
       "L 3163 331 \n",
       "L 3163 0 \n",
       "L 909 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"274.23735\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 125 -->\n",
       "      <g transform=\"translate(264.6936 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"316.930218\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(307.386468 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"359.623086\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 175 -->\n",
       "      <g transform=\"translate(350.079336 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_9\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m275294f6e9\" x=\"402.315953\" y=\"243.24375\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(392.772203 257.842188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- Step -->\n",
       "     <g transform=\"translate(219.096094 271.520312) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-53\" d=\"M 594 225 \n",
       "L 594 1288 \n",
       "L 953 1284 \n",
       "Q 969 753 1261 498 \n",
       "Q 1553 244 2150 244 \n",
       "Q 2706 244 2998 464 \n",
       "Q 3291 684 3291 1106 \n",
       "Q 3291 1444 3114 1625 \n",
       "Q 2938 1806 2369 1978 \n",
       "L 1753 2163 \n",
       "Q 1084 2366 811 2669 \n",
       "Q 538 2972 538 3500 \n",
       "Q 538 4094 959 4422 \n",
       "Q 1381 4750 2144 4750 \n",
       "Q 2469 4750 2856 4679 \n",
       "Q 3244 4609 3681 4475 \n",
       "L 3681 3481 \n",
       "L 3328 3481 \n",
       "Q 3275 3975 2998 4195 \n",
       "Q 2722 4416 2156 4416 \n",
       "Q 1663 4416 1405 4214 \n",
       "Q 1147 4013 1147 3628 \n",
       "Q 1147 3294 1340 3103 \n",
       "Q 1534 2913 2163 2725 \n",
       "L 2741 2553 \n",
       "Q 3375 2363 3645 2067 \n",
       "Q 3916 1772 3916 1275 \n",
       "Q 3916 597 3481 253 \n",
       "Q 3047 -91 2188 -91 \n",
       "Q 1803 -91 1404 -12 \n",
       "Q 1006 66 594 225 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-74\" d=\"M 691 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 691 3322 \n",
       "L 691 4353 \n",
       "L 1269 4353 \n",
       "L 1269 3322 \n",
       "L 2350 3322 \n",
       "L 2350 2988 \n",
       "L 1269 2988 \n",
       "L 1269 878 \n",
       "Q 1269 456 1350 337 \n",
       "Q 1431 219 1650 219 \n",
       "Q 1875 219 1978 351 \n",
       "Q 2081 484 2088 781 \n",
       "L 2522 781 \n",
       "Q 2497 328 2275 118 \n",
       "Q 2053 -91 1600 -91 \n",
       "Q 1103 -91 897 129 \n",
       "Q 691 350 691 878 \n",
       "L 691 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-65\" d=\"M 3469 1600 \n",
       "L 991 1600 \n",
       "L 991 1575 \n",
       "Q 991 903 1244 561 \n",
       "Q 1497 219 1991 219 \n",
       "Q 2369 219 2611 417 \n",
       "Q 2853 616 2950 1006 \n",
       "L 3413 1006 \n",
       "Q 3275 459 2904 184 \n",
       "Q 2534 -91 1931 -91 \n",
       "Q 1203 -91 761 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2450 753 2931 \n",
       "Q 1188 3413 1894 3413 \n",
       "Q 2647 3413 3050 2948 \n",
       "Q 3453 2484 3469 1600 \n",
       "z\n",
       "M 2791 1931 \n",
       "Q 2772 2513 2545 2808 \n",
       "Q 2319 3103 1894 3103 \n",
       "Q 1497 3103 1269 2806 \n",
       "Q 1041 2509 991 1931 \n",
       "L 2791 1931 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-70\" d=\"M 1313 1825 \n",
       "L 1313 1497 \n",
       "Q 1313 897 1542 583 \n",
       "Q 1772 269 2209 269 \n",
       "Q 2650 269 2876 622 \n",
       "Q 3103 975 3103 1663 \n",
       "Q 3103 2353 2876 2703 \n",
       "Q 2650 3053 2209 3053 \n",
       "Q 1772 3053 1542 2737 \n",
       "Q 1313 2422 1313 1825 \n",
       "z\n",
       "M 738 2988 \n",
       "L 184 2988 \n",
       "L 184 3322 \n",
       "L 1313 3322 \n",
       "L 1313 2803 \n",
       "Q 1481 3116 1742 3264 \n",
       "Q 2003 3413 2388 3413 \n",
       "Q 3000 3413 3387 2928 \n",
       "Q 3775 2444 3775 1663 \n",
       "Q 3775 881 3387 395 \n",
       "Q 3000 -91 2388 -91 \n",
       "Q 2003 -91 1742 57 \n",
       "Q 1481 206 1313 519 \n",
       "L 1313 -997 \n",
       "L 1856 -997 \n",
       "L 1856 -1331 \n",
       "L 184 -1331 \n",
       "L 184 -997 \n",
       "L 738 -997 \n",
       "L 738 2988 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-53\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-74\" x=\"68.505859\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-65\" x=\"108.691406\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-70\" x=\"167.871094\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"m9f22c532dc\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"234.581069\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(20.878125 238.380288) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-2e\" d=\"M 603 325 \n",
       "Q 603 500 722 622 \n",
       "Q 841 744 1019 744 \n",
       "Q 1191 744 1312 622 \n",
       "Q 1434 500 1434 325 \n",
       "Q 1434 153 1312 31 \n",
       "Q 1191 -91 1019 -91 \n",
       "Q 841 -91 722 29 \n",
       "Q 603 150 603 325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"206.342598\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.1 -->\n",
       "      <g transform=\"translate(20.878125 210.141817) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-31\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"178.104127\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(20.878125 181.903346) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"149.865656\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.3 -->\n",
       "      <g transform=\"translate(20.878125 153.664874) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-33\" d=\"M 622 4469 \n",
       "Q 988 4606 1323 4678 \n",
       "Q 1659 4750 1953 4750 \n",
       "Q 2638 4750 3022 4454 \n",
       "Q 3406 4159 3406 3634 \n",
       "Q 3406 3213 3140 2930 \n",
       "Q 2875 2647 2388 2547 \n",
       "Q 2963 2466 3280 2130 \n",
       "Q 3597 1794 3597 1259 \n",
       "Q 3597 606 3158 257 \n",
       "Q 2719 -91 1894 -91 \n",
       "Q 1528 -91 1179 -12 \n",
       "Q 831 66 488 225 \n",
       "L 488 1131 \n",
       "L 838 1131 \n",
       "Q 869 681 1141 450 \n",
       "Q 1413 219 1906 219 \n",
       "Q 2384 219 2661 495 \n",
       "Q 2938 772 2938 1253 \n",
       "Q 2938 1803 2653 2086 \n",
       "Q 2369 2369 1819 2369 \n",
       "L 1522 2369 \n",
       "L 1522 2688 \n",
       "L 1678 2688 \n",
       "Q 2225 2688 2498 2914 \n",
       "Q 2772 3141 2772 3597 \n",
       "Q 2772 4006 2547 4223 \n",
       "Q 2322 4441 1900 4441 \n",
       "Q 1478 4441 1245 4241 \n",
       "Q 1013 4041 972 3647 \n",
       "L 622 3647 \n",
       "L 622 4469 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-33\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"121.627184\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(20.878125 125.426403) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-34\" d=\"M 2234 1581 \n",
       "L 2234 4063 \n",
       "L 641 1581 \n",
       "L 2234 1581 \n",
       "z\n",
       "M 3609 0 \n",
       "L 1484 0 \n",
       "L 1484 331 \n",
       "L 2234 331 \n",
       "L 2234 1247 \n",
       "L 197 1247 \n",
       "L 197 1588 \n",
       "L 2241 4750 \n",
       "L 2859 4750 \n",
       "L 2859 1581 \n",
       "L 3750 1581 \n",
       "L 3750 1247 \n",
       "L 2859 1247 \n",
       "L 2859 331 \n",
       "L 3609 331 \n",
       "L 3609 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"93.388713\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(20.878125 97.187932) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"65.150242\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_17\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(20.878125 68.949461) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSerif-36\" d=\"M 2094 219 \n",
       "Q 2534 219 2771 542 \n",
       "Q 3009 866 3009 1472 \n",
       "Q 3009 2078 2771 2401 \n",
       "Q 2534 2725 2094 2725 \n",
       "Q 1647 2725 1412 2412 \n",
       "Q 1178 2100 1178 1509 \n",
       "Q 1178 888 1415 553 \n",
       "Q 1653 219 2094 219 \n",
       "z\n",
       "M 1075 2569 \n",
       "Q 1288 2803 1556 2918 \n",
       "Q 1825 3034 2163 3034 \n",
       "Q 2859 3034 3264 2615 \n",
       "Q 3669 2197 3669 1472 \n",
       "Q 3669 763 3233 336 \n",
       "Q 2797 -91 2069 -91 \n",
       "Q 1278 -91 853 498 \n",
       "Q 428 1088 428 2181 \n",
       "Q 428 3406 931 4078 \n",
       "Q 1434 4750 2350 4750 \n",
       "Q 2597 4750 2869 4703 \n",
       "Q 3141 4656 3425 4563 \n",
       "L 3425 3794 \n",
       "L 3072 3794 \n",
       "Q 3034 4109 2831 4275 \n",
       "Q 2628 4441 2284 4441 \n",
       "Q 1678 4441 1381 3981 \n",
       "Q 1084 3522 1075 2569 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9f22c532dc\" x=\"43.78125\" y=\"36.91177\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_18\">\n",
       "      <!-- 0.7 -->\n",
       "      <g transform=\"translate(20.878125 40.710989) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSerif-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSerif-37\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- Loss -->\n",
       "     <g transform=\"translate(14.798438 144.242656) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSerif-4c\" d=\"M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 2175 4666 \n",
       "L 2175 4331 \n",
       "L 1581 4331 \n",
       "L 1581 384 \n",
       "L 3713 384 \n",
       "L 3713 1166 \n",
       "L 4097 1166 \n",
       "L 4097 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-6f\" d=\"M 1925 219 \n",
       "Q 2388 219 2623 584 \n",
       "Q 2859 950 2859 1663 \n",
       "Q 2859 2375 2623 2739 \n",
       "Q 2388 3103 1925 3103 \n",
       "Q 1463 3103 1227 2739 \n",
       "Q 991 2375 991 1663 \n",
       "Q 991 950 1228 584 \n",
       "Q 1466 219 1925 219 \n",
       "z\n",
       "M 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2456 758 2934 \n",
       "Q 1197 3413 1925 3413 \n",
       "Q 2653 3413 3092 2934 \n",
       "Q 3531 2456 3531 1663 \n",
       "Q 3531 869 3092 389 \n",
       "Q 2653 -91 1925 -91 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSerif-73\" d=\"M 359 184 \n",
       "L 359 959 \n",
       "L 691 959 \n",
       "Q 703 588 923 403 \n",
       "Q 1144 219 1575 219 \n",
       "Q 1963 219 2166 364 \n",
       "Q 2369 509 2369 788 \n",
       "Q 2369 1006 2220 1140 \n",
       "Q 2072 1275 1594 1428 \n",
       "L 1178 1569 \n",
       "Q 750 1706 558 1912 \n",
       "Q 366 2119 366 2438 \n",
       "Q 366 2894 700 3153 \n",
       "Q 1034 3413 1625 3413 \n",
       "Q 1888 3413 2178 3344 \n",
       "Q 2469 3275 2778 3144 \n",
       "L 2778 2419 \n",
       "L 2447 2419 \n",
       "Q 2434 2741 2221 2922 \n",
       "Q 2009 3103 1644 3103 \n",
       "Q 1281 3103 1095 2975 \n",
       "Q 909 2847 909 2591 \n",
       "Q 909 2381 1050 2254 \n",
       "Q 1191 2128 1613 1997 \n",
       "L 2069 1856 \n",
       "Q 2541 1709 2748 1489 \n",
       "Q 2956 1269 2956 922 \n",
       "Q 2956 450 2595 179 \n",
       "Q 2234 -91 1600 -91 \n",
       "Q 1278 -91 972 -22 \n",
       "Q 666 47 359 184 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"66.40625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"126.611328\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"177.929688\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_18\">\n",
       "    <path d=\"M 60.773011 37.476873 \n",
       "L 62.480726 38.675439 \n",
       "L 64.188441 35.138144 \n",
       "L 65.896155 39.753241 \n",
       "L 67.60387 40.499698 \n",
       "L 69.311585 42.609923 \n",
       "L 71.0193 40.625581 \n",
       "L 72.727014 36.98999 \n",
       "L 74.434729 38.150787 \n",
       "L 76.142444 40.827255 \n",
       "L 77.850158 35.718408 \n",
       "L 79.557873 39.265381 \n",
       "L 81.265588 40.945917 \n",
       "L 82.973303 43.357256 \n",
       "L 84.681017 41.007385 \n",
       "L 86.388732 39.97279 \n",
       "L 88.096447 41.377811 \n",
       "L 89.804161 33.402503 \n",
       "L 91.511876 39.69455 \n",
       "L 93.219591 38.68771 \n",
       "L 94.927306 38.797653 \n",
       "L 96.63502 40.498335 \n",
       "L 98.342735 42.785526 \n",
       "L 100.05045 37.055043 \n",
       "L 101.758164 36.819958 \n",
       "L 103.465879 37.77011 \n",
       "L 105.173594 34.06657 \n",
       "L 106.881309 39.328786 \n",
       "L 110.296738 39.824455 \n",
       "L 112.004453 41.793245 \n",
       "L 113.712167 41.642334 \n",
       "L 115.419882 34.965571 \n",
       "L 117.127597 35.491015 \n",
       "L 118.835312 43.455316 \n",
       "L 120.543026 32.360199 \n",
       "L 122.250741 34.25956 \n",
       "L 123.958456 39.709075 \n",
       "L 125.66617 39.408449 \n",
       "L 127.373885 37.193953 \n",
       "L 129.0816 41.28438 \n",
       "L 130.789314 38.450285 \n",
       "L 134.204744 39.94347 \n",
       "L 135.912459 37.460832 \n",
       "L 137.620173 39.10651 \n",
       "L 139.327888 40.225262 \n",
       "L 141.035603 39.778993 \n",
       "L 142.743317 38.855923 \n",
       "L 144.451032 38.787385 \n",
       "L 146.158747 39.871465 \n",
       "L 149.574176 38.086524 \n",
       "L 151.281891 39.841976 \n",
       "L 152.989606 39.147359 \n",
       "L 154.69732 37.472934 \n",
       "L 156.405035 38.504348 \n",
       "L 158.11275 38.377708 \n",
       "L 159.820465 39.530561 \n",
       "L 161.528179 38.29446 \n",
       "L 163.235894 38.886943 \n",
       "L 164.943609 39.266442 \n",
       "L 166.651323 39.052716 \n",
       "L 168.359038 39.50183 \n",
       "L 170.066753 40.101029 \n",
       "L 171.774468 39.164847 \n",
       "L 173.482182 40.196884 \n",
       "L 175.189897 38.055352 \n",
       "L 176.897612 37.638438 \n",
       "L 178.605326 38.312671 \n",
       "L 180.313041 39.64961 \n",
       "L 182.020756 40.325476 \n",
       "L 183.72847 38.859626 \n",
       "L 185.436185 41.971524 \n",
       "L 187.1439 38.974113 \n",
       "L 188.851615 40.457367 \n",
       "L 190.559329 36.510462 \n",
       "L 192.267044 40.697468 \n",
       "L 193.974759 37.805877 \n",
       "L 195.682473 41.026607 \n",
       "L 197.390188 38.857976 \n",
       "L 199.097903 35.379524 \n",
       "L 200.805618 41.059748 \n",
       "L 202.513332 38.764225 \n",
       "L 204.221047 39.291992 \n",
       "L 205.928762 41.103981 \n",
       "L 207.636476 35.293784 \n",
       "L 209.344191 40.051932 \n",
       "L 211.051906 35.991481 \n",
       "L 212.759621 40.720224 \n",
       "L 214.467335 40.35929 \n",
       "L 216.17505 43.30454 \n",
       "L 217.882765 40.538125 \n",
       "L 219.590479 38.236627 \n",
       "L 221.298194 38.167063 \n",
       "L 223.005909 37.295026 \n",
       "L 224.713624 43.97869 \n",
       "L 226.421338 40.729027 \n",
       "L 228.129053 39.083854 \n",
       "L 229.836768 36.38362 \n",
       "L 231.544482 38.24474 \n",
       "L 233.252197 38.902109 \n",
       "L 234.959912 40.323978 \n",
       "L 236.667626 40.934589 \n",
       "L 238.375341 39.151483 \n",
       "L 240.083056 38.599883 \n",
       "L 241.790771 39.875387 \n",
       "L 245.2062 40.037524 \n",
       "L 246.913915 41.684329 \n",
       "L 248.621629 41.300555 \n",
       "L 250.329344 42.196526 \n",
       "L 252.037059 41.369059 \n",
       "L 253.744774 41.352228 \n",
       "L 255.452488 41.55168 \n",
       "L 257.160203 44.543065 \n",
       "L 258.867918 44.399543 \n",
       "L 260.575632 47.20149 \n",
       "L 262.283347 37.876872 \n",
       "L 263.991062 50.490505 \n",
       "L 265.698777 50.277032 \n",
       "L 267.406491 54.517174 \n",
       "L 269.114206 49.267583 \n",
       "L 270.821921 57.681014 \n",
       "L 272.529635 62.816017 \n",
       "L 274.23735 57.709409 \n",
       "L 275.945065 76.398417 \n",
       "L 277.65278 82.360282 \n",
       "L 279.360494 70.644421 \n",
       "L 281.068209 81.249121 \n",
       "L 282.775924 82.040283 \n",
       "L 284.483638 90.770145 \n",
       "L 286.191353 91.337735 \n",
       "L 287.899068 103.91412 \n",
       "L 289.606782 111.201603 \n",
       "L 291.314497 106.143663 \n",
       "L 293.022212 120.806431 \n",
       "L 294.729927 127.723312 \n",
       "L 296.437641 144.829702 \n",
       "L 298.145356 150.328912 \n",
       "L 299.853071 155.434907 \n",
       "L 301.560785 171.660739 \n",
       "L 304.976215 194.497246 \n",
       "L 306.68393 188.406849 \n",
       "L 308.391644 202.798892 \n",
       "L 310.099359 205.278853 \n",
       "L 311.807074 212.777178 \n",
       "L 313.514788 215.934842 \n",
       "L 315.222503 219.50715 \n",
       "L 316.930218 220.903669 \n",
       "L 318.637933 223.088398 \n",
       "L 320.345647 224.522713 \n",
       "L 322.053362 225.594006 \n",
       "L 327.176506 228.05413 \n",
       "L 328.884221 228.796813 \n",
       "L 330.591936 229.110954 \n",
       "L 332.29965 229.580548 \n",
       "L 337.422794 230.574337 \n",
       "L 340.838224 230.973109 \n",
       "L 344.253653 231.361748 \n",
       "L 345.961368 231.542932 \n",
       "L 349.376797 231.77984 \n",
       "L 352.792227 232.018076 \n",
       "L 354.499941 232.022242 \n",
       "L 357.915371 232.248287 \n",
       "L 371.577089 232.712238 \n",
       "L 400.608239 233.201676 \n",
       "L 400.608239 233.201676 \n",
       "\" clip-path=\"url(#pbb24b7a837)\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 243.24375 \n",
       "L 43.78125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 417.6 243.24375 \n",
       "L 417.6 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 243.24375 \n",
       "L 417.6 243.24375 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 22.318125 \n",
       "L 417.6 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- Final Accuracy: 1.0 -->\n",
       "    <g transform=\"translate(171.7575 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSerif-46\" d=\"M 353 0 \n",
       "L 353 331 \n",
       "L 947 331 \n",
       "L 947 4331 \n",
       "L 353 4331 \n",
       "L 353 4666 \n",
       "L 4172 4666 \n",
       "L 4172 3628 \n",
       "L 3788 3628 \n",
       "L 3788 4281 \n",
       "L 1581 4281 \n",
       "L 1581 2719 \n",
       "L 3175 2719 \n",
       "L 3175 3303 \n",
       "L 3559 3303 \n",
       "L 3559 1753 \n",
       "L 3175 1753 \n",
       "L 3175 2338 \n",
       "L 1581 2338 \n",
       "L 1581 331 \n",
       "L 2328 331 \n",
       "L 2328 0 \n",
       "L 353 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-69\" d=\"M 622 4353 \n",
       "Q 622 4497 726 4603 \n",
       "Q 831 4709 978 4709 \n",
       "Q 1122 4709 1226 4603 \n",
       "Q 1331 4497 1331 4353 \n",
       "Q 1331 4206 1228 4103 \n",
       "Q 1125 4000 978 4000 \n",
       "Q 831 4000 726 4103 \n",
       "Q 622 4206 622 4353 \n",
       "z\n",
       "M 1356 331 \n",
       "L 1900 331 \n",
       "L 1900 0 \n",
       "L 231 0 \n",
       "L 231 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 331 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-6e\" d=\"M 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2988 \n",
       "L 231 2988 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1516 3069 1770 3241 \n",
       "Q 2025 3413 2363 3413 \n",
       "Q 2913 3413 3172 3097 \n",
       "Q 3431 2781 3431 2113 \n",
       "L 3431 331 \n",
       "L 3944 331 \n",
       "L 3944 0 \n",
       "L 2356 0 \n",
       "L 2356 331 \n",
       "L 2853 331 \n",
       "L 2853 1931 \n",
       "Q 2853 2541 2703 2767 \n",
       "Q 2553 2994 2175 2994 \n",
       "Q 1775 2994 1565 2701 \n",
       "Q 1356 2409 1356 1850 \n",
       "L 1356 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 263 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-61\" d=\"M 2547 1044 \n",
       "L 2547 1747 \n",
       "L 1806 1747 \n",
       "Q 1378 1747 1168 1562 \n",
       "Q 959 1378 959 997 \n",
       "Q 959 650 1171 447 \n",
       "Q 1384 244 1747 244 \n",
       "Q 2106 244 2326 466 \n",
       "Q 2547 688 2547 1044 \n",
       "z\n",
       "M 3122 2075 \n",
       "L 3122 331 \n",
       "L 3634 331 \n",
       "L 3634 0 \n",
       "L 2547 0 \n",
       "L 2547 359 \n",
       "Q 2356 128 2106 18 \n",
       "Q 1856 -91 1522 -91 \n",
       "Q 969 -91 644 203 \n",
       "Q 319 497 319 997 \n",
       "Q 319 1513 691 1797 \n",
       "Q 1063 2081 1741 2081 \n",
       "L 2547 2081 \n",
       "L 2547 2309 \n",
       "Q 2547 2688 2317 2895 \n",
       "Q 2088 3103 1672 3103 \n",
       "Q 1328 3103 1125 2947 \n",
       "Q 922 2791 872 2484 \n",
       "L 575 2484 \n",
       "L 575 3156 \n",
       "Q 875 3284 1158 3348 \n",
       "Q 1441 3413 1709 3413 \n",
       "Q 2400 3413 2761 3070 \n",
       "Q 3122 2728 3122 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-6c\" d=\"M 1313 331 \n",
       "L 1856 331 \n",
       "L 1856 0 \n",
       "L 184 0 \n",
       "L 184 331 \n",
       "L 738 331 \n",
       "L 738 4531 \n",
       "L 184 4531 \n",
       "L 184 4863 \n",
       "L 1313 4863 \n",
       "L 1313 331 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-41\" d=\"M 1281 1691 \n",
       "L 2994 1691 \n",
       "L 2138 3909 \n",
       "L 1281 1691 \n",
       "z\n",
       "M -38 0 \n",
       "L -38 331 \n",
       "L 372 331 \n",
       "L 2034 4666 \n",
       "L 2559 4666 \n",
       "L 4225 331 \n",
       "L 4684 331 \n",
       "L 4684 0 \n",
       "L 2988 0 \n",
       "L 2988 331 \n",
       "L 3506 331 \n",
       "L 3116 1356 \n",
       "L 1153 1356 \n",
       "L 763 331 \n",
       "L 1275 331 \n",
       "L 1275 0 \n",
       "L -38 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-63\" d=\"M 3291 997 \n",
       "Q 3169 466 2822 187 \n",
       "Q 2475 -91 1925 -91 \n",
       "Q 1200 -91 759 389 \n",
       "Q 319 869 319 1663 \n",
       "Q 319 2459 759 2936 \n",
       "Q 1200 3413 1925 3413 \n",
       "Q 2241 3413 2553 3339 \n",
       "Q 2866 3266 3181 3116 \n",
       "L 3181 2266 \n",
       "L 2847 2266 \n",
       "Q 2781 2703 2561 2903 \n",
       "Q 2341 3103 1931 3103 \n",
       "Q 1466 3103 1228 2742 \n",
       "Q 991 2381 991 1663 \n",
       "Q 991 944 1227 581 \n",
       "Q 1463 219 1931 219 \n",
       "Q 2303 219 2525 412 \n",
       "Q 2747 606 2828 997 \n",
       "L 3291 997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-75\" d=\"M 2266 3322 \n",
       "L 3341 3322 \n",
       "L 3341 331 \n",
       "L 3884 331 \n",
       "L 3884 0 \n",
       "L 2766 0 \n",
       "L 2766 588 \n",
       "Q 2606 256 2353 82 \n",
       "Q 2100 -91 1766 -91 \n",
       "Q 1213 -91 952 223 \n",
       "Q 691 538 691 1209 \n",
       "L 691 2988 \n",
       "L 172 2988 \n",
       "L 172 3322 \n",
       "L 1269 3322 \n",
       "L 1269 1388 \n",
       "Q 1269 781 1417 556 \n",
       "Q 1566 331 1947 331 \n",
       "Q 2347 331 2556 625 \n",
       "Q 2766 919 2766 1478 \n",
       "L 2766 2988 \n",
       "L 2266 2988 \n",
       "L 2266 3322 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-72\" d=\"M 3059 3328 \n",
       "L 3059 2497 \n",
       "L 2728 2497 \n",
       "Q 2713 2744 2591 2866 \n",
       "Q 2469 2988 2234 2988 \n",
       "Q 1809 2988 1582 2694 \n",
       "Q 1356 2400 1356 1850 \n",
       "L 1356 331 \n",
       "L 2022 331 \n",
       "L 2022 0 \n",
       "L 263 0 \n",
       "L 263 331 \n",
       "L 781 331 \n",
       "L 781 2994 \n",
       "L 231 2994 \n",
       "L 231 3322 \n",
       "L 1356 3322 \n",
       "L 1356 2731 \n",
       "Q 1525 3078 1790 3245 \n",
       "Q 2056 3413 2438 3413 \n",
       "Q 2578 3413 2733 3391 \n",
       "Q 2888 3369 3059 3328 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-79\" d=\"M 1381 -609 \n",
       "L 1600 -56 \n",
       "L 359 2988 \n",
       "L -19 2988 \n",
       "L -19 3322 \n",
       "L 1509 3322 \n",
       "L 1509 2988 \n",
       "L 978 2988 \n",
       "L 1913 703 \n",
       "L 2847 2988 \n",
       "L 2350 2988 \n",
       "L 2350 3322 \n",
       "L 3597 3322 \n",
       "L 3597 2988 \n",
       "L 3225 2988 \n",
       "L 1703 -750 \n",
       "Q 1547 -1138 1356 -1280 \n",
       "Q 1166 -1422 819 -1422 \n",
       "Q 672 -1422 517 -1397 \n",
       "Q 363 -1372 206 -1325 \n",
       "L 206 -691 \n",
       "L 500 -691 \n",
       "Q 519 -903 608 -995 \n",
       "Q 697 -1088 884 -1088 \n",
       "Q 1056 -1088 1161 -992 \n",
       "Q 1266 -897 1381 -609 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSerif-3a\" d=\"M 666 325 \n",
       "Q 666 500 786 622 \n",
       "Q 906 744 1081 744 \n",
       "Q 1256 744 1376 622 \n",
       "Q 1497 500 1497 325 \n",
       "Q 1497 150 1378 29 \n",
       "Q 1259 -91 1081 -91 \n",
       "Q 903 -91 784 29 \n",
       "Q 666 150 666 325 \n",
       "z\n",
       "M 666 2363 \n",
       "Q 666 2538 786 2658 \n",
       "Q 906 2778 1081 2778 \n",
       "Q 1259 2778 1378 2659 \n",
       "Q 1497 2541 1497 2363 \n",
       "Q 1497 2184 1378 2065 \n",
       "Q 1259 1947 1081 1947 \n",
       "Q 906 1947 786 2067 \n",
       "Q 666 2188 666 2363 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSerif-46\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-69\" x=\"69.384766\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6e\" x=\"101.367188\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-61\" x=\"165.771484\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-6c\" x=\"225.390625\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-20\" x=\"257.373047\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-41\" x=\"289.160156\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-63\" x=\"361.376953\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-63\" x=\"417.382812\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-75\" x=\"473.388672\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-72\" x=\"537.792969\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-61\" x=\"585.595703\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-63\" x=\"645.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-79\" x=\"701.220703\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-3a\" x=\"757.714844\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-20\" x=\"791.40625\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-31\" x=\"823.193359\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-2e\" x=\"886.816406\"/>\n",
       "     <use xlink:href=\"#DejaVuSerif-30\" x=\"918.603516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 355.676562 44.99625 \n",
       "L 410.6 44.99625 \n",
       "Q 412.6 44.99625 412.6 42.99625 \n",
       "L 412.6 29.318125 \n",
       "Q 412.6 27.318125 410.6 27.318125 \n",
       "L 355.676562 27.318125 \n",
       "Q 353.676562 27.318125 353.676562 29.318125 \n",
       "L 353.676562 42.99625 \n",
       "Q 353.676562 44.99625 355.676562 44.99625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 357.676562 35.416562 \n",
       "L 367.676562 35.416562 \n",
       "L 377.676562 35.416562 \n",
       "\" style=\"fill: none; stroke: #0000ff; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_21\">\n",
       "     <!-- Loss -->\n",
       "     <g transform=\"translate(385.676562 38.916562) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSerif-4c\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-6f\" x=\"66.40625\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"126.611328\"/>\n",
       "      <use xlink:href=\"#DejaVuSerif-73\" x=\"177.929688\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pbb24b7a837\">\n",
       "   <rect x=\"43.78125\" y=\"22.318125\" width=\"373.81875\" height=\"220.925625\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classify clockwise vs anticlockwise spirals\n",
    "\n",
    "def dataloader(arrays, batch_size):\n",
    "    \"\"\"\n",
    "    Custom data loader that yields batches of data.\n",
    "    \n",
    "    Parameters:\n",
    "    arrays : tuple of arrays\n",
    "        The data arrays to be batched.\n",
    "    batch_size : int\n",
    "        The size of each batch.\n",
    "    \n",
    "    Yields:\n",
    "    tuple of arrays\n",
    "        Batches of data.\n",
    "    \"\"\"\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = np.random.permutation(indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    \"\"\"\n",
    "    Generate spiral data for classification.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset_size : int\n",
    "        The size of the dataset.\n",
    "    key : jax.random.PRNGKey\n",
    "        Random key for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    tuple of arrays\n",
    "        The generated data and labels.\n",
    "    \"\"\"\n",
    "    t = jnp.linspace(0, 2 * jnp.pi, 16)\n",
    "    offset = random.uniform(key, (dataset_size, 1), minval=0, maxval=2 * jnp.pi)\n",
    "    x1 = jnp.sin(t + offset) / (1 + t)\n",
    "    x2 = jnp.cos(t + offset) / (1 + t)\n",
    "    y = jnp.ones((dataset_size, 1))\n",
    "\n",
    "    half_dataset_size = dataset_size // 2\n",
    "    x1 = x1.at[:half_dataset_size].multiply(-1)\n",
    "    y = y.at[:half_dataset_size].set(0)\n",
    "    x = jnp.stack([x1, x2], axis=-1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = random.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias)\n",
    "    \n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, x, y, opt_state):\n",
    "    loss, grads = compute_loss(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state\n",
    "\n",
    "# Parameters\n",
    "depth = 1\n",
    "seed = 5678\n",
    "steps = 200\n",
    "batch_size = 32\n",
    "hidden_size = 16\n",
    "dataset_size = 10000\n",
    "learning_rate = 3e-3\n",
    "\n",
    "# Generate data\n",
    "data_key, model_key = random.split(random.PRNGKey(seed), 2)\n",
    "xs, ys = get_data(dataset_size, key=data_key)\n",
    "iter_data = dataloader((xs, ys), batch_size)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n",
    "optim = optax.adam(learning_rate)\n",
    "opt_state = optim.init(model)\n",
    "\n",
    "# Initialize lists to store loss values and prediction data\n",
    "losses, pred_ys_list = [], []\n",
    "\n",
    "# Training loop\n",
    "for step, (x, y) in zip(range(steps), iter_data):\n",
    "    loss, model, opt_state = make_step(model, x, y, opt_state)\n",
    "    losses.append(loss.item())\n",
    "    if step % 50 == 0:\n",
    "        print(f\"step={step}, loss={loss.item()}\")\n",
    "\n",
    "# Evaluate model\n",
    "pred_ys = jax.vmap(model)(xs)\n",
    "pred_ys_list.append(pred_ys)\n",
    "num_correct = jnp.sum((pred_ys > 0.5) == ys)\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 1, figsize=(6, 4), tight_layout=True)\n",
    "axs.plot(range(steps), losses, 'b-', label='Loss')\n",
    "axs.set_xlabel('Step')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.set_title(f\"Final Accuracy: {(num_correct / dataset_size).item()}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers: An Overview\n",
    "\n",
    "Transformers are a type of neural network architecture that uses a mechanism called **attention**, which allows the model to focus on different parts of the input data by assigning varying weights to different elements. Unlike traditional models, these weights are determined based on the input itself, creating flexible, context-dependent representations. Transformers \"transform\" input vectors into a new set of vectors in a different space that is better suited for solving complex tasks, like translation or question-answering. \n",
    "\n",
    "The key advantages of transformers include:\n",
    "\n",
    "1. **Parallelization**: Transformers are highly compatible with parallel processing on GPUs, allowing large models with trillions of parameters to be trained efficiently.\n",
    "2. **Transfer Learning**: Transformers can be trained on a broad dataset and then fine-tuned on specific tasks, enabling effective transfer learning. Large, general-purpose models created this way are called **foundation models** because they serve as adaptable bases for various applications.\n",
    "3. **Self-Supervised Learning**: Transformers can learn patterns in vast amounts of unlabelled data, such as text from the internet, making them suitable for tasks like language modeling.\n",
    "\n",
    "Transformers were initially designed for language processing tasks, which share some characteristics with image processing, such as:\n",
    "\n",
    "- **High dimensionality**: The input data has many variables (e.g., words in a sentence or pixels in an image).\n",
    "- **Consistency in Statistics**: The statistical properties remain similar across different parts (e.g., word meaning doesn’t change based on position).\n",
    "\n",
    "A unique challenge in language data is that text sequences vary in length, which transformers manage through a mechanism called **attention**.\n",
    "\n",
    "### The Attention Mechanism\n",
    "\n",
    "The attention mechanism is the core of the transformer and operates as follows:\n",
    "\n",
    "1. **Input Representation**: The input is represented as a matrix, $ \\mathbf{X} $, where each column is an individual input vector. The matrix is of size $ D \\times N $, where $ D $ is the dimensionality of each vector and $ N $ is the number of inputs.\n",
    "\n",
    "2. **Defining Queries, Keys, and Values**: Three matrices — the query $ \\mathbf{Q} $, the key $ \\mathbf{K} $, and the value $ \\mathbf{V} $ — are derived from the input:\n",
    "   $$\n",
    "   \\mathbf{Q} = \\mathbf{W}_q \\mathbf{X}, \\quad \\mathbf{K} = \\mathbf{W}_k \\mathbf{X}, \\quad \\mathbf{V} = \\mathbf{W}_v \\mathbf{X}\n",
    "   $$\n",
    "   where $ \\mathbf{W}_q $, $ \\mathbf{W}_k $, and $ \\mathbf{W}_v $ are learned weight matrices.\n",
    "\n",
    "3. **Calculating Attention Weights**: The model computes the similarity between queries and keys through inner products. A **softmax** function is then applied to normalize these similarities into attention weights:\n",
    "   $$\n",
    "   \\mathbf{A} = \\text{Softmax}(\\mathbf{K}^T \\mathbf{Q})\n",
    "   $$\n",
    "   Here, the attention weights determine how much focus each input (key) should receive relative to the query.\n",
    "\n",
    "4. **Output Calculation**: The attention weights are used to combine the values:\n",
    "   $$\n",
    "   \\mathbf{Sa} = \\mathbf{V} \\, \\text{Softmax}(\\mathbf{K}^T \\mathbf{Q})\n",
    "   $$\n",
    "   This weighted sum forms the final output of the attention mechanism, reflecting the model’s focus on various input elements based on the context.\n",
    "\n",
    "The attention mechanism is nonlinear due to the softmax operation, making it capable of capturing complex patterns. Since it relies on a shared set of weights, it can handle varying sequence lengths and scales efficiently.\n",
    "\n",
    "### Extensions to Attention\n",
    "\n",
    "Three common extensions enhance the attention mechanism in transformers:\n",
    "\n",
    "1. **Positional Encoding**: Since attention itself doesn’t account for word order, positional encodings are added to preserve sequence information. These can be absolute (added to the input vectors) or relative (applied to the attention weights).\n",
    "\n",
    "2. **Scaled Attention**: To avoid issues where large inner products overpower the softmax function and slow down training, the inner products are scaled by the square root of the query dimension $ D_q $:\n",
    "   $$\n",
    "   \\mathbf{Sa}[\\mathbf{X}] = \\mathbf{V} \\, \\text{Softmax} \\left( \\frac{\\mathbf{K}^T \\mathbf{Q}}{\\sqrt{D_q}} \\right)\n",
    "   $$\n",
    "\n",
    "3. **Multi-Head Attention**: In practice, multiple attention mechanisms (called heads) operate in parallel, each with its own set of $ \\mathbf{Q} $, $ \\mathbf{K} $, and $ \\mathbf{V} $ matrices. These outputs are combined through a final linear transformation, allowing the model to capture diverse relationships in the data.\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "The core of a transformer is a **multi-head self-attention unit** followed by a **fully connected feed-forward network**. The structure enables words or tokens to interact with each other through attention and then apply nonlinear transformations independently. This structure is repeated in layers to build deep representations.\n",
    "\n",
    "Each layer includes:\n",
    "\n",
    "1. **Multi-Head Attention**: The word representations interact through multi-head attention, and the output is combined with the input using a residual connection.\n",
    "2. **Layer Normalization**: Similar to batch normalization, but using statistics from each sequence, it improves model stability and convergence.\n",
    "3. **Feed-Forward Network**: A fully connected network operates separately on each token, adding more representation power.\n",
    "\n",
    "With multiple stacked layers, transformers can learn intricate dependencies and capture diverse language structures, making them effective for many natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Networks (GNNs): An Overview\n",
    "\n",
    "Graphs are highly flexible structures consisting of **nodes** (or vertices) connected by **edges** (or links). They are commonly used to model complex relationships in various domains, from social networks to biological systems. Graphs are often sparse, meaning only a small subset of possible connections between nodes exists.\n",
    "\n",
    "Some common types of graphs include:\n",
    "1. **Social Networks**: Represented as undirected graphs where connections between nodes (people) are symmetric.\n",
    "2. **Citation Networks**: Directed graphs where one paper cites another, creating asymmetric relationships.\n",
    "3. **Knowledge Graphs**: Directed multigraphs where nodes can represent various types of entities (people, places) with multiple edges denoting different relationships.\n",
    "4. **Geometric Graphs**: Represent a set of points in a 3D space connected based on proximity.\n",
    "5. **Hierarchical Graphs**: Represent complex structures where subgraphs represent parts of an overall system, such as objects in a scene.\n",
    "\n",
    "Working with graphs introduces unique challenges:\n",
    "- **Variable topology**: Graph structures vary widely, making it difficult to design expressive models that handle this variation.\n",
    "- **Large scale**: Graphs can be massive, as in social networks with billions of nodes.\n",
    "- **Single-instance training**: Often, there is only one large graph available, making standard training/testing splits less feasible.\n",
    "\n",
    "### Representing Graphs Mathematically\n",
    "\n",
    "Graphs are represented with matrices to capture node features and relationships:\n",
    "- **Adjacency Matrix** $ \\mathbf{A} $: A binary matrix where element $ (m, n) $ is 1 if there is an edge between nodes $ m $ and $ n $.\n",
    "- **Node Data Matrix** $ \\mathbf{X} $: Stores feature vectors for each node.\n",
    "- **Edge Data Matrix** $ \\mathbf{E} $: Stores feature vectors for each edge.\n",
    "\n",
    "The adjacency matrix provides information about connectivity patterns, where powers of $ \\mathbf{A} $ reveal multi-step connections. For instance, $ \\mathbf{A}^2 $ indicates paths of length two between nodes.\n",
    "\n",
    "### Graph Neural Network Tasks\n",
    "\n",
    "GNNs are designed for tasks such as:\n",
    "- **Graph Classification**: Aggregates node features into a single vector to classify the entire graph.\n",
    "- **Node Classification**: Predicts labels for individual nodes based on their local neighborhood and global graph context.\n",
    "- **Edge Prediction**: Estimates the probability of missing edges, often used for link prediction tasks.\n",
    "\n",
    "### Graph Convolutional Networks (GCNs)\n",
    "\n",
    "GCNs are a popular type of GNN that use convolution-like operations to process graph data. They apply a function $ \\mathbf{F} $ that aggregates information from each node's neighbors, creating new node embeddings layer by layer. This aggregation process creates a **relational inductive bias**, focusing the model on information from neighboring nodes.\n",
    "\n",
    "Each layer of a GCN can be represented as:\n",
    "$$\n",
    "\\mathbf{H}_{k+1} = \\mathbf{F}\\left(\\mathbf{H}_k, \\mathbf{A}, \\phi_k\\right),\n",
    "$$\n",
    "where:\n",
    "- $ \\mathbf{H}_k $ denotes node embeddings at layer $ k $.\n",
    "- $ \\mathbf{A} $ is the adjacency matrix.\n",
    "- $ \\phi_k $ represents learned parameters for layer $ k $.\n",
    "\n",
    "The layers successively refine node embeddings, gathering information from increasingly larger neighborhoods.\n",
    "\n",
    "### Key Properties of GNNs\n",
    "\n",
    "1. **Permutation Invariance**: Since the order of nodes is arbitrary, GNN layers must be **equivariant** to permutations. This means that if node indices are reordered, the resulting node embeddings should also reorder in the same way, preserving the overall structure.\n",
    "\n",
    "2. **Parameter Sharing**: GNNs use shared parameters across nodes, reducing the number of parameters required and allowing generalization to different graphs with similar structures.\n",
    "\n",
    "3. **Inductive vs. Transductive Learning**:\n",
    "   - **Inductive**: Involves training on multiple graphs and generalizing to new, unseen graphs.\n",
    "   - **Transductive**: Operates on a single large graph where some nodes are labeled during training, and the model learns to predict labels for other nodes in the same graph.\n",
    "\n",
    "### Receptive Fields in GNNs\n",
    "\n",
    "The concept of **receptive fields** in GNNs is analogous to CNNs. Each node’s receptive field grows with each layer, gathering information from nodes within an increasing number of “hops.” For instance, in a 2-layer GCN, a node’s receptive field would include all nodes within a two-hop neighborhood, effectively capturing local structure around each node.\n",
    "\n",
    "### Equivariance and Invariance in GNNs\n",
    "\n",
    "For GNNs to respect the arbitrary ordering of nodes:\n",
    "- **Equivariance**: Each layer must respect permutations in the node order.\n",
    "- **Invariance**: For graph-level tasks, the final output should be invariant to node order since the overall structure, not individual node positions, is important.\n",
    "\n",
    "### Graph Neural Network Applications\n",
    "\n",
    "GNNs have broad applications in areas like social network analysis, recommendation systems, molecular graph analysis, and knowledge representation. By efficiently processing complex graph structures, GNNs enable powerful insights and predictions on various types of relational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix: \n",
      " [[[1. 1. 0. 0.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [0. 1. 1. 1.]\n",
      "  [0. 1. 1. 1.]]]\n",
      "Input features: \n",
      " [[[0. 1.]\n",
      "  [2. 3.]\n",
      "  [4. 5.]\n",
      "  [6. 7.]]]\n",
      "Output features: \n",
      " [[[1. 2.]\n",
      "  [3. 4.]\n",
      "  [4. 5.]\n",
      "  [4. 5.]]]\n"
     ]
    }
   ],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    c_out: int  # Output feature size\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Graph Convolutional Network (GCN) layer.\n",
    "        \n",
    "        Inputs:\n",
    "            node_feats - Array with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(axis=-1, keepdims=True)\n",
    "        \n",
    "        # Linear transformation of node features\n",
    "        node_feats = nn.Dense(features=self.c_out, name='projection')(node_feats)\n",
    "        \n",
    "        # Aggregation of neighboring node features\n",
    "        node_feats = jax.lax.batch_matmul(adj_matrix, node_feats)\n",
    "        \n",
    "        # Normalization by the number of neighbors\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        \n",
    "        return node_feats\n",
    "\n",
    "# Example input features and adjacency matrix\n",
    "node_feats = jnp.arange(8, dtype=jnp.float32).reshape((1, 4, 2))\n",
    "adj_matrix = jnp.array([[[1, 1, 0, 0],\n",
    "                         [1, 1, 1, 1],\n",
    "                         [0, 1, 1, 1],\n",
    "                         [0, 1, 1, 1]]]).astype(jnp.float32)\n",
    "\n",
    "# Initialize the GCN layer\n",
    "layer = GCNLayer(c_out=2)\n",
    "\n",
    "# Define custom parameters instead of using random initialization\n",
    "params = {'projection': {\n",
    "          'kernel': jnp.array([[1., 0.], [0., 1.]]),\n",
    "          'bias'  : jnp.array([0., 0.])}}\n",
    "\n",
    "# Apply the GCN layer to the input features and adjacency matrix\n",
    "out_feats = layer.apply({'params': params}, node_feats, adj_matrix)\n",
    "\n",
    "# Print the results\n",
    "print(\"Adjacency matrix: \\n\", adj_matrix)\n",
    "print(\"Input features: \\n\", node_feats)\n",
    "print(\"Output features: \\n\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Neural Networks (BNNs)\n",
    "\n",
    "In traditional deep neural networks (DNNs), the model is usually trained to find a single set of parameters by optimizing a maximum likelihood objective. However, this approach overlooks the fact that a single parameter setting is only one of many that could fit the training data well. Different parameter settings can lead to different generalization behaviors. Bayesian Neural Networks (BNNs) address this by averaging over possible parameter configurations to improve both accuracy and uncertainty estimation, a process known as **Bayesian model averaging**. This averaging produces the **posterior predictive distribution**:\n",
    "$$\n",
    "p(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\mathcal{D}) = \\int p(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\mathcal{D}) \\, d \\boldsymbol{\\theta}\n",
    "$$\n",
    "where $ p(\\boldsymbol{\\theta} \\mid \\mathcal{D}) $ is the posterior over parameters, obtained by combining a prior $ p(\\boldsymbol{\\theta}) $ and the likelihood of the data $ p(\\mathcal{D} \\mid \\boldsymbol{\\theta}) $.\n",
    "\n",
    "**Challenges in BNNs**: Applying Bayesian inference to neural networks involves two main difficulties:\n",
    "1. **Specifying Priors**: Selecting suitable prior distributions over parameters.\n",
    "2. **Posterior Computation**: Calculating the posterior distribution efficiently, which is challenging due to the high dimensionality of parameters and large datasets. \n",
    "\n",
    "BNNs are also a part of the broader area of **Bayesian deep learning (BDL)**, where Bayesian methods help neural networks represent uncertainty. In contrast, **deep Bayesian learning (DBL)** uses deep models to expedite Bayesian inference in classical models, often through variational inference or importance sampling techniques.\n",
    "\n",
    "### Variational Inference\n",
    "\n",
    "In **variational inference** for BNNs, we approximate the intractable posterior with a simpler distribution $ q_{\\boldsymbol{\\psi}}(\\boldsymbol{\\theta}) $, parameterized by $ \\boldsymbol{\\psi} $. The aim is to minimize the Kullback-Leibler (KL) divergence between the approximate posterior $ q $ and the true posterior $ p $, by finding the optimal $ \\boldsymbol{\\psi} $. A common choice for the approximate posterior is a **Gaussian distribution** $ q_{\\boldsymbol{\\psi}}(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) $, allowing the use of the **reparameterization trick** to estimate gradients for the **evidence lower bound (ELBO)**, an objective used in variational inference. Although Gaussian approximations are often used, they can differ significantly from other approximations like the Laplace approximation.\n",
    "\n",
    "### Markov Chain Monte Carlo (MCMC) Methods\n",
    "\n",
    "MCMC methods, like **Hamiltonian Monte Carlo (HMC)**, can approximate the posterior in BNNs without strong assumptions about its shape. This is considered the gold standard for BNNs, as it theoretically provides a faithful representation of the posterior distribution. However, traditional MCMC methods are computationally intensive, as they need to process the entire dataset at each step. To address this, **Stochastic Gradient MCMC** methods, such as **Stochastic Gradient Langevin Dynamics (SGLD)**, use mini-batches to improve scalability. However, as model complexity and depth increase, MCMC approaches, including stochastic variants, can become prohibitively slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning (AL)\n",
    "\n",
    "**Active Learning (AL)** is a semi-supervised approach that allows the model to **query for new labels** from a user or oracle selectively. Its purpose is to achieve high accuracy with fewer labeled data points by targeting the most informative samples for labeling. This is particularly useful in applications where obtaining labeled data is expensive or time-consuming.\n",
    "\n",
    "In **Gaussian process (GP) regression**, active learning selects the next data point to label based on the model's uncertainty, measured by the **posterior variance**. The next point is chosen by **maximizing the expected reduction in variance**, effectively refining the model in areas of high uncertainty. This criterion can be expressed as:\n",
    "$$\n",
    "\\mathbb{E}_{p(\\theta)} \\left[ \\left( \\sigma^2(\\theta) - \\sigma^2(\\theta, x) \\right)^2 \\right]\n",
    "$$\n",
    "where $ \\sigma^2(\\theta) $ is the current posterior variance and $ \\sigma^2(\\theta, x) $ is the posterior variance after adding a new data point $ x $. This formulation prioritizes points that are likely to reduce the overall model uncertainty, leading to more efficient learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
